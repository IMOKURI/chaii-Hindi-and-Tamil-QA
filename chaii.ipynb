{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chaii.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO7VVJ9IjH4hN+vfW3duHxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IMOKURI/chaii-Hindi-and-Tamil-QA/blob/main/chaii.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8HABKC9jNl"
      },
      "source": [
        "# üìî About this notebook ...\n",
        "\n",
        "[chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U4_rc_e9rOY"
      },
      "source": [
        "# Memo\n",
        "\n",
        "\n",
        "\n",
        "## ToDo\n",
        "\n",
        "- [ ] [„É©„Éô„É´„Éé„Ç§„Ç∫Ë£úÊ≠£](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264395)\n",
        "    - [ ] [„Åì„Çå„ÇÇ„Åã„Å™](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266109)\n",
        "- „É¢„Éá„É´\n",
        "    - [ ] [monsoon-nlp/hindi-tpu-electra](https://huggingface.co/monsoon-nlp/hindi-tpu-electra)\n",
        "    - [ ] [RemBERT](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267827)\n",
        "    - [ ] [google/muril-base-cased](https://huggingface.co/google/muril-base-cased)\n",
        "        - unknown token „ÅåÂ§ö„Åù„ÅÜ [It looks like Muril has even more unknowns.](https://www.kaggle.com/nbroad/chaii-qa-character-token-languages-eda?rvi=1&scriptVersionId=74271440&cellId=10)\n",
        "    - „Åù„ÅÆ‰ªñ [multilingual & QA models](https://huggingface.co/models?filter=multilingual&pipeline_tag=question-answering)\n",
        "\n",
        "## Done\n",
        "\n",
        "\n",
        "## Works Well\n",
        "\n",
        "- [x] [Post process „Åß„Çπ„Ç≥„Ç¢„Ç¢„ÉÉ„Éó](https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765)\n",
        "    - [x] ÂõûÁ≠îÂâçÂæå„ÅÆ„Çπ„Éö„Éº„Çπ„ÇÑÊîπË°å„ÇÇÂâäÈô§\n",
        "\n",
        "## Doesn't Work\n",
        "\n",
        "\n",
        "## Not To Do\n",
        "\n",
        "- [2„Å§„ÅÆ„É¢„Éá„É´„Çí‰Ωú„Çã](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267604) - [ÁµåÁ∑Ø](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264749)\n",
        "- [‰ΩçÁΩÆ„Å´„Çà„Çã„Éö„Éä„É´„ÉÜ„Ç£„ÇíË™≤„Åô Loss](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266832)\n",
        "- [x] „É¢„Éá„É´„ÇØ„É©„Çπ„Åß  `AutoModelForQuestionAnswering` „ÇØ„É©„Çπ „Çí‰Ωø„Å£„Å¶„Åø„Çã\n",
        "\n",
        "\n",
        "## Additional Datasets\n",
        "\n",
        "Search from [here](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264581).\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) hindi „ÅÆ„Åø\n",
        "- [Squad_Translated_to_Tamil for Chaii](https://www.kaggle.com/msafi04/squad-translated-to-tamil-for-chaii) tamil „ÅÆ„Åø\n",
        "\n",
        "\n",
        "## Reference Notebooks\n",
        "\n",
        "- [ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBhwVS89vKZ"
      },
      "source": [
        "# Prepare for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEN6L1ol9d6d",
        "outputId": "5d01f4b4-54cb-4fe1-f852-7175bf6feb2f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 15 01:55:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl14Cnli91Ol",
        "outputId": "3b46991d-7004-4be9-db4a-09ef9c0205d0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('init.txt'):\n",
        "    print(\"Already initialized.\")\n",
        "\n",
        "else:\n",
        "    if 'google.colab' in sys.modules:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        dataset_dir = \"/content/drive/MyDrive/Datasets\"\n",
        "\n",
        "        # ====================================================\n",
        "        # Competition datasets\n",
        "        # ====================================================\n",
        "        with zipfile.ZipFile(f\"{dataset_dir}/chaii-hindi-and-tamil-question-answering.zip\", \"r\") as zp:\n",
        "            zp.extractall(path=\"./\")\n",
        "        # with zipfile.ZipFile(f\"{dataset_dir}/chaii-external-data-mlqa-xquad-preprocessing.zip\", \"r\") as zp:\n",
        "        #     zp.extractall(path=\"./\")\n",
        "        # with zipfile.ZipFile(f\"{dataset_dir}/chaii-Squad_Translated_to_Tamil.zip\", \"r\") as zp:\n",
        "        #     zp.extractall(path=\"./\")\n",
        "\n",
        "    # for StratifiedGroupKFold\n",
        "    # !pip uninstall -y scikit-learn\n",
        "    # !pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n",
        "\n",
        "    # for MultilabelStratifiedKFold\n",
        "    # !pip install -q iterative-stratification\n",
        "\n",
        "    # for CosineAnnealingWarmupRestarts\n",
        "    # !pip install -qU 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
        "\n",
        "    !pip install -q wandb\n",
        "    # !pip install -q optuna\n",
        "\n",
        "    # ====================================================\n",
        "    # Competition specific libraries\n",
        "    # ====================================================\n",
        "    !pip install -q transformers\n",
        "    !pip install -q sentencepiece\n",
        "    # !pip install -q textstat\n",
        "    # !pip install -q nlpaug\n",
        "\n",
        "    !touch init.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXvMPm5_4h0"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB47hsio_5y6"
      },
      "source": [
        "# General libraries\n",
        "import collections\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import statistics\n",
        "import time\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.cuda.amp as amp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "# from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
        "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error, jaccard_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold  # , StratifiedGroupKFold\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyiVJefp4oOi"
      },
      "source": [
        "# Competition specific libraries\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import nlpaug.augmenter.sentence as nas\n",
        "# import nltk\n",
        "# import textstat\n",
        "import transformers as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-snxxwfCAO92"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxr17nzJAS2c"
      },
      "source": [
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxTu8mx-AXcw",
        "outputId": "91c53a42-6311-4356-877d-1e544c00bba6"
      },
      "source": [
        "netrc = \"/content/drive/MyDrive/.netrc\" if 'google.colab' in sys.modules else \"../input/wandbtoken/.netrc\"\n",
        "!cp -f {netrc} ~/\n",
        "!wandb login\n",
        "\n",
        "wandb_tags = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WavcpUepAQOs"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    wandb_tags.append(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_GYEnAnAqmJ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbzq6jTIAszZ"
      },
      "source": [
        "DATA_DIR = \"./\" if 'google.colab' in sys.modules else \"../input/chaii-hindi-and-tamil-question-answering/\"\n",
        "OUTPUT_DIR = \"./\"\n",
        "MODEL_DIR = \"./models/\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "!rm -rf {MODEL_DIR}/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ey9Vh4CBMgo"
      },
      "source": [
        "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
        "sub = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n",
        "\n",
        "#external_squad_translated_tamil = pd.read_csv(DATA_DIR + \"squad_translated_tamil.csv\")\n",
        "#external_mlqa = pd.read_csv(DATA_DIR + \"mlqa_hindi.csv\")\n",
        "#external_xquad = pd.read_csv(DATA_DIR + \"xquad.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2Lqm0KBeb_"
      },
      "source": [
        "#  Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsNi7VmB9_M",
        "outputId": "f77f1d6c-2d92-49c3-d717-578c9e2159c3"
      },
      "source": [
        "# seed = random.randrange(10000)\n",
        "seed = 440\n",
        "print(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDIEcAvEBdqj"
      },
      "source": [
        "class Config:\n",
        "    wandb_entity = \"imokuri\"\n",
        "    wandb_project = \"chaii\"\n",
        "    print_freq = 100\n",
        "\n",
        "    preprocess = False\n",
        "    train = True\n",
        "    validate = False\n",
        "    inference = False\n",
        "\n",
        "    debug = False\n",
        "    num_debug_data = 50\n",
        "\n",
        "    amp = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI5SEjO0CS8k"
      },
      "source": [
        "config_defaults = {\n",
        "    \"seed\": seed,\n",
        "    # \"n_class\": 1,\n",
        "    \"n_fold\": 5,\n",
        "    \"epochs\": 2,\n",
        "    \"batch_size\": 3,\n",
        "    \"gradient_accumulation_steps\": 7,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"criterion\": \"ChaiiCrossEntropyLoss\",\n",
        "    \"optimizer\": \"BertAdamW\",\n",
        "    \"scheduler\": \"get_cosine_schedule_with_warmup\",\n",
        "    \"max_lr\": 5e-5,\n",
        "    \"lr\": 2e-5,\n",
        "    \"min_lr\": 1e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    # \"model_name\": \"deepset/xlm-roberta-large-squad2\",\n",
        "    # \"model_name\": \"deepset/xlm-roberta-base-squad2\",\n",
        "    \"model_name\": \"google/rembert\",\n",
        "    \"model_class\": \"bare\", # bare, qa\n",
        "    \"max_len\": 384,\n",
        "    \"doc_stride\": 128,\n",
        "    \"dropout\": 0.0,\n",
        "    \"init_weights\": True,\n",
        "    \"init_layers\": 1,\n",
        "    # \"freeze_layers\": 0,\n",
        "    \"datasets\": [\n",
        "        \"mlqa:v1\",\n",
        "        \"xquad:v1\",\n",
        "        \"squad_translated_tamil:v1\",\n",
        "    ],\n",
        "    \"models\": [\n",
        "        \"base-models:v1\",\n",
        "    ]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Pk66rRDTGC"
      },
      "source": [
        "if Config.debug:\n",
        "    config_defaults[\"n_fold\"] = 3\n",
        "    config_defaults[\"epochs\"] = 1\n",
        "    Config.print_freq = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9WtvbYBtfN"
      },
      "source": [
        "if Config.train:\n",
        "    wandb_job_type = \"training\"\n",
        "\n",
        "elif Config.inference:\n",
        "    wandb_job_type = \"inference\"\n",
        "\n",
        "elif Config.validate:\n",
        "    wandb_job_type = \"validation\"\n",
        "\n",
        "elif Config.preprocess:\n",
        "    wandb_job_type = \"preprocess\"\n",
        "\n",
        "else:\n",
        "    wandb_job_type = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rVE3iK7Xaf"
      },
      "source": [
        "if Config.debug:\n",
        "    wandb_tags.append(\"debug\")\n",
        "    \n",
        "if Config.amp:\n",
        "    wandb_tags.append(\"amp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "9kskKEy9Dyqk",
        "outputId": "5b17f617-6992-41cf-8a5f-5be1ccac3c6a"
      },
      "source": [
        "if Config.debug:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        tags=wandb_tags,\n",
        "        mode=\"disabled\",\n",
        "    )\n",
        "else:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        job_type=wandb_job_type,\n",
        "        tags=wandb_tags,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">logical-monkey-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/imokuri/chaii\" target=\"_blank\">https://wandb.ai/imokuri/chaii</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/imokuri/chaii/runs/txirbquo\" target=\"_blank\">https://wandb.ai/imokuri/chaii/runs/txirbquo</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210915_015547-txirbquo</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev3bvwDvEMuS"
      },
      "source": [
        "config = wandb.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1pXRWoHy1_"
      },
      "source": [
        "# Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSYiJXBD4oy"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_data = []\n",
        "    for name_version in config.datasets:\n",
        "        name, version = name_version.split(\":\")\n",
        "        os.makedirs(name, exist_ok=True)\n",
        "\n",
        "        if Config.debug:\n",
        "            artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{name_version}\"\n",
        "            api = wandb.Api()\n",
        "            artifact = api.artifact(artifact_path)\n",
        "\n",
        "        else:\n",
        "            artifact_path = f\"{name_version}\"\n",
        "            artifact = run.use_artifact(artifact_path)\n",
        "\n",
        "        artifact.download(name)\n",
        "\n",
        "        df = pd.read_csv(f\"{name}/{name}.csv\")\n",
        "        external_data.append(df)\n",
        "\n",
        "    external_train = pd.concat(external_data)\n",
        "\n",
        "    external_train = external_train.drop_duplicates(keep=\"last\")\n",
        "    external_train = external_train.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh7C_jN8EGC"
      },
      "source": [
        "if Config.inference:\n",
        "    api = wandb.Api()\n",
        "    for artifact_id in config.models:\n",
        "        name_version = artifact_id.replace(\":\", \"-\")\n",
        "        if not os.path.exists(name_version):\n",
        "            os.makedirs(name_version)\n",
        "\n",
        "        for fold in range(config.n_fold):\n",
        "            try:\n",
        "                artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{artifact_id}\"\n",
        "                artifact = api.artifact(artifact_path)\n",
        "                artifact.download(name_version)\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {artifact_path}, {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JepYxTDVFarm"
      },
      "source": [
        "# EDA-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGORKcNtFcAk",
        "outputId": "fb96d1d0-5c23-4363-b185-2d33f6a9d693"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "id              0\n",
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "id          0\n",
            "context     0\n",
            "question    0\n",
            "language    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvCjtVwvGacB",
        "outputId": "0fa5c827-f57f-4bbc-b25f-5d1c2dce2636"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df[\"language\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "hindi    746\n",
            "tamil    368\n",
            "Name: language, dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    3\n",
            "tamil    2\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUXUouHDHa-i",
        "outputId": "14171c34-cc31-4e50-b589-171d86e00add"
      },
      "source": [
        "if config.datasets != []:\n",
        "    print(external_train.isnull().sum())\n",
        "    print(f\"=\" * 120)\n",
        "    print(external_train[\"language\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    6610\n",
            "tamil    3566\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW1WEUKWEPrV"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0_3tcXS_AEO"
      },
      "source": [
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mGJYUQ0liem"
      },
      "source": [
        "def correct_labels(df):\n",
        "    df.loc[df['id'] == '', 'answer_text'] = ''\n",
        "    df.loc[df['id'] == '', 'answer_start'] = 0\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdjatcP_FJ9r"
      },
      "source": [
        "def get_train_data(train):\n",
        "    train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n",
        "\n",
        "    return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-A8bRGjFVMw"
      },
      "source": [
        "def get_test_data(test):\n",
        "\n",
        "    return test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yae6ysRGvMH"
      },
      "source": [
        "train = get_train_data(train)\n",
        "\n",
        "if config.datasets != []:\n",
        "    external_train = get_train_data(external_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-okotQ7GwJT"
      },
      "source": [
        "test = get_test_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54joajJkBRbm"
      },
      "source": [
        "### External Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8RSTX1SBZef"
      },
      "source": [
        "# ÂâçÂá¶ÁêÜ\n",
        "if False and Config.preprocess:\n",
        "    external_squad_translated_tamil[\"language\"] = \"tamil\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0bvUgZyzasg"
      },
      "source": [
        "# dataset ‰øùÂ≠ò\n",
        "if False and Config.preprocess:\n",
        "    !mkdir -p squad_translated_tamil\n",
        "    external_squad_translated_tamil.to_csv(\"squad_translated_tamil/squad_translated_tamil.csv\", index=False)\n",
        "    artifact = wandb.Artifact('squad_translated_tamil', type='dataset')\n",
        "    artifact.add_dir(\"squad_translated_tamil/\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    !mkdir -p mlqa\n",
        "    external_mlqa.to_csv(\"mlqa/mlqa.csv\", index=False)\n",
        "    artifact = wandb.Artifact('mlqa', type='dataset')\n",
        "    artifact.add_dir(\"mlqa/\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    !mkdir -p xquad\n",
        "    external_xquad.to_csv(\"xquad/xquad.csv\", index=False)\n",
        "    artifact = wandb.Artifact('xquad', type='dataset')\n",
        "    artifact.add_dir(\"xquad/\")\n",
        "    run.log_artifact(artifact)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEkueCnpHpGW"
      },
      "source": [
        "# EDA-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CEM8_FTTHqrM",
        "outputId": "c692b8f7-4aae-4e55-a386-dc747c0ab4b6"
      },
      "source": [
        "for df in [train, test, sub]:\n",
        "    print(f\"=\" * 120)\n",
        "    df.info()\n",
        "    display(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1114 entries, 0 to 1113\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id            1114 non-null   object\n",
            " 1   context       1114 non-null   object\n",
            " 2   question      1114 non-null   object\n",
            " 3   answer_text   1114 non-null   object\n",
            " 4   answer_start  1114 non-null   int64 \n",
            " 5   language      1114 non-null   object\n",
            " 6   answers       1114 non-null   object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 61.0+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>903deec17</td>\n",
              "      <td>‡Æí‡Æ∞‡ØÅ ‡Æö‡Ææ‡Æ§‡Ææ‡Æ∞‡Æ£ ‡Æµ‡Æ≥‡Æ∞‡Øç‡Æ®‡Øç‡Æ§ ‡ÆÆ‡Æ©‡Æø‡Æ§‡Æ©‡ØÅ‡Æü‡Øà‡ÆØ ‡Æé‡Æ≤‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Øç‡Æï‡ØÇ‡Æü‡ØÅ ‡Æ™‡Æø‡Æ©...</td>\n",
              "      <td>‡ÆÆ‡Æ©‡Æø‡Æ§ ‡Æâ‡Æü‡Æ≤‡Æø‡Æ≤‡Øç ‡Æé‡Æ§‡Øç‡Æ§‡Æ©‡Øà ‡Æé‡Æ≤‡ØÅ‡ÆÆ‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æ©?</td>\n",
              "      <td>206</td>\n",
              "      <td>53</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [53], 'text': ['206']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d9841668c</td>\n",
              "      <td>‡Æï‡Ææ‡Æ≥‡Æø‡Æ§‡Ææ‡Æö‡Æ©‡Øç (‡Æ§‡Øá‡Æµ‡Æ®‡Ææ‡Æï‡Æ∞‡Æø: ‡§ï‡§æ‡§≤‡§ø‡§¶‡§æ‡§∏) ‡Æö‡ÆÆ‡Æ∏‡Øç‡Æï‡Æø‡Æ∞‡ØÅ‡Æ§ ‡Æá‡Æ≤‡Æï‡Øç‡Æï‡Æø...</td>\n",
              "      <td>‡Æï‡Ææ‡Æ≥‡Æø‡Æ§‡Ææ‡Æö‡Æ©‡Øç ‡Æé‡Æô‡Øç‡Æï‡ØÅ ‡Æ™‡Æø‡Æ±‡Æ®‡Øç‡Æ§‡Ææ‡Æ∞‡Øç?</td>\n",
              "      <td>‡Æï‡Ææ‡Æö‡ØÅ‡ÆÆ‡ØÄ‡Æ∞‡Æø‡Æ≤‡Øç</td>\n",
              "      <td>2358</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [2358], 'text': ['‡Æï‡Ææ‡Æö‡ØÅ‡ÆÆ‡ØÄ‡Æ∞‡Æø‡Æ≤‡Øç']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29d154b56</td>\n",
              "      <td>‡Æö‡Æ∞‡Øç ‡ÆÖ‡Æ≤‡ØÜ‡Æï‡Øç‡Æ∏‡Ææ‡Æ£‡Øç‡Æü‡Æ∞‡Øç ‡ÆÉ‡Æ™‡Æø‡Æ≥‡ØÜ‡ÆÆ‡Æø‡Æô‡Øç (Sir Alexander Flem...</td>\n",
              "      <td>‡Æ™‡ØÜ‡Æ©‡Øç‡Æö‡Æø‡Æ≤‡Æø‡Æ©‡Øç ‡Æï‡Æ£‡Øç‡Æü‡ØÅ‡Æ™‡Æø‡Æü‡Æø‡Æ§‡Øç‡Æ§‡Æµ‡Æ∞‡Øç ‡ÆØ‡Ææ‡Æ∞‡Øç?</td>\n",
              "      <td>‡Æö‡Æ∞‡Øç ‡ÆÖ‡Æ≤‡ØÜ‡Æï‡Øç‡Æ∏‡Ææ‡Æ£‡Øç‡Æü‡Æ∞‡Øç ‡ÆÉ‡Æ™‡Æø‡Æ≥‡ØÜ‡ÆÆ‡Æø‡Æô‡Øç</td>\n",
              "      <td>0</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [0], 'text': ['‡Æö‡Æ∞‡Øç ‡ÆÖ‡Æ≤‡ØÜ‡Æï‡Øç‡Æ∏‡Ææ‡Æ£‡Øç‡Æü...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41660850a</td>\n",
              "      <td>‡Æï‡ØÅ‡Æ¥‡Æ®‡Øç‡Æ§‡Øà‡ÆØ‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ¥‡ØÅ‡Æï‡Øà‡ÆØ‡Øà  ‡Æ®‡Æø‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡Æµ‡ØÅ‡ÆÆ‡Øç, ‡Æ§‡ØÇ‡Æô‡Øç‡Æï ‡Æµ‡Øà‡Æï‡Øç‡Æï‡Æµ...</td>\n",
              "      <td>‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ‡Æï‡ØÅ‡Æ¥‡Æ®‡Øç‡Æ§‡Øà‡Æï‡Æ≥‡Øà ‡Æ§‡ØÇ‡Æô‡Øç‡Æï ‡Æµ‡Øà‡Æï‡Øç‡Æï ‡Æ™‡Ææ‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Ææ...</td>\n",
              "      <td>‡Æ§‡Ææ‡Æ≤‡Ææ‡Æü‡Øç‡Æü‡ØÅ</td>\n",
              "      <td>68</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [68], 'text': ['‡Æ§‡Ææ‡Æ≤‡Ææ‡Æü‡Øç‡Æü‡ØÅ']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b29c82c22</td>\n",
              "      <td>‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æï‡Øç ‡Æï‡ØÅ‡Æü‡ØÅ‡ÆÆ‡Øç‡Æ™‡ÆÆ‡Øç \\n‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æï‡Øç ‡Æï‡ØÅ‡Æü‡ØÅ‡ÆÆ‡Øç‡Æ™‡ÆÆ‡Øç (Solar S...</td>\n",
              "      <td>‡Æ™‡ØÇ‡ÆÆ‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡Æµ‡Æø‡Æ£‡Øç‡ÆÆ‡ØÄ‡Æ©‡Øç ‡Æé‡Æ§‡ØÅ?</td>\n",
              "      <td>‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡ØÅ‡ÆÆ‡Øç</td>\n",
              "      <td>585</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [585], 'text': ['‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡ØÅ‡ÆÆ‡Øç']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            answers\n",
              "0  903deec17  ...            {'answer_start': [53], 'text': ['206']}\n",
              "1  d9841668c  ...   {'answer_start': [2358], 'text': ['‡Æï‡Ææ‡Æö‡ØÅ‡ÆÆ‡ØÄ‡Æ∞‡Æø‡Æ≤‡Øç']}\n",
              "2  29d154b56  ...  {'answer_start': [0], 'text': ['‡Æö‡Æ∞‡Øç ‡ÆÖ‡Æ≤‡ØÜ‡Æï‡Øç‡Æ∏‡Ææ‡Æ£‡Øç‡Æü...\n",
              "3  41660850a  ...       {'answer_start': [68], 'text': ['‡Æ§‡Ææ‡Æ≤‡Ææ‡Æü‡Øç‡Æü‡ØÅ']}\n",
              "4  b29c82c22  ...     {'answer_start': [585], 'text': ['‡Æö‡ØÇ‡Æ∞‡Æø‡ÆØ‡Æ©‡ØÅ‡ÆÆ‡Øç']}\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        5 non-null      object\n",
            " 1   context   5 non-null      object\n",
            " 2   question  5 non-null      object\n",
            " 3   language  5 non-null      object\n",
            "dtypes: object(4)\n",
            "memory usage: 288.0+ bytes\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ (‡§ú‡§®‡•ç‡§Æ: 7 ‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞ 1983; ‡§µ‡§∞‡•ç‡§ß‡§æ, ‡§Æ‡§π‡§æ...</td>\n",
              "      <td>‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§ï‡•Ä ‡§Æ‡§æ‡§Å ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>‡§ó‡•Ç‡§ó‡§≤ ‡§Æ‡§æ‡§®‡§ö‡§ø‡§§‡•ç‡§∞ (Google Maps) (‡§™‡•Ç‡§∞‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§ó‡•Ç‡§ó‡§≤ ‡§≤‡•ã...</td>\n",
              "      <td>‡§ó‡•Ç‡§ó‡§≤ ‡§Æ‡•à‡§™‡•ç‡§∏ ‡§ï‡§¨ ‡§≤‡•â‡§®‡•ç‡§ö ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>‡§ó‡•Å‡§∏‡•ç‡§§‡§æ‡§µ ‡§∞‡•â‡§¨‡§∞‡•ç‡§ü ‡§ï‡§ø‡§∞‡§ö‡•â‡§´‡§º (‡•ß‡•® ‡§Æ‡§æ‡§∞‡•ç‡§ö ‡•ß‡•Æ‡•®‡•™ - ‡•ß‡•≠ ‡§Ö‡§ï‡•ç...</td>\n",
              "      <td>‡§ó‡•Å‡§∏‡•ç‡§§‡§æ‡§µ ‡§ï‡§ø‡§∞‡§ö‡•â‡§´ ‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ ‡§ï‡§¨ ‡§π‡•Å‡§Ü ‡§•‡§æ?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>‡ÆÖ‡Æ≤‡ØÅ‡ÆÆ‡Æø‡Æ©‡Æø‡ÆØ‡ÆÆ‡Øç (‡ÆÜ‡Æô‡Øç‡Æï‡Æø‡Æ≤‡ÆÆ‡Øç: ‡ÆÖ‡Æ≤‡ØÅ‡ÆÆ‡Æø‡Æ©‡Æø‡ÆØ‡ÆÆ‡Øç; ‡Æµ‡Æü ‡ÆÖ‡ÆÆ‡ØÜ‡Æ∞‡Æø‡Æï‡Øç‡Æï ...</td>\n",
              "      <td>‡ÆÖ‡Æ≤‡ØÅ‡ÆÆ‡Æø‡Æ©‡Æø‡ÆØ‡Æ§‡Øç‡Æ§‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ£‡ØÅ ‡Æé‡Æ£‡Øç ‡Æé‡Æ©‡Øç‡Æ©?</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>‡Æï‡ØÇ‡Æü‡Øç‡Æü‡ØÅ‡Æ±‡Æµ‡ØÅ ‡Æá‡ÆØ‡Æï‡Øç‡Æï ‡Æµ‡Æ∞‡Æ≤‡Ææ‡Æ±‡ØÅ, ‡Æá‡Æô‡Øç‡Æï‡Æø‡Æ≤‡Ææ‡Æ®‡Øç‡Æ§‡ØÅ  ‡Æ®‡Ææ‡Æü‡Øç‡Æü‡Æø‡Æ≤‡Øç ...</td>\n",
              "      <td>‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ≤‡Øç ‡Æ™‡Æö‡ØÅ‡ÆÆ‡Øà ‡Æ™‡ØÅ‡Æ∞‡Æü‡Øç‡Æö‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æ§‡Æ®‡Øç‡Æ§‡Øà ‡Æé‡Æ©‡Øç‡Æ±‡ØÅ ‡Æï‡Æ∞‡ØÅ‡Æ§...</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... language\n",
              "0  22bff3dec  ...    hindi\n",
              "1  282758170  ...    hindi\n",
              "2  d60987e0e  ...    hindi\n",
              "3  f99c770dc  ...    tamil\n",
              "4  40dec1964  ...    tamil\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   id                5 non-null      object \n",
            " 1   PredictionString  0 non-null      float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 208.0+ bytes\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  PredictionString\n",
              "0  22bff3dec               NaN\n",
              "1  282758170               NaN\n",
              "2  d60987e0e               NaN\n",
              "3  f99c770dc               NaN\n",
              "4  40dec1964               NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODUeiq5P_O8s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyZ9-uMH2gK"
      },
      "source": [
        "# CV Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6xvuGssH-4"
      },
      "source": [
        "if Config.debug:\n",
        "    train = train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if config.datasets != []:\n",
        "        external_train = external_train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if len(sub) > Config.num_debug_data:\n",
        "        test = test.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "        sub = sub.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XnW0e1AH4Cn",
        "outputId": "3064ce40-0aff-457f-a42c-6b4f8008a7c8"
      },
      "source": [
        "Fold = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"language\"])):\n",
        "    train.loc[val_index, \"fold\"] = int(n)\n",
        "train[\"fold\"] = train[\"fold\"].astype(np.int8)\n",
        "print(train.groupby([\"fold\", \"language\"]).size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold  language\n",
            "0     hindi       149\n",
            "      tamil        74\n",
            "1     hindi       149\n",
            "      tamil        74\n",
            "2     hindi       149\n",
            "      tamil        74\n",
            "3     hindi       150\n",
            "      tamil        73\n",
            "4     hindi       149\n",
            "      tamil        73\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W902aP490hEk"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_train[\"fold\"] = -1\n",
        "    external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "    train = pd.concat([train, external_train]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0991CRIMH-"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veHPCKXQIJRv"
      },
      "source": [
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    LOGGER.info(f\"[{name}] start\")\n",
        "    yield\n",
        "    LOGGER.info(f\"[{name}] done in {time.time() - t0:.0f} s.\")\n",
        "\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_torch(seed=config.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTjVqALnIXKQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqzv3qSpITRU"
      },
      "source": [
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, df, model_name, include_labels=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = T.AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.features = []\n",
        "        if include_labels:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_train_features(row)\n",
        "        else:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_test_features(row)\n",
        "\n",
        "        self.include_labels = include_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        feature = self.features[item]\n",
        "\n",
        "        if self.include_labels:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                # 'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }\n",
        "\n",
        "    def prepare_train_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "        features = []\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "\n",
        "            input_ids = tokenized_example[\"input_ids\"][i]\n",
        "            attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "            feature['input_ids'] = input_ids\n",
        "            feature['attention_mask'] = attention_mask\n",
        "            feature['offset_mapping'] = offsets\n",
        "\n",
        "            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "            sequence_ids = tokenized_example.sequence_ids(i)\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in sequence_ids]\n",
        "\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = example[\"answers\"]\n",
        "\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != 1:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != 1:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    feature[\"start_position\"] = cls_index\n",
        "                    feature[\"end_position\"] = cls_index\n",
        "                else:\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    feature[\"start_position\"] = token_start_index - 1\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "            features.append(feature)\n",
        "        return features\n",
        "\n",
        "    def prepare_test_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        features = []\n",
        "        for i in range(len(tokenized_example[\"input_ids\"])):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "            feature['input_ids'] = tokenized_example['input_ids'][i]\n",
        "            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n",
        "            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n",
        "            features.append(feature)\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rjufTiUtJb",
        "outputId": "40f25929-bc0f-4983-ef7b-9436a765bf26"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    train_ds = BaseDataset(train, config.model_name)\n",
        "    print(train_ds[0])\n",
        "    print(len(train_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([   312, 106970,  51825,  73441,  10362,   6403,  18509,  10362, 106450,\n",
            "         27743,   4366,  69316,    628,    313,   6775, 153730, 193880, 148427,\n",
            "          9078, 106970,   6719,  50894,  10362, 106450,  27743, 239164,   5366,\n",
            "         62226,  96327,  33731,    579, 133155,  85841, 106450,  27743, 113280,\n",
            "         56807,  57308,   3590, 206274,  27548,  15360,  31618,    582, 226935,\n",
            "         47016,  10362, 106450,  27743,  62866,  25306, 136865,    572,  10501,\n",
            "        226935,  51825,  70679, 231833,  34513, 115952,  52560, 101230, 167313,\n",
            "        174699,    573, 124156, 108929, 239164,   7237,    572, 240622,  12995,\n",
            "           571,  53003,   3590, 162825, 226935,  47016, 106970,   4902,  15313,\n",
            "           571,   6775, 163564, 195732,   9758,  21631,  10362, 106450,  27743,\n",
            "           579,  10149,  39301,  14349,  45046,    582,  29685,   6775, 163564,\n",
            "        195732,  14809, 100914,  14968,   6140, 161449, 106450,  27743,  95917,\n",
            "        187538,   5366,  54124,    615,  73447,   9078,  37976,  10362, 106450,\n",
            "         27743, 198379,  63243,  10362, 106450, 159708,   3590,  66045,  35530,\n",
            "         75870,  12017,    571,  28197,  19262,  73447,   9078, 144770,  23158,\n",
            "        106450,  27743,    615, 113280,   6613,    589,  32303,  18148, 130655,\n",
            "         11766,  23375,  10362, 106450,  27743,   4366,  85190,   5366,   1293,\n",
            "        226935, 130655,  14225,  14968,   6140, 161449, 106450,  27743,   4366,\n",
            "          3000,  87996,   3590, 206274,  57575,  44994,    572, 106970, 206233,\n",
            "         63379, 172119,   6658,   1182,  10362, 106450,  27743,   4366,    579,\n",
            "         24929,   6140,   9296,  15473,  23562, 122901,  27743, 198379,   7730,\n",
            "        161444,    582,  69316,    615,  95531,  10362,  26460, 206233,  63379,\n",
            "         38485,    579,    664, 135136,    582,  10362, 106450,  27743,  57308,\n",
            "         31138,   1010,  98348,  10362, 106450,  27743,  57308,  31138,    579,\n",
            "        221524, 155882,    582,  86613,  56391, 193190,    572,    579,  13034,\n",
            "        228754, 140452,   4366, 160380, 113793,  70095,  11519,  95917,   7237,\n",
            "        140452,  62866,    573, 175090, 222479,    572,    582,    573,    573,\n",
            "           573, 206233,  63379,  38485,  10362, 106450,  27743,   4366,  18607,\n",
            "           573,    637,   9765, 194203,  23158, 106450,  27743,    579,  14667,\n",
            "           796,  22090,    582,    573,    573,    648,  21454,  34073,  23158,\n",
            "        106450,  27743,    579,  36874,    580,   3496,  22090,    582,   3944,\n",
            "           573,    680,   7756,  23172,  29940, 194203,  23158, 106450,  27743,\n",
            "           579, 234547,  22090,    582,   3944,    573,    737,  29942, 160044,\n",
            "         10362, 106450,  27743,    579,    588, 173426,    796,  22090,    582,\n",
            "           573,   7689,  13655,  29641,  10362, 106450,  27743,    579,    574,\n",
            "         47790,  35476,  22090,    582,    573,   9765, 202322,  28613,  23158,\n",
            "        106450,  27743,    579,  16546,   1426,    922,  22090,    582,  98348,\n",
            "         10362, 106450,  27743,   4366,  41949,    573,    851, 100914,  72074,\n",
            "         23172,  10362, 106450,  27743,    579,   1050, 174816,    582,    573,\n",
            "           814, 182835, 238652,  23172,  10362, 106450,  27743,    579,  11311,\n",
            "          3150,    582,   3944,    573,   4227,  37392, 123745, 106450,  27743,\n",
            "           579,  14855,  15900,  22090,    582,   3944,    573,    752,   7756,\n",
            "         34743,  10362, 106450,  27743,    579,    313]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start_position': tensor(29), 'end_position': tensor(29)}\n",
            "27346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBEpGYP-kjH",
        "outputId": "29a2ae07-54ae-4276-dd31-e2d63a6f5271"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    test_ds = BaseDataset(test, config.model_name, include_labels=False)\n",
        "    print(test_ds[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([   312, 140778,  94912,  13838, 105901,   1130, 123420,   1257,   7857,\n",
            "         19768,   1002,    313, 140778,  94912,  13838, 105901,    579, 101796,\n",
            "           581,    851, 138679,   5593,    615,  85347,  37468,    571,  48908,\n",
            "           582,   1430,  12834,  30019,   4317,  33700,  28667,   1702, 104421,\n",
            "          2897,   7896,   1768,    661,    573,    573, 113785,   3890,  19826,\n",
            "           573, 140778,  94912,  13838, 105901,   1257,  17128,    851, 138679,\n",
            "          5593,   1274,  85347,  37468,    571,  48908,    969,  12226,   3371,\n",
            "           661,  16778,  62875,  38314,    572, 194103,  70389,   7583,  34781,\n",
            "          1325,  90332,   8960, 110688,  34158,   1251,   1768,    661,  27282,\n",
            "         90332,   8960, 110688,  13838, 105901,  59941,  15870,   5751,    969,\n",
            "          7149,  16694,   9184,  15933,    847,   4661,   5901,  19546,  84724,\n",
            "           661, 140778,  94912,  13838, 105901,   1130, 113785,   3890,  51699,\n",
            "          8424,   1002,  31145,  53809,   1251,  18439,   1325,  65975,   3101,\n",
            "          1251,  15283,  30019,   4317,  33700,  28667,   1702,  15891,   2927,\n",
            "          2422,  28357,   3239,    661,    573,    573,   4527,  11325,   2132,\n",
            "         44802,    573,    760,  15202,   1130, 126751,   1251,   3392, 140778,\n",
            "         94912,  13838, 105901,   2059,  26140,    572,  37917,    572,  35749,\n",
            "        175399,   1251, 122194,  14445,  14033,   2927,  28357,   2839,   8525,\n",
            "          3371,    661,  26140,    572,  37917,    572,  35749, 175399,   5901,\n",
            "           847,  17035, 152837,  15891,   5602,  38458,  94922,   1768, 155014,\n",
            "          6760,  52511,   5219, 137768, 156126,   1251, 129238,   3239,   4264,\n",
            "          1002,    661,  59941,  15870,   1022,  15202,   1130, 126751,    969,\n",
            "         15283,  17330,   6430,    573, 139542,  30019,   4317,  33700,  28667,\n",
            "          1702,  49034,   3101, 231921,  38458,   3010,  54367,   1199,   9145,\n",
            "           661,  15202,   1743,    969, 140778,  94912,  13838, 105901,   2059,\n",
            "          1112,  15202,   1130, 126751,    969,  52305,  44802,    573, 139542,\n",
            "         30019,   4317,  33700,  28667,   1702,  49034,   3101, 231921,  38458,\n",
            "          3010,  54367,   1199,    661,  35910,  15202,  15283,  84430,   7583,\n",
            "         11222,    573,  76179,  36181,    847,   4661,  20952,   4838,  39763,\n",
            "           969, 103783,   1199,  17373,   3669,  10925, 137033,    847,  20952,\n",
            "          4838,  39763,  52305,  44802,    573, 139542,  30019,   4317,  33700,\n",
            "         28667,   1702,  49034,   3101, 231921,  38458,   3010,   1325,  11246,\n",
            "        239889,    573, 139542,  30019,   4317,  33700,  28667,   1702,  49034,\n",
            "          3101, 231921,  38458,   3010,    969,  54367, 114171,   1130,    661,\n",
            "         84430,   7583,  11222,    573,  76179,  36181,    847,   4661,  27282,\n",
            "        103783,   1199,  49576,  95246,   1911,  10213,   7792,  19024,   1199,\n",
            "           661,   2292,   1251,   1402,   7792, 117577,  52192,  15870, 140778,\n",
            "         94912,  13838, 105901,   2059, 137033,    847,    573, 139542,  93728,\n",
            "          2805,  97162,    969,  54367, 114171,   1130,    661,  61847,  19070,\n",
            "         20952,   4838,  39763,    847,   4661,    575, 109289, 140778,  94912,\n",
            "         13838, 105901,   2059,  76485,   3343,  20952,   4838,  39763,    969,\n",
            "          2422, 111620, 114171,   1130,   1325,   5901,   1130,  20952,   4838,\n",
            "         39763,    969,  17057,  97600,  23749,    313]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'offset_mapping': [(0, 0), (0, 2), (2, 6), (6, 9), (9, 13), (13, 16), (16, 20), (20, 23), (23, 27), (27, 32), (32, 35), (0, 0), (0, 2), (2, 6), (6, 9), (9, 13), (13, 15), (15, 19), (19, 20), (20, 22), (22, 29), (29, 34), (34, 35), (35, 39), (39, 41), (41, 42), (42, 53), (53, 54), (54, 57), (57, 64), (64, 67), (67, 68), (68, 70), (70, 72), (72, 73), (73, 76), (76, 78), (78, 80), (80, 84), (84, 85), (85, 86), (86, 87), (87, 95), (95, 97), (97, 102), (102, 103), (103, 106), (106, 110), (110, 113), (113, 117), (117, 120), (120, 125), (125, 127), (127, 134), (134, 139), (139, 142), (142, 146), (146, 148), (148, 149), (149, 160), (160, 164), (164, 168), (168, 171), (171, 172), (172, 177), (177, 182), (182, 185), (185, 186), (186, 194), (194, 198), (198, 199), (199, 201), (201, 204), (204, 208), (208, 211), (211, 213), (213, 217), (217, 220), (220, 224), (224, 225), (225, 230), (230, 234), (234, 237), (237, 239), (239, 242), (242, 246), (246, 251), (251, 255), (255, 260), (260, 264), (264, 269), (269, 272), (272, 274), (274, 277), (277, 280), (280, 284), (284, 289), (289, 292), (292, 296), (296, 297), (297, 300), (300, 304), (304, 307), (307, 311), (311, 314), (314, 322), (322, 324), (324, 328), (328, 330), (330, 333), (333, 335), (335, 339), (339, 342), (342, 346), (346, 349), (349, 353), (353, 354), (354, 357), (357, 366), (366, 369), (369, 370), (370, 372), (372, 374), (374, 375), (375, 379), (379, 381), (381, 384), (384, 389), (389, 394), (394, 395), (395, 396), (396, 397), (397, 399), (399, 400), (400, 401), (401, 404), (404, 405), (405, 408), (408, 412), (412, 415), (415, 420), (420, 423), (423, 426), (426, 429), (429, 433), (433, 436), (436, 440), (440, 443), (443, 446), (446, 447), (447, 449), (449, 450), (450, 453), (453, 455), (455, 458), (458, 464), (464, 467), (467, 470), (470, 472), (472, 477), (477, 480), (480, 485), (485, 488), (488, 489), (489, 492), (492, 493), (493, 495), (495, 496), (496, 499), (499, 501), (501, 506), (506, 509), (509, 514), (514, 519), (519, 523), (523, 527), (527, 529), (529, 533), (533, 537), (537, 545), (545, 547), (547, 550), (550, 551), (551, 557), (557, 564), (564, 567), (567, 576), (576, 581), (581, 585), (585, 588), (588, 589), (589, 594), (594, 598), (598, 601), (601, 605), (605, 608), (608, 613), (613, 617), (617, 626), (626, 629), (629, 631), (631, 632), (632, 637), (637, 640), (640, 641), (641, 643), (643, 645), (645, 646), (646, 649), (649, 650), (650, 654), (654, 656), (656, 657), (657, 661), (661, 662), (662, 665), (665, 666), (666, 670), (670, 675), (675, 679), (679, 682), (682, 686), (686, 689), (689, 693), (693, 696), (696, 699), (699, 703), (703, 706), (706, 711), (711, 715), (715, 719), (719, 722), (722, 723), (723, 728), (728, 731), (731, 732), (732, 734), (734, 736), (736, 737), (737, 740), (740, 741), (741, 745), (745, 747), (747, 748), (748, 752), (752, 753), (753, 754), (754, 758), (758, 762), (762, 771), (771, 775), (775, 776), (776, 778), (778, 779), (779, 782), (782, 785), (785, 788), (788, 792), (792, 794), (794, 795), (795, 798), (798, 802), (802, 807), (807, 808), (808, 812), (812, 814), (814, 818), (818, 826), (826, 829), (829, 831), (831, 832), (832, 835), (835, 839), (839, 842), (842, 843), (843, 848), (848, 851), (851, 852), (852, 854), (854, 856), (856, 857), (857, 860), (860, 861), (861, 865), (865, 867), (867, 868), (868, 871), (871, 874), (874, 878), (878, 879), (879, 884), (884, 887), (887, 888), (888, 890), (890, 892), (892, 893), (893, 896), (896, 897), (897, 901), (901, 903), (903, 904), (904, 908), (908, 912), (912, 918), (918, 921), (921, 922), (922, 926), (926, 927), (927, 929), (929, 930), (930, 933), (933, 936), (936, 939), (939, 943), (943, 948), (948, 953), (953, 954), (954, 959), (959, 963), (963, 964), (964, 968), (968, 971), (971, 974), (974, 975), (975, 976), (976, 981), (981, 984), (984, 989), (989, 992), (992, 999), (999, 1003), (1003, 1007), (1007, 1010), (1010, 1014), (1014, 1017), (1017, 1021), (1021, 1024), (1024, 1032), (1032, 1035), (1035, 1036), (1036, 1041), (1041, 1045), (1045, 1046), (1046, 1058), (1058, 1062), (1062, 1066), (1066, 1072), (1072, 1075), (1075, 1076), (1076, 1079), (1079, 1085), (1085, 1087), (1087, 1088), (1088, 1091), (1091, 1094), (1094, 1098), (1098, 1099), (1099, 1102), (1102, 1105), (1105, 1109), (1109, 1112), (1112, 1116), (1116, 1119), (1119, 1125), (1125, 1127), (1127, 1129), (1129, 1130), (1130, 1133), (1133, 1137), (1137, 1140), (1140, 1146), (1146, 1152), (1152, 1155), (1155, 1158), (1158, 1163), (1163, 1166), (1166, 1168), (1168, 1169), (1169, 1172), (1172, 1176), (1176, 1181), (1181, 1187), (1187, 1189), (0, 0)], 'sequence_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'id': '22bff3dec', 'context': '‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ (‡§ú‡§®‡•ç‡§Æ: 7 ‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞ 1983; ‡§µ‡§∞‡•ç‡§ß‡§æ, ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞) ‡§è‡§ï ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡•Ä ‡§π‡•à‡§Ç‡•§ \\n ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§ú‡•Ä‡§µ‡§® \\n‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§ï‡§æ ‡§ú‡§®‡•ç‡§Æ 7 ‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞ 1983 ‡§ï‡•ã ‡§µ‡§∞‡•ç‡§ß‡§æ, ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§π‡•Å‡§Ü ‡§•‡§æ‡•§ ‡§â‡§®‡§ï‡•á ‡§™‡§ø‡§§‡§æ ‡§è‡§Æ. ‡§ï‡•ç‡§∞‡§æ‡§Ç‡§§‡§ø ‡§§‡•á‡§≤‡•Å‡§ó‡•Å ‡§î‡§∞ ‡§Æ‡§æ‡§Ç ‡§Ø‡•á‡§≤‡§® ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§π‡•à‡§Ç‡•§ ‡§â‡§®‡§ï‡•Ä ‡§Æ‡§æ‡§Ç ‡§Ø‡•á‡§≤‡§® ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§™‡§π‡§≤‡•Ä ‡§¨‡§æ‡§∞ 1977 ‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•á ‡§¶‡§æ‡§¶‡§æ ‡§ú‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≠‡§æ‡§∞‡§§ ‡§Ü‡§à ‡§•‡•Ä‡§Ç‡•§ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§™‡§¢‡§º‡§æ‡§à ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§∏‡•á ‡§π‡•Å‡§à ‡§î‡§∞ ‡§Ø‡§π‡•Ä‡§Ç ‡§∏‡•á ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡•á‡§≤‡§®‡§æ ‡§≠‡•Ä ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§ø‡§Ø‡§æ‡•§ \\n ‡§ï‡•Ö‡§∞‡§ø‡§Ø‡§∞ \\n10 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§∞ ‡§∏‡•á ‡§π‡•Ä ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á ‡§è‡§∏.‡§è‡§Æ. ‡§Ü‡§∞‡§ø‡§´ ‡§∏‡•á ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§≤‡•á‡§®‡§æ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ ‡§•‡§æ‡•§ ‡§è‡§∏.‡§è‡§Æ. ‡§Ü‡§∞‡§ø‡§´ ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§ú‡§æ‡§®‡•á ‡§Æ‡§æ‡§®‡•á ‡§ñ‡•á‡§≤ ‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à‡§Ç ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§¶‡•ç‡§∞‡•ã‡§£‡§æ‡§ö‡§æ‡§∞‡•ç‡§Ø ‡§Ö‡§µ‡§æ‡§∞‡•ç‡§° ‡§∏‡•á ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§ ‡§™‡§π‡§≤‡•Ä ‡§¨‡§æ‡§∞ 13 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§Æ‡§ø‡§®‡•Ä ‡§®‡•á‡§∂‡§®‡§≤ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™ ‡§ú‡•Ä‡§§‡•Ä ‡§•‡•Ä‡•§ ‡§∏‡§æ‡§≤ 2000 ‡§Æ‡•á‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á 17 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§â‡§Æ‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§ú‡•Ç‡§®‡§ø‡§Ø‡§∞ ‡§®‡•á‡§∂‡§®‡§≤ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™ ‡§ú‡•Ä‡§§‡•Ä‡•§ ‡§á‡§∏‡•Ä ‡§∏‡§æ‡§≤ ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§∂‡•ç‡§∞‡•Å‡§§‡§ø ‡§ï‡•Å‡§∞‡§ø‡§Ø‡§® ‡§ï‡•á ‡§∏‡§æ‡§• ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§ú‡•ã‡§°‡§º‡•Ä ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•Å‡§è ‡§Æ‡§π‡§ø‡§≤‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§ú‡•Ç‡§®‡§ø‡§Ø‡§∞ ‡§®‡•á‡§∂‡§®‡§≤ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™ ‡§î‡§∞ ‡§∏‡•Ä‡§®‡§ø‡§Ø‡§∞ ‡§®‡•á‡§∂‡§®‡§≤ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™ ‡§Æ‡•á‡§Ç ‡§ú‡•Ä‡§§ ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ï‡•Ä‡•§ ‡§∂‡•ç‡§∞‡•Å‡§§‡§ø ‡§ï‡•Å‡§∞‡§ø‡§Ø‡§® ‡§ï‡•á ‡§∏‡§æ‡§• ‡§â‡§®‡§ï‡•Ä ‡§ú‡•ã‡§°‡§º‡•Ä ‡§ï‡§æ‡§´‡•Ä ‡§≤‡§Ç‡§¨‡•á ‡§∏‡§Æ‡§Ø ‡§§‡§ï ‡§ö‡§≤‡•Ä‡•§ 2002 ‡§∏‡•á 2008 ‡§§‡§ï ‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§∏‡§æ‡§§ ‡§¨‡§æ‡§∞ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á ‡§Æ‡§π‡§ø‡§≤‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§®‡•á‡§∂‡§®‡§≤ ‡§Ø‡•Å‡§ó‡§≤ ‡§™‡•ç‡§∞‡§§‡§ø‡§Ø‡•ã‡§ó‡§ø‡§§‡§æ ‡§Æ‡•á‡§Ç ‡§ú‡•Ä‡§§ ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ï‡•Ä‡•§[2]\\n‡§Æ‡§π‡§ø‡§≤‡§æ ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§•-‡§∏‡§æ‡§• ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á ‡§Æ‡§ø‡§∂‡•ç‡§∞‡§ø‡§§ ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§∏‡§´‡§≤‡§§‡§æ ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ï‡•Ä ‡§î‡§∞ ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§∏‡§¨‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§¨‡§®‡•Ä‡§Ç‡•§[3] 2010 ‡§ï‡•â‡§Æ‡§®‡§µ‡•á‡§≤‡•ç‡§• ‡§ó‡•á‡§Æ‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§™‡§æ‡§∞‡•ç‡§ü‡§®‡§∞ ‡§Ö‡§∂‡•ç‡§µ‡§ø‡§®‡•Ä ‡§™‡•ã‡§®‡§™‡•ç‡§™‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§µ‡§∞‡•ç‡§£ ‡§™‡§¶‡§ï ‡§ú‡•Ä‡§§‡§æ‡•§ ‡§ï‡•â‡§Æ‡§®‡§µ‡•á‡§≤‡•ç‡§• ‡§ó‡•á‡§Æ‡•ç‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡•á ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§´‡§ø‡§∞ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§Æ‡•á‡§Ç ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§æ ‡§µ‡§ø‡§∑‡§Ø ‡§¨‡§® ‡§ó‡§à ‡§π‡•à‡§Ç‡•§[4][5]\\n‡§ó‡•ç‡§≤‡§æ‡§∏‡§ó‡•ã ‡§Æ‡•á‡§Ç ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§ï‡•â‡§Æ‡§®‡§µ‡•á‡§≤‡•ç‡§• ‡§ó‡•á‡§Æ‡•ç‡§∏, 2014 ‡§Æ‡•á‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§®‡•á ‡§∏‡•ç‡§µ‡§∞‡•ç‡§£ ‡§™‡§¶‡§ï ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ï‡§ø‡§Ø‡§æ‡•§\\n ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡§ó‡§§ ‡§ú‡•Ä‡§µ‡§® \\n‡§Æ‡•à‡§¶‡§æ‡§® ‡§™‡§∞ ‡§¨‡§æ‡§è‡§Ç ‡§π‡§æ‡§• ‡§∏‡•á ‡§§‡•á‡§ú-‡§§‡§∞‡•ç‡§∞‡§æ‡§∞ ‡§∂‡•â‡§ü ‡§≤‡§ó‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§®‡§ø‡§ú‡•Ä ‡§ú‡§ø‡§Ç‡§¶‡§ó‡•Ä ‡§Æ‡•á‡§Ç ‡§≠‡•Ä ‡§ï‡§æ‡§´‡•Ä ‡§§‡•á‡§ú ‡§î‡§∞ ‡§ö‡§∞‡•ç‡§ö‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§õ‡§æ‡§à ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§®‡•á 2005 ‡§Æ‡•á‡§Ç ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ö‡•á‡§§‡§® ‡§Ü‡§®‡§Ç‡§¶ ‡§∏‡•á ‡§∂‡§æ‡§¶‡•Ä ‡§ï‡•Ä ‡§•‡•Ä, 29 ‡§ú‡•Ç‡§® 2011 ‡§ï‡•ã ‡§â‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§™‡§§‡§ø ‡§™‡•Ç‡§∞‡•ç‡§µ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§ö‡•á‡§§‡§® ‡§Ü‡§®‡§Ç‡§¶ ‡§∏‡•á ‡§§‡§≤‡§æ‡§ï ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§ö‡•á‡§§‡§® ‡§Ü‡§®‡§Ç‡§¶ ‡§≠‡•Ä ‡§è‡§ï ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä ‡§π‡•à‡§Ç‡•§\\n ‡§´‡§ø‡§≤‡•ç‡§Æ‡•ã‡§ó‡•ç‡§∞‡§æ‡§´‡•Ä \\nGunde Jaari Gallanthayyinde[6] \\n‡§´‡•Å‡§ó‡§≤‡•Ä (2014)\\n ‡§â‡§™‡§≤‡§¨‡•ç‡§ß‡§ø‡§Ø‡§æ‡§Ç \\n‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° 13 ‡§¨‡§æ‡§∞ ‡§®‡•á‡§∂‡§®‡§≤ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ö‡•à‡§Ç‡§™‡§ø‡§Ø‡§®‡§∂‡§ø‡§™ ‡§ï‡•Ä ‡§µ‡§ø‡§ú‡•á‡§§‡§æ‡•§ \\n‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∏‡§¨‡§∏‡•á ‡§¨‡•á‡§π‡§§‡§∞‡•Ä‡§® ‡§°‡§¨‡§≤‡•ç‡§∏ ‡§™‡•ç‡§≤‡•á‡§Ø‡§∞‡•§ \\n‡§∏‡§æ‡§≤ 2011 ‡§Æ‡•á‡§Ç ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‚Äú‡§Ö‡§∞‡•ç‡§ú‡•Å‡§® ‡§™‡•Å‡§∞‡§∏‡•ç‡§ï‡§æ‡§∞‚Äù ‡§∏‡•á ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§®‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ \\n‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§Æ‡§Ç‡§°‡§≤ ‡§ñ‡•á‡§≤, 2014 (‡§ó‡•ç‡§≤‡§æ‡§∏‡§ó‡•ã) ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§µ‡§∞‡•ç‡§£ ‡§™‡§¶‡§ï ‡§ú‡•Ä‡§§‡§æ‡•§ \\n ‡§ö‡§ø‡§§‡•ç‡§∞ ‡§¶‡•Ä‡§∞‡•ç‡§ò‡§æ \\n\\n\\n‡§µ‡•Ä ‡§¶‡•Ä‡§ú‡•Ç ‡§î‡§∞ ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ\\n‡§ï‡•á‡§¨‡•Ä‡§∏‡•Ä ‡§ï‡•á ‡§∏‡•á‡§ü ‡§™‡§∞ ‡§∏‡•Å‡§∂‡•Ä‡§≤ ‡§ï‡•Å‡§Æ‡§æ‡§∞, ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ, ‡§≤‡§ø‡§è‡§Ç‡§°‡§∞ ‡§™‡•á‡§∏, ‡§∂‡•ç‡§∞‡•Ä‡§∏‡§Ç‡§§\\n‡§ï‡•á‡§¨‡•Ä‡§∏‡•Ä ‡§ï‡•á ‡§∏‡•á‡§ü ‡§™‡§∞ ‡§∏‡•Å‡§∂‡•Ä‡§≤ ‡§ï‡•Å‡§Æ‡§æ‡§∞, ‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ, ‡§≤‡§ø‡§è‡§Ç‡§°‡§∞ ‡§™‡•á‡§∏, ‡§∂‡•ç‡§∞‡•Ä‡§∏‡§Ç‡§§\\n\\n ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ \\n\\n ‡§¨‡§æ‡§π‡§∞‡•Ä ‡§ï‡§°‡§º‡§ø‡§Ø‡§æ‡§Å \\n\\n\\n\\n\\n\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§π‡§ø‡§®‡•ç‡§¶ ‡§ï‡•Ä ‡§¨‡•á‡§ü‡§ø‡§Ø‡§æ‡§Å\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§µ‡§ø‡§ï‡§ø‡§™‡§∞‡§ø‡§Ø‡•ã‡§ú‡§®‡§æ ‡§π‡§ø‡§®‡•ç‡§¶ ‡§ï‡•Ä ‡§¨‡•á‡§ü‡§ø‡§Ø‡§æ‡§Å\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:1983 ‡§Æ‡•á‡§Ç ‡§ú‡§®‡•ç‡§Æ‡•á ‡§≤‡•ã‡§ó\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§ú‡•Ä‡§µ‡§ø‡§§ ‡§≤‡•ã‡§ó\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§Æ‡§Ç‡§°‡§≤ ‡§ñ‡•á‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§™‡§¶‡§ï ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§‡§ï‡§∞‡•ç‡§§‡§æ\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§≤‡•ã‡§ó\\n‡§∂‡•ç‡§∞‡•á‡§£‡•Ä:‡§¨‡•à‡§°‡§Æ‡§ø‡§Ç‡§ü‡§® ‡§ñ‡§ø‡§≤‡§æ‡§°‡§º‡•Ä', 'question': '‡§ú‡•ç‡§µ‡§æ‡§≤‡§æ ‡§ó‡•Å‡§ü‡•ç‡§ü‡§æ ‡§ï‡•Ä ‡§Æ‡§æ‡§Å ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4zuBuCCAv-8"
      },
      "source": [
        "# üöó Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tJieX2AAueP",
        "outputId": "7d95c258-3248-4690-fa1f-5eeb1cded625"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    print(T.AutoConfig.from_pretrained(config.model_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RemBertConfig {\n",
            "  \"_name_or_path\": \"artefacts/pt_model\",\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 312,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"embedding_dropout_prob\": 0,\n",
            "  \"embedding_size\": 256,\n",
            "  \"eos_token_id\": 313,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1152,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"input_embedding_size\": 256,\n",
            "  \"intermediate_size\": 4608,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"rembert\",\n",
            "  \"num_attention_heads\": 18,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"output_embedding_size\": 1664,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250300\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZtghUknA1b8"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.auto_config = T.AutoConfig.from_pretrained(model_name)\n",
        "        self.auto_config.update({\n",
        "            \"hidden_dropout_prob\": config.dropout,\n",
        "            \"layer_norm_eps\": 1e-7,\n",
        "        })\n",
        "\n",
        "        self.auto_model = T.AutoModel.from_pretrained(model_name, config=self.auto_config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Linear(self.auto_config.hidden_size, 2)\n",
        "\n",
        "        if config.init_weights:\n",
        "            self._init_weights(self.qa_outputs)\n",
        "\n",
        "        if config.init_layers > 0:\n",
        "            self._init_layers()\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.auto_config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def _init_layers(self):\n",
        "        # re-init pooler\n",
        "        # self.auto_model.pooler.dense.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "        # self.auto_model.pooler.dense.bias.data.zero_()\n",
        "        # for p in self.auto_model.pooler.parameters():\n",
        "        #     p.requires_grad = True\n",
        "\n",
        "        # re-init encoder\n",
        "        layers = self.auto_model.encoder.layer[-config.init_layers:]\n",
        "        for layer in layers:\n",
        "            for module in layer.modules():\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                    # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                    module.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "                    if module.bias is not None:\n",
        "                        module.bias.data.zero_()\n",
        "                elif isinstance(module, nn.Embedding):\n",
        "                    module.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "                    if module.padding_idx is not None:\n",
        "                        module.weight.data[module.padding_idx].zero_()\n",
        "                elif isinstance(module, nn.LayerNorm):\n",
        "                    module.bias.data.zero_()\n",
        "                    module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "    ):\n",
        "        outputs = self.auto_model(\n",
        "            input_ids,\n",
        "            attention_mask,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = outputs[0]\n",
        "        \n",
        "        qa_logits = self.qa_outputs(last_hidden_state)\n",
        "        \n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1cZpfnGmuyI"
      },
      "source": [
        "class QAModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.auto_config = T.AutoConfig.from_pretrained(model_name)\n",
        "        self.auto_config.update({\n",
        "            \"hidden_dropout_prob\": config.dropout,\n",
        "            # \"layer_norm_eps\": 1e-7,\n",
        "        })\n",
        "\n",
        "        self.auto_model = T.AutoModelForQuestionAnswering.from_pretrained(model_name, config=self.auto_config)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "    ):\n",
        "        outputs = self.auto_model(\n",
        "            input_ids,\n",
        "            attention_mask,\n",
        "        )\n",
        "\n",
        "        return outputs.start_logits, outputs.end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M92xPyOyCSQZ",
        "outputId": "7b613e99-3d18-429f-adfb-f8d55c2aea8f"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    if config.model_class == \"bare\":\n",
        "        model = BaseModel(config.model_name)\n",
        "    elif config.model_class == \"qa\":\n",
        "        model = QAModel(config.model_name)\n",
        "    print(model)\n",
        "\n",
        "    train_dataset = BaseDataset(train, config.model_name)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    for features in train_loader:\n",
        "        output = model(features[\"input_ids\"], features[\"attention_mask\"])\n",
        "        print(output)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RemBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RemBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseModel(\n",
            "  (auto_model): RemBertModel(\n",
            "    (embeddings): RemBertEmbeddings(\n",
            "      (word_embeddings): Embedding(250300, 256, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 256)\n",
            "      (token_type_embeddings): Embedding(2, 256)\n",
            "      (LayerNorm): LayerNorm((256,), eps=1e-07, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): RemBertEncoder(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=256, out_features=1152, bias=True)\n",
            "      (layer): ModuleList(\n",
            "        (0): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (24): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (25): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (26): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (27): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (28): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (29): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (30): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (31): RemBertLayer(\n",
            "          (attention): RemBertAttention(\n",
            "            (self): RemBertSelfAttention(\n",
            "              (query): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (key): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (value): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "            (output): RemBertSelfOutput(\n",
            "              (dense): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "              (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RemBertIntermediate(\n",
            "            (dense): Linear(in_features=1152, out_features=4608, bias=True)\n",
            "          )\n",
            "          (output): RemBertOutput(\n",
            "            (dense): Linear(in_features=4608, out_features=1152, bias=True)\n",
            "            (LayerNorm): LayerNorm((1152,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=1152, out_features=2, bias=True)\n",
            ")\n",
            "(tensor([[ 0.8883,  0.0826,  1.0008,  ...,  0.7417, -0.2556,  0.0323],\n",
            "        [ 0.4524,  1.7093,  0.4704,  ...,  0.7154,  0.9816,  0.2316],\n",
            "        [ 0.9052,  0.5363, -0.1640,  ...,  0.4522,  1.0466,  0.0984],\n",
            "        [ 1.0439,  1.3129,  0.3864,  ...,  1.7583,  1.1712,  0.2623]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 0.8944, -0.4392,  0.4604,  ...,  0.8303, -0.1268,  0.4117],\n",
            "        [ 0.0905,  1.1819,  0.9191,  ..., -0.2783, -0.3356,  0.4450],\n",
            "        [ 0.9261,  0.9710,  0.0519,  ...,  0.5110, -0.3522,  0.4320],\n",
            "        [ 0.7981, -0.0720, -0.2747,  ...,  0.0550,  0.0522,  0.3958]],\n",
            "       grad_fn=<SqueezeBackward1>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wEQxmYCwUr",
        "outputId": "a8732c26-5581-40df-d0bf-c3e9a2227f31"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    for n, (name, tensor) in enumerate(list(model.named_parameters())):\n",
        "        print(f\"{n:>4}: {tensor.requires_grad}, {name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0: True, auto_model.embeddings.word_embeddings.weight\n",
            "   1: True, auto_model.embeddings.position_embeddings.weight\n",
            "   2: True, auto_model.embeddings.token_type_embeddings.weight\n",
            "   3: True, auto_model.embeddings.LayerNorm.weight\n",
            "   4: True, auto_model.embeddings.LayerNorm.bias\n",
            "   5: True, auto_model.encoder.embedding_hidden_mapping_in.weight\n",
            "   6: True, auto_model.encoder.embedding_hidden_mapping_in.bias\n",
            "   7: True, auto_model.encoder.layer.0.attention.self.query.weight\n",
            "   8: True, auto_model.encoder.layer.0.attention.self.query.bias\n",
            "   9: True, auto_model.encoder.layer.0.attention.self.key.weight\n",
            "  10: True, auto_model.encoder.layer.0.attention.self.key.bias\n",
            "  11: True, auto_model.encoder.layer.0.attention.self.value.weight\n",
            "  12: True, auto_model.encoder.layer.0.attention.self.value.bias\n",
            "  13: True, auto_model.encoder.layer.0.attention.output.dense.weight\n",
            "  14: True, auto_model.encoder.layer.0.attention.output.dense.bias\n",
            "  15: True, auto_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "  16: True, auto_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "  17: True, auto_model.encoder.layer.0.intermediate.dense.weight\n",
            "  18: True, auto_model.encoder.layer.0.intermediate.dense.bias\n",
            "  19: True, auto_model.encoder.layer.0.output.dense.weight\n",
            "  20: True, auto_model.encoder.layer.0.output.dense.bias\n",
            "  21: True, auto_model.encoder.layer.0.output.LayerNorm.weight\n",
            "  22: True, auto_model.encoder.layer.0.output.LayerNorm.bias\n",
            "  23: True, auto_model.encoder.layer.1.attention.self.query.weight\n",
            "  24: True, auto_model.encoder.layer.1.attention.self.query.bias\n",
            "  25: True, auto_model.encoder.layer.1.attention.self.key.weight\n",
            "  26: True, auto_model.encoder.layer.1.attention.self.key.bias\n",
            "  27: True, auto_model.encoder.layer.1.attention.self.value.weight\n",
            "  28: True, auto_model.encoder.layer.1.attention.self.value.bias\n",
            "  29: True, auto_model.encoder.layer.1.attention.output.dense.weight\n",
            "  30: True, auto_model.encoder.layer.1.attention.output.dense.bias\n",
            "  31: True, auto_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "  32: True, auto_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "  33: True, auto_model.encoder.layer.1.intermediate.dense.weight\n",
            "  34: True, auto_model.encoder.layer.1.intermediate.dense.bias\n",
            "  35: True, auto_model.encoder.layer.1.output.dense.weight\n",
            "  36: True, auto_model.encoder.layer.1.output.dense.bias\n",
            "  37: True, auto_model.encoder.layer.1.output.LayerNorm.weight\n",
            "  38: True, auto_model.encoder.layer.1.output.LayerNorm.bias\n",
            "  39: True, auto_model.encoder.layer.2.attention.self.query.weight\n",
            "  40: True, auto_model.encoder.layer.2.attention.self.query.bias\n",
            "  41: True, auto_model.encoder.layer.2.attention.self.key.weight\n",
            "  42: True, auto_model.encoder.layer.2.attention.self.key.bias\n",
            "  43: True, auto_model.encoder.layer.2.attention.self.value.weight\n",
            "  44: True, auto_model.encoder.layer.2.attention.self.value.bias\n",
            "  45: True, auto_model.encoder.layer.2.attention.output.dense.weight\n",
            "  46: True, auto_model.encoder.layer.2.attention.output.dense.bias\n",
            "  47: True, auto_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "  48: True, auto_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "  49: True, auto_model.encoder.layer.2.intermediate.dense.weight\n",
            "  50: True, auto_model.encoder.layer.2.intermediate.dense.bias\n",
            "  51: True, auto_model.encoder.layer.2.output.dense.weight\n",
            "  52: True, auto_model.encoder.layer.2.output.dense.bias\n",
            "  53: True, auto_model.encoder.layer.2.output.LayerNorm.weight\n",
            "  54: True, auto_model.encoder.layer.2.output.LayerNorm.bias\n",
            "  55: True, auto_model.encoder.layer.3.attention.self.query.weight\n",
            "  56: True, auto_model.encoder.layer.3.attention.self.query.bias\n",
            "  57: True, auto_model.encoder.layer.3.attention.self.key.weight\n",
            "  58: True, auto_model.encoder.layer.3.attention.self.key.bias\n",
            "  59: True, auto_model.encoder.layer.3.attention.self.value.weight\n",
            "  60: True, auto_model.encoder.layer.3.attention.self.value.bias\n",
            "  61: True, auto_model.encoder.layer.3.attention.output.dense.weight\n",
            "  62: True, auto_model.encoder.layer.3.attention.output.dense.bias\n",
            "  63: True, auto_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "  64: True, auto_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "  65: True, auto_model.encoder.layer.3.intermediate.dense.weight\n",
            "  66: True, auto_model.encoder.layer.3.intermediate.dense.bias\n",
            "  67: True, auto_model.encoder.layer.3.output.dense.weight\n",
            "  68: True, auto_model.encoder.layer.3.output.dense.bias\n",
            "  69: True, auto_model.encoder.layer.3.output.LayerNorm.weight\n",
            "  70: True, auto_model.encoder.layer.3.output.LayerNorm.bias\n",
            "  71: True, auto_model.encoder.layer.4.attention.self.query.weight\n",
            "  72: True, auto_model.encoder.layer.4.attention.self.query.bias\n",
            "  73: True, auto_model.encoder.layer.4.attention.self.key.weight\n",
            "  74: True, auto_model.encoder.layer.4.attention.self.key.bias\n",
            "  75: True, auto_model.encoder.layer.4.attention.self.value.weight\n",
            "  76: True, auto_model.encoder.layer.4.attention.self.value.bias\n",
            "  77: True, auto_model.encoder.layer.4.attention.output.dense.weight\n",
            "  78: True, auto_model.encoder.layer.4.attention.output.dense.bias\n",
            "  79: True, auto_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "  80: True, auto_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "  81: True, auto_model.encoder.layer.4.intermediate.dense.weight\n",
            "  82: True, auto_model.encoder.layer.4.intermediate.dense.bias\n",
            "  83: True, auto_model.encoder.layer.4.output.dense.weight\n",
            "  84: True, auto_model.encoder.layer.4.output.dense.bias\n",
            "  85: True, auto_model.encoder.layer.4.output.LayerNorm.weight\n",
            "  86: True, auto_model.encoder.layer.4.output.LayerNorm.bias\n",
            "  87: True, auto_model.encoder.layer.5.attention.self.query.weight\n",
            "  88: True, auto_model.encoder.layer.5.attention.self.query.bias\n",
            "  89: True, auto_model.encoder.layer.5.attention.self.key.weight\n",
            "  90: True, auto_model.encoder.layer.5.attention.self.key.bias\n",
            "  91: True, auto_model.encoder.layer.5.attention.self.value.weight\n",
            "  92: True, auto_model.encoder.layer.5.attention.self.value.bias\n",
            "  93: True, auto_model.encoder.layer.5.attention.output.dense.weight\n",
            "  94: True, auto_model.encoder.layer.5.attention.output.dense.bias\n",
            "  95: True, auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "  96: True, auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "  97: True, auto_model.encoder.layer.5.intermediate.dense.weight\n",
            "  98: True, auto_model.encoder.layer.5.intermediate.dense.bias\n",
            "  99: True, auto_model.encoder.layer.5.output.dense.weight\n",
            " 100: True, auto_model.encoder.layer.5.output.dense.bias\n",
            " 101: True, auto_model.encoder.layer.5.output.LayerNorm.weight\n",
            " 102: True, auto_model.encoder.layer.5.output.LayerNorm.bias\n",
            " 103: True, auto_model.encoder.layer.6.attention.self.query.weight\n",
            " 104: True, auto_model.encoder.layer.6.attention.self.query.bias\n",
            " 105: True, auto_model.encoder.layer.6.attention.self.key.weight\n",
            " 106: True, auto_model.encoder.layer.6.attention.self.key.bias\n",
            " 107: True, auto_model.encoder.layer.6.attention.self.value.weight\n",
            " 108: True, auto_model.encoder.layer.6.attention.self.value.bias\n",
            " 109: True, auto_model.encoder.layer.6.attention.output.dense.weight\n",
            " 110: True, auto_model.encoder.layer.6.attention.output.dense.bias\n",
            " 111: True, auto_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            " 112: True, auto_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            " 113: True, auto_model.encoder.layer.6.intermediate.dense.weight\n",
            " 114: True, auto_model.encoder.layer.6.intermediate.dense.bias\n",
            " 115: True, auto_model.encoder.layer.6.output.dense.weight\n",
            " 116: True, auto_model.encoder.layer.6.output.dense.bias\n",
            " 117: True, auto_model.encoder.layer.6.output.LayerNorm.weight\n",
            " 118: True, auto_model.encoder.layer.6.output.LayerNorm.bias\n",
            " 119: True, auto_model.encoder.layer.7.attention.self.query.weight\n",
            " 120: True, auto_model.encoder.layer.7.attention.self.query.bias\n",
            " 121: True, auto_model.encoder.layer.7.attention.self.key.weight\n",
            " 122: True, auto_model.encoder.layer.7.attention.self.key.bias\n",
            " 123: True, auto_model.encoder.layer.7.attention.self.value.weight\n",
            " 124: True, auto_model.encoder.layer.7.attention.self.value.bias\n",
            " 125: True, auto_model.encoder.layer.7.attention.output.dense.weight\n",
            " 126: True, auto_model.encoder.layer.7.attention.output.dense.bias\n",
            " 127: True, auto_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            " 128: True, auto_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            " 129: True, auto_model.encoder.layer.7.intermediate.dense.weight\n",
            " 130: True, auto_model.encoder.layer.7.intermediate.dense.bias\n",
            " 131: True, auto_model.encoder.layer.7.output.dense.weight\n",
            " 132: True, auto_model.encoder.layer.7.output.dense.bias\n",
            " 133: True, auto_model.encoder.layer.7.output.LayerNorm.weight\n",
            " 134: True, auto_model.encoder.layer.7.output.LayerNorm.bias\n",
            " 135: True, auto_model.encoder.layer.8.attention.self.query.weight\n",
            " 136: True, auto_model.encoder.layer.8.attention.self.query.bias\n",
            " 137: True, auto_model.encoder.layer.8.attention.self.key.weight\n",
            " 138: True, auto_model.encoder.layer.8.attention.self.key.bias\n",
            " 139: True, auto_model.encoder.layer.8.attention.self.value.weight\n",
            " 140: True, auto_model.encoder.layer.8.attention.self.value.bias\n",
            " 141: True, auto_model.encoder.layer.8.attention.output.dense.weight\n",
            " 142: True, auto_model.encoder.layer.8.attention.output.dense.bias\n",
            " 143: True, auto_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            " 144: True, auto_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            " 145: True, auto_model.encoder.layer.8.intermediate.dense.weight\n",
            " 146: True, auto_model.encoder.layer.8.intermediate.dense.bias\n",
            " 147: True, auto_model.encoder.layer.8.output.dense.weight\n",
            " 148: True, auto_model.encoder.layer.8.output.dense.bias\n",
            " 149: True, auto_model.encoder.layer.8.output.LayerNorm.weight\n",
            " 150: True, auto_model.encoder.layer.8.output.LayerNorm.bias\n",
            " 151: True, auto_model.encoder.layer.9.attention.self.query.weight\n",
            " 152: True, auto_model.encoder.layer.9.attention.self.query.bias\n",
            " 153: True, auto_model.encoder.layer.9.attention.self.key.weight\n",
            " 154: True, auto_model.encoder.layer.9.attention.self.key.bias\n",
            " 155: True, auto_model.encoder.layer.9.attention.self.value.weight\n",
            " 156: True, auto_model.encoder.layer.9.attention.self.value.bias\n",
            " 157: True, auto_model.encoder.layer.9.attention.output.dense.weight\n",
            " 158: True, auto_model.encoder.layer.9.attention.output.dense.bias\n",
            " 159: True, auto_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            " 160: True, auto_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            " 161: True, auto_model.encoder.layer.9.intermediate.dense.weight\n",
            " 162: True, auto_model.encoder.layer.9.intermediate.dense.bias\n",
            " 163: True, auto_model.encoder.layer.9.output.dense.weight\n",
            " 164: True, auto_model.encoder.layer.9.output.dense.bias\n",
            " 165: True, auto_model.encoder.layer.9.output.LayerNorm.weight\n",
            " 166: True, auto_model.encoder.layer.9.output.LayerNorm.bias\n",
            " 167: True, auto_model.encoder.layer.10.attention.self.query.weight\n",
            " 168: True, auto_model.encoder.layer.10.attention.self.query.bias\n",
            " 169: True, auto_model.encoder.layer.10.attention.self.key.weight\n",
            " 170: True, auto_model.encoder.layer.10.attention.self.key.bias\n",
            " 171: True, auto_model.encoder.layer.10.attention.self.value.weight\n",
            " 172: True, auto_model.encoder.layer.10.attention.self.value.bias\n",
            " 173: True, auto_model.encoder.layer.10.attention.output.dense.weight\n",
            " 174: True, auto_model.encoder.layer.10.attention.output.dense.bias\n",
            " 175: True, auto_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            " 176: True, auto_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            " 177: True, auto_model.encoder.layer.10.intermediate.dense.weight\n",
            " 178: True, auto_model.encoder.layer.10.intermediate.dense.bias\n",
            " 179: True, auto_model.encoder.layer.10.output.dense.weight\n",
            " 180: True, auto_model.encoder.layer.10.output.dense.bias\n",
            " 181: True, auto_model.encoder.layer.10.output.LayerNorm.weight\n",
            " 182: True, auto_model.encoder.layer.10.output.LayerNorm.bias\n",
            " 183: True, auto_model.encoder.layer.11.attention.self.query.weight\n",
            " 184: True, auto_model.encoder.layer.11.attention.self.query.bias\n",
            " 185: True, auto_model.encoder.layer.11.attention.self.key.weight\n",
            " 186: True, auto_model.encoder.layer.11.attention.self.key.bias\n",
            " 187: True, auto_model.encoder.layer.11.attention.self.value.weight\n",
            " 188: True, auto_model.encoder.layer.11.attention.self.value.bias\n",
            " 189: True, auto_model.encoder.layer.11.attention.output.dense.weight\n",
            " 190: True, auto_model.encoder.layer.11.attention.output.dense.bias\n",
            " 191: True, auto_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            " 192: True, auto_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            " 193: True, auto_model.encoder.layer.11.intermediate.dense.weight\n",
            " 194: True, auto_model.encoder.layer.11.intermediate.dense.bias\n",
            " 195: True, auto_model.encoder.layer.11.output.dense.weight\n",
            " 196: True, auto_model.encoder.layer.11.output.dense.bias\n",
            " 197: True, auto_model.encoder.layer.11.output.LayerNorm.weight\n",
            " 198: True, auto_model.encoder.layer.11.output.LayerNorm.bias\n",
            " 199: True, auto_model.encoder.layer.12.attention.self.query.weight\n",
            " 200: True, auto_model.encoder.layer.12.attention.self.query.bias\n",
            " 201: True, auto_model.encoder.layer.12.attention.self.key.weight\n",
            " 202: True, auto_model.encoder.layer.12.attention.self.key.bias\n",
            " 203: True, auto_model.encoder.layer.12.attention.self.value.weight\n",
            " 204: True, auto_model.encoder.layer.12.attention.self.value.bias\n",
            " 205: True, auto_model.encoder.layer.12.attention.output.dense.weight\n",
            " 206: True, auto_model.encoder.layer.12.attention.output.dense.bias\n",
            " 207: True, auto_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
            " 208: True, auto_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
            " 209: True, auto_model.encoder.layer.12.intermediate.dense.weight\n",
            " 210: True, auto_model.encoder.layer.12.intermediate.dense.bias\n",
            " 211: True, auto_model.encoder.layer.12.output.dense.weight\n",
            " 212: True, auto_model.encoder.layer.12.output.dense.bias\n",
            " 213: True, auto_model.encoder.layer.12.output.LayerNorm.weight\n",
            " 214: True, auto_model.encoder.layer.12.output.LayerNorm.bias\n",
            " 215: True, auto_model.encoder.layer.13.attention.self.query.weight\n",
            " 216: True, auto_model.encoder.layer.13.attention.self.query.bias\n",
            " 217: True, auto_model.encoder.layer.13.attention.self.key.weight\n",
            " 218: True, auto_model.encoder.layer.13.attention.self.key.bias\n",
            " 219: True, auto_model.encoder.layer.13.attention.self.value.weight\n",
            " 220: True, auto_model.encoder.layer.13.attention.self.value.bias\n",
            " 221: True, auto_model.encoder.layer.13.attention.output.dense.weight\n",
            " 222: True, auto_model.encoder.layer.13.attention.output.dense.bias\n",
            " 223: True, auto_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
            " 224: True, auto_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
            " 225: True, auto_model.encoder.layer.13.intermediate.dense.weight\n",
            " 226: True, auto_model.encoder.layer.13.intermediate.dense.bias\n",
            " 227: True, auto_model.encoder.layer.13.output.dense.weight\n",
            " 228: True, auto_model.encoder.layer.13.output.dense.bias\n",
            " 229: True, auto_model.encoder.layer.13.output.LayerNorm.weight\n",
            " 230: True, auto_model.encoder.layer.13.output.LayerNorm.bias\n",
            " 231: True, auto_model.encoder.layer.14.attention.self.query.weight\n",
            " 232: True, auto_model.encoder.layer.14.attention.self.query.bias\n",
            " 233: True, auto_model.encoder.layer.14.attention.self.key.weight\n",
            " 234: True, auto_model.encoder.layer.14.attention.self.key.bias\n",
            " 235: True, auto_model.encoder.layer.14.attention.self.value.weight\n",
            " 236: True, auto_model.encoder.layer.14.attention.self.value.bias\n",
            " 237: True, auto_model.encoder.layer.14.attention.output.dense.weight\n",
            " 238: True, auto_model.encoder.layer.14.attention.output.dense.bias\n",
            " 239: True, auto_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
            " 240: True, auto_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
            " 241: True, auto_model.encoder.layer.14.intermediate.dense.weight\n",
            " 242: True, auto_model.encoder.layer.14.intermediate.dense.bias\n",
            " 243: True, auto_model.encoder.layer.14.output.dense.weight\n",
            " 244: True, auto_model.encoder.layer.14.output.dense.bias\n",
            " 245: True, auto_model.encoder.layer.14.output.LayerNorm.weight\n",
            " 246: True, auto_model.encoder.layer.14.output.LayerNorm.bias\n",
            " 247: True, auto_model.encoder.layer.15.attention.self.query.weight\n",
            " 248: True, auto_model.encoder.layer.15.attention.self.query.bias\n",
            " 249: True, auto_model.encoder.layer.15.attention.self.key.weight\n",
            " 250: True, auto_model.encoder.layer.15.attention.self.key.bias\n",
            " 251: True, auto_model.encoder.layer.15.attention.self.value.weight\n",
            " 252: True, auto_model.encoder.layer.15.attention.self.value.bias\n",
            " 253: True, auto_model.encoder.layer.15.attention.output.dense.weight\n",
            " 254: True, auto_model.encoder.layer.15.attention.output.dense.bias\n",
            " 255: True, auto_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
            " 256: True, auto_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
            " 257: True, auto_model.encoder.layer.15.intermediate.dense.weight\n",
            " 258: True, auto_model.encoder.layer.15.intermediate.dense.bias\n",
            " 259: True, auto_model.encoder.layer.15.output.dense.weight\n",
            " 260: True, auto_model.encoder.layer.15.output.dense.bias\n",
            " 261: True, auto_model.encoder.layer.15.output.LayerNorm.weight\n",
            " 262: True, auto_model.encoder.layer.15.output.LayerNorm.bias\n",
            " 263: True, auto_model.encoder.layer.16.attention.self.query.weight\n",
            " 264: True, auto_model.encoder.layer.16.attention.self.query.bias\n",
            " 265: True, auto_model.encoder.layer.16.attention.self.key.weight\n",
            " 266: True, auto_model.encoder.layer.16.attention.self.key.bias\n",
            " 267: True, auto_model.encoder.layer.16.attention.self.value.weight\n",
            " 268: True, auto_model.encoder.layer.16.attention.self.value.bias\n",
            " 269: True, auto_model.encoder.layer.16.attention.output.dense.weight\n",
            " 270: True, auto_model.encoder.layer.16.attention.output.dense.bias\n",
            " 271: True, auto_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
            " 272: True, auto_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
            " 273: True, auto_model.encoder.layer.16.intermediate.dense.weight\n",
            " 274: True, auto_model.encoder.layer.16.intermediate.dense.bias\n",
            " 275: True, auto_model.encoder.layer.16.output.dense.weight\n",
            " 276: True, auto_model.encoder.layer.16.output.dense.bias\n",
            " 277: True, auto_model.encoder.layer.16.output.LayerNorm.weight\n",
            " 278: True, auto_model.encoder.layer.16.output.LayerNorm.bias\n",
            " 279: True, auto_model.encoder.layer.17.attention.self.query.weight\n",
            " 280: True, auto_model.encoder.layer.17.attention.self.query.bias\n",
            " 281: True, auto_model.encoder.layer.17.attention.self.key.weight\n",
            " 282: True, auto_model.encoder.layer.17.attention.self.key.bias\n",
            " 283: True, auto_model.encoder.layer.17.attention.self.value.weight\n",
            " 284: True, auto_model.encoder.layer.17.attention.self.value.bias\n",
            " 285: True, auto_model.encoder.layer.17.attention.output.dense.weight\n",
            " 286: True, auto_model.encoder.layer.17.attention.output.dense.bias\n",
            " 287: True, auto_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
            " 288: True, auto_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
            " 289: True, auto_model.encoder.layer.17.intermediate.dense.weight\n",
            " 290: True, auto_model.encoder.layer.17.intermediate.dense.bias\n",
            " 291: True, auto_model.encoder.layer.17.output.dense.weight\n",
            " 292: True, auto_model.encoder.layer.17.output.dense.bias\n",
            " 293: True, auto_model.encoder.layer.17.output.LayerNorm.weight\n",
            " 294: True, auto_model.encoder.layer.17.output.LayerNorm.bias\n",
            " 295: True, auto_model.encoder.layer.18.attention.self.query.weight\n",
            " 296: True, auto_model.encoder.layer.18.attention.self.query.bias\n",
            " 297: True, auto_model.encoder.layer.18.attention.self.key.weight\n",
            " 298: True, auto_model.encoder.layer.18.attention.self.key.bias\n",
            " 299: True, auto_model.encoder.layer.18.attention.self.value.weight\n",
            " 300: True, auto_model.encoder.layer.18.attention.self.value.bias\n",
            " 301: True, auto_model.encoder.layer.18.attention.output.dense.weight\n",
            " 302: True, auto_model.encoder.layer.18.attention.output.dense.bias\n",
            " 303: True, auto_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
            " 304: True, auto_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
            " 305: True, auto_model.encoder.layer.18.intermediate.dense.weight\n",
            " 306: True, auto_model.encoder.layer.18.intermediate.dense.bias\n",
            " 307: True, auto_model.encoder.layer.18.output.dense.weight\n",
            " 308: True, auto_model.encoder.layer.18.output.dense.bias\n",
            " 309: True, auto_model.encoder.layer.18.output.LayerNorm.weight\n",
            " 310: True, auto_model.encoder.layer.18.output.LayerNorm.bias\n",
            " 311: True, auto_model.encoder.layer.19.attention.self.query.weight\n",
            " 312: True, auto_model.encoder.layer.19.attention.self.query.bias\n",
            " 313: True, auto_model.encoder.layer.19.attention.self.key.weight\n",
            " 314: True, auto_model.encoder.layer.19.attention.self.key.bias\n",
            " 315: True, auto_model.encoder.layer.19.attention.self.value.weight\n",
            " 316: True, auto_model.encoder.layer.19.attention.self.value.bias\n",
            " 317: True, auto_model.encoder.layer.19.attention.output.dense.weight\n",
            " 318: True, auto_model.encoder.layer.19.attention.output.dense.bias\n",
            " 319: True, auto_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
            " 320: True, auto_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
            " 321: True, auto_model.encoder.layer.19.intermediate.dense.weight\n",
            " 322: True, auto_model.encoder.layer.19.intermediate.dense.bias\n",
            " 323: True, auto_model.encoder.layer.19.output.dense.weight\n",
            " 324: True, auto_model.encoder.layer.19.output.dense.bias\n",
            " 325: True, auto_model.encoder.layer.19.output.LayerNorm.weight\n",
            " 326: True, auto_model.encoder.layer.19.output.LayerNorm.bias\n",
            " 327: True, auto_model.encoder.layer.20.attention.self.query.weight\n",
            " 328: True, auto_model.encoder.layer.20.attention.self.query.bias\n",
            " 329: True, auto_model.encoder.layer.20.attention.self.key.weight\n",
            " 330: True, auto_model.encoder.layer.20.attention.self.key.bias\n",
            " 331: True, auto_model.encoder.layer.20.attention.self.value.weight\n",
            " 332: True, auto_model.encoder.layer.20.attention.self.value.bias\n",
            " 333: True, auto_model.encoder.layer.20.attention.output.dense.weight\n",
            " 334: True, auto_model.encoder.layer.20.attention.output.dense.bias\n",
            " 335: True, auto_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
            " 336: True, auto_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
            " 337: True, auto_model.encoder.layer.20.intermediate.dense.weight\n",
            " 338: True, auto_model.encoder.layer.20.intermediate.dense.bias\n",
            " 339: True, auto_model.encoder.layer.20.output.dense.weight\n",
            " 340: True, auto_model.encoder.layer.20.output.dense.bias\n",
            " 341: True, auto_model.encoder.layer.20.output.LayerNorm.weight\n",
            " 342: True, auto_model.encoder.layer.20.output.LayerNorm.bias\n",
            " 343: True, auto_model.encoder.layer.21.attention.self.query.weight\n",
            " 344: True, auto_model.encoder.layer.21.attention.self.query.bias\n",
            " 345: True, auto_model.encoder.layer.21.attention.self.key.weight\n",
            " 346: True, auto_model.encoder.layer.21.attention.self.key.bias\n",
            " 347: True, auto_model.encoder.layer.21.attention.self.value.weight\n",
            " 348: True, auto_model.encoder.layer.21.attention.self.value.bias\n",
            " 349: True, auto_model.encoder.layer.21.attention.output.dense.weight\n",
            " 350: True, auto_model.encoder.layer.21.attention.output.dense.bias\n",
            " 351: True, auto_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
            " 352: True, auto_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
            " 353: True, auto_model.encoder.layer.21.intermediate.dense.weight\n",
            " 354: True, auto_model.encoder.layer.21.intermediate.dense.bias\n",
            " 355: True, auto_model.encoder.layer.21.output.dense.weight\n",
            " 356: True, auto_model.encoder.layer.21.output.dense.bias\n",
            " 357: True, auto_model.encoder.layer.21.output.LayerNorm.weight\n",
            " 358: True, auto_model.encoder.layer.21.output.LayerNorm.bias\n",
            " 359: True, auto_model.encoder.layer.22.attention.self.query.weight\n",
            " 360: True, auto_model.encoder.layer.22.attention.self.query.bias\n",
            " 361: True, auto_model.encoder.layer.22.attention.self.key.weight\n",
            " 362: True, auto_model.encoder.layer.22.attention.self.key.bias\n",
            " 363: True, auto_model.encoder.layer.22.attention.self.value.weight\n",
            " 364: True, auto_model.encoder.layer.22.attention.self.value.bias\n",
            " 365: True, auto_model.encoder.layer.22.attention.output.dense.weight\n",
            " 366: True, auto_model.encoder.layer.22.attention.output.dense.bias\n",
            " 367: True, auto_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
            " 368: True, auto_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
            " 369: True, auto_model.encoder.layer.22.intermediate.dense.weight\n",
            " 370: True, auto_model.encoder.layer.22.intermediate.dense.bias\n",
            " 371: True, auto_model.encoder.layer.22.output.dense.weight\n",
            " 372: True, auto_model.encoder.layer.22.output.dense.bias\n",
            " 373: True, auto_model.encoder.layer.22.output.LayerNorm.weight\n",
            " 374: True, auto_model.encoder.layer.22.output.LayerNorm.bias\n",
            " 375: True, auto_model.encoder.layer.23.attention.self.query.weight\n",
            " 376: True, auto_model.encoder.layer.23.attention.self.query.bias\n",
            " 377: True, auto_model.encoder.layer.23.attention.self.key.weight\n",
            " 378: True, auto_model.encoder.layer.23.attention.self.key.bias\n",
            " 379: True, auto_model.encoder.layer.23.attention.self.value.weight\n",
            " 380: True, auto_model.encoder.layer.23.attention.self.value.bias\n",
            " 381: True, auto_model.encoder.layer.23.attention.output.dense.weight\n",
            " 382: True, auto_model.encoder.layer.23.attention.output.dense.bias\n",
            " 383: True, auto_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
            " 384: True, auto_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
            " 385: True, auto_model.encoder.layer.23.intermediate.dense.weight\n",
            " 386: True, auto_model.encoder.layer.23.intermediate.dense.bias\n",
            " 387: True, auto_model.encoder.layer.23.output.dense.weight\n",
            " 388: True, auto_model.encoder.layer.23.output.dense.bias\n",
            " 389: True, auto_model.encoder.layer.23.output.LayerNorm.weight\n",
            " 390: True, auto_model.encoder.layer.23.output.LayerNorm.bias\n",
            " 391: True, auto_model.encoder.layer.24.attention.self.query.weight\n",
            " 392: True, auto_model.encoder.layer.24.attention.self.query.bias\n",
            " 393: True, auto_model.encoder.layer.24.attention.self.key.weight\n",
            " 394: True, auto_model.encoder.layer.24.attention.self.key.bias\n",
            " 395: True, auto_model.encoder.layer.24.attention.self.value.weight\n",
            " 396: True, auto_model.encoder.layer.24.attention.self.value.bias\n",
            " 397: True, auto_model.encoder.layer.24.attention.output.dense.weight\n",
            " 398: True, auto_model.encoder.layer.24.attention.output.dense.bias\n",
            " 399: True, auto_model.encoder.layer.24.attention.output.LayerNorm.weight\n",
            " 400: True, auto_model.encoder.layer.24.attention.output.LayerNorm.bias\n",
            " 401: True, auto_model.encoder.layer.24.intermediate.dense.weight\n",
            " 402: True, auto_model.encoder.layer.24.intermediate.dense.bias\n",
            " 403: True, auto_model.encoder.layer.24.output.dense.weight\n",
            " 404: True, auto_model.encoder.layer.24.output.dense.bias\n",
            " 405: True, auto_model.encoder.layer.24.output.LayerNorm.weight\n",
            " 406: True, auto_model.encoder.layer.24.output.LayerNorm.bias\n",
            " 407: True, auto_model.encoder.layer.25.attention.self.query.weight\n",
            " 408: True, auto_model.encoder.layer.25.attention.self.query.bias\n",
            " 409: True, auto_model.encoder.layer.25.attention.self.key.weight\n",
            " 410: True, auto_model.encoder.layer.25.attention.self.key.bias\n",
            " 411: True, auto_model.encoder.layer.25.attention.self.value.weight\n",
            " 412: True, auto_model.encoder.layer.25.attention.self.value.bias\n",
            " 413: True, auto_model.encoder.layer.25.attention.output.dense.weight\n",
            " 414: True, auto_model.encoder.layer.25.attention.output.dense.bias\n",
            " 415: True, auto_model.encoder.layer.25.attention.output.LayerNorm.weight\n",
            " 416: True, auto_model.encoder.layer.25.attention.output.LayerNorm.bias\n",
            " 417: True, auto_model.encoder.layer.25.intermediate.dense.weight\n",
            " 418: True, auto_model.encoder.layer.25.intermediate.dense.bias\n",
            " 419: True, auto_model.encoder.layer.25.output.dense.weight\n",
            " 420: True, auto_model.encoder.layer.25.output.dense.bias\n",
            " 421: True, auto_model.encoder.layer.25.output.LayerNorm.weight\n",
            " 422: True, auto_model.encoder.layer.25.output.LayerNorm.bias\n",
            " 423: True, auto_model.encoder.layer.26.attention.self.query.weight\n",
            " 424: True, auto_model.encoder.layer.26.attention.self.query.bias\n",
            " 425: True, auto_model.encoder.layer.26.attention.self.key.weight\n",
            " 426: True, auto_model.encoder.layer.26.attention.self.key.bias\n",
            " 427: True, auto_model.encoder.layer.26.attention.self.value.weight\n",
            " 428: True, auto_model.encoder.layer.26.attention.self.value.bias\n",
            " 429: True, auto_model.encoder.layer.26.attention.output.dense.weight\n",
            " 430: True, auto_model.encoder.layer.26.attention.output.dense.bias\n",
            " 431: True, auto_model.encoder.layer.26.attention.output.LayerNorm.weight\n",
            " 432: True, auto_model.encoder.layer.26.attention.output.LayerNorm.bias\n",
            " 433: True, auto_model.encoder.layer.26.intermediate.dense.weight\n",
            " 434: True, auto_model.encoder.layer.26.intermediate.dense.bias\n",
            " 435: True, auto_model.encoder.layer.26.output.dense.weight\n",
            " 436: True, auto_model.encoder.layer.26.output.dense.bias\n",
            " 437: True, auto_model.encoder.layer.26.output.LayerNorm.weight\n",
            " 438: True, auto_model.encoder.layer.26.output.LayerNorm.bias\n",
            " 439: True, auto_model.encoder.layer.27.attention.self.query.weight\n",
            " 440: True, auto_model.encoder.layer.27.attention.self.query.bias\n",
            " 441: True, auto_model.encoder.layer.27.attention.self.key.weight\n",
            " 442: True, auto_model.encoder.layer.27.attention.self.key.bias\n",
            " 443: True, auto_model.encoder.layer.27.attention.self.value.weight\n",
            " 444: True, auto_model.encoder.layer.27.attention.self.value.bias\n",
            " 445: True, auto_model.encoder.layer.27.attention.output.dense.weight\n",
            " 446: True, auto_model.encoder.layer.27.attention.output.dense.bias\n",
            " 447: True, auto_model.encoder.layer.27.attention.output.LayerNorm.weight\n",
            " 448: True, auto_model.encoder.layer.27.attention.output.LayerNorm.bias\n",
            " 449: True, auto_model.encoder.layer.27.intermediate.dense.weight\n",
            " 450: True, auto_model.encoder.layer.27.intermediate.dense.bias\n",
            " 451: True, auto_model.encoder.layer.27.output.dense.weight\n",
            " 452: True, auto_model.encoder.layer.27.output.dense.bias\n",
            " 453: True, auto_model.encoder.layer.27.output.LayerNorm.weight\n",
            " 454: True, auto_model.encoder.layer.27.output.LayerNorm.bias\n",
            " 455: True, auto_model.encoder.layer.28.attention.self.query.weight\n",
            " 456: True, auto_model.encoder.layer.28.attention.self.query.bias\n",
            " 457: True, auto_model.encoder.layer.28.attention.self.key.weight\n",
            " 458: True, auto_model.encoder.layer.28.attention.self.key.bias\n",
            " 459: True, auto_model.encoder.layer.28.attention.self.value.weight\n",
            " 460: True, auto_model.encoder.layer.28.attention.self.value.bias\n",
            " 461: True, auto_model.encoder.layer.28.attention.output.dense.weight\n",
            " 462: True, auto_model.encoder.layer.28.attention.output.dense.bias\n",
            " 463: True, auto_model.encoder.layer.28.attention.output.LayerNorm.weight\n",
            " 464: True, auto_model.encoder.layer.28.attention.output.LayerNorm.bias\n",
            " 465: True, auto_model.encoder.layer.28.intermediate.dense.weight\n",
            " 466: True, auto_model.encoder.layer.28.intermediate.dense.bias\n",
            " 467: True, auto_model.encoder.layer.28.output.dense.weight\n",
            " 468: True, auto_model.encoder.layer.28.output.dense.bias\n",
            " 469: True, auto_model.encoder.layer.28.output.LayerNorm.weight\n",
            " 470: True, auto_model.encoder.layer.28.output.LayerNorm.bias\n",
            " 471: True, auto_model.encoder.layer.29.attention.self.query.weight\n",
            " 472: True, auto_model.encoder.layer.29.attention.self.query.bias\n",
            " 473: True, auto_model.encoder.layer.29.attention.self.key.weight\n",
            " 474: True, auto_model.encoder.layer.29.attention.self.key.bias\n",
            " 475: True, auto_model.encoder.layer.29.attention.self.value.weight\n",
            " 476: True, auto_model.encoder.layer.29.attention.self.value.bias\n",
            " 477: True, auto_model.encoder.layer.29.attention.output.dense.weight\n",
            " 478: True, auto_model.encoder.layer.29.attention.output.dense.bias\n",
            " 479: True, auto_model.encoder.layer.29.attention.output.LayerNorm.weight\n",
            " 480: True, auto_model.encoder.layer.29.attention.output.LayerNorm.bias\n",
            " 481: True, auto_model.encoder.layer.29.intermediate.dense.weight\n",
            " 482: True, auto_model.encoder.layer.29.intermediate.dense.bias\n",
            " 483: True, auto_model.encoder.layer.29.output.dense.weight\n",
            " 484: True, auto_model.encoder.layer.29.output.dense.bias\n",
            " 485: True, auto_model.encoder.layer.29.output.LayerNorm.weight\n",
            " 486: True, auto_model.encoder.layer.29.output.LayerNorm.bias\n",
            " 487: True, auto_model.encoder.layer.30.attention.self.query.weight\n",
            " 488: True, auto_model.encoder.layer.30.attention.self.query.bias\n",
            " 489: True, auto_model.encoder.layer.30.attention.self.key.weight\n",
            " 490: True, auto_model.encoder.layer.30.attention.self.key.bias\n",
            " 491: True, auto_model.encoder.layer.30.attention.self.value.weight\n",
            " 492: True, auto_model.encoder.layer.30.attention.self.value.bias\n",
            " 493: True, auto_model.encoder.layer.30.attention.output.dense.weight\n",
            " 494: True, auto_model.encoder.layer.30.attention.output.dense.bias\n",
            " 495: True, auto_model.encoder.layer.30.attention.output.LayerNorm.weight\n",
            " 496: True, auto_model.encoder.layer.30.attention.output.LayerNorm.bias\n",
            " 497: True, auto_model.encoder.layer.30.intermediate.dense.weight\n",
            " 498: True, auto_model.encoder.layer.30.intermediate.dense.bias\n",
            " 499: True, auto_model.encoder.layer.30.output.dense.weight\n",
            " 500: True, auto_model.encoder.layer.30.output.dense.bias\n",
            " 501: True, auto_model.encoder.layer.30.output.LayerNorm.weight\n",
            " 502: True, auto_model.encoder.layer.30.output.LayerNorm.bias\n",
            " 503: True, auto_model.encoder.layer.31.attention.self.query.weight\n",
            " 504: True, auto_model.encoder.layer.31.attention.self.query.bias\n",
            " 505: True, auto_model.encoder.layer.31.attention.self.key.weight\n",
            " 506: True, auto_model.encoder.layer.31.attention.self.key.bias\n",
            " 507: True, auto_model.encoder.layer.31.attention.self.value.weight\n",
            " 508: True, auto_model.encoder.layer.31.attention.self.value.bias\n",
            " 509: True, auto_model.encoder.layer.31.attention.output.dense.weight\n",
            " 510: True, auto_model.encoder.layer.31.attention.output.dense.bias\n",
            " 511: True, auto_model.encoder.layer.31.attention.output.LayerNorm.weight\n",
            " 512: True, auto_model.encoder.layer.31.attention.output.LayerNorm.bias\n",
            " 513: True, auto_model.encoder.layer.31.intermediate.dense.weight\n",
            " 514: True, auto_model.encoder.layer.31.intermediate.dense.bias\n",
            " 515: True, auto_model.encoder.layer.31.output.dense.weight\n",
            " 516: True, auto_model.encoder.layer.31.output.dense.bias\n",
            " 517: True, auto_model.encoder.layer.31.output.LayerNorm.weight\n",
            " 518: True, auto_model.encoder.layer.31.output.LayerNorm.bias\n",
            " 519: True, qa_outputs.weight\n",
            " 520: True, qa_outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6NSxPjSDAp0"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_stCiHpfE4Cj"
      },
      "source": [
        "def bert_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    if (\n",
        "        \"base\" in config.model_name\n",
        "        or \"L-12\" in config.model_name\n",
        "    ):\n",
        "        bert_parameters = named_parameters[:197]    \n",
        "        regressor_parameters = named_parameters[197:]\n",
        "        second_block = 69\n",
        "        third_block = 133\n",
        "\n",
        "    elif (\n",
        "        \"large\" in config.model_name\n",
        "        or \"L-24\" in config.model_name\n",
        "    ):\n",
        "        bert_parameters = named_parameters[:389]    \n",
        "        regressor_parameters = named_parameters[389:]\n",
        "        second_block = 133\n",
        "        third_block = 261\n",
        "\n",
        "    elif \"rembert\" in config.model_name:\n",
        "        bert_parameters = named_parameters[:519]\n",
        "        regressor_parameters = named_parameters[519:]\n",
        "        second_block = 199\n",
        "        third_block = 359\n",
        "        \n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(bert_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else config.weight_decay\n",
        "\n",
        "        if layer_num >= third_block:\n",
        "            lr = config.max_lr\n",
        "        elif layer_num >= second_block:\n",
        "            lr = config.lr\n",
        "        else:\n",
        "            lr = config.min_lr\n",
        "\n",
        "        parameters.append({\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr})\n",
        "\n",
        "    return T.AdamW(parameters, eps=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENcidivxDVIj"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAGe7WFDWG4"
      },
      "source": [
        "def chaii_cross_entropy(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJr7FSz5KvLt"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXFVz_OVKu2O"
      },
      "source": [
        "def jaccard(row): \n",
        "    str1 = row[0]\n",
        "    str2 = row[1]\n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfqKjGpLaMMX"
      },
      "source": [
        "def get_result(result_df, fold=config.n_fold):\n",
        "    score = result_df[\"jaccard\"].mean()\n",
        "    LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "    if fold == config.n_fold:\n",
        "        wandb.log({\"CV\": score})\n",
        "    else:\n",
        "        wandb.log({f\"CV_fold{fold}\": score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7aZ38xCMG__"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD2wDdHMMMSc"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrVjLs3H8DXV"
      },
      "source": [
        "def compute_grad_norm(parameters, norm_type=2.0):\n",
        "    \"\"\"Refer to torch.nn.utils.clip_grad_norm_\"\"\"\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    norm_type = float(norm_type)\n",
        "    total_norm = 0\n",
        "    for p in parameters:\n",
        "        param_norm = p.grad.data.norm(norm_type)\n",
        "        total_norm += param_norm.item() ** norm_type\n",
        "    total_norm = total_norm ** (1. / norm_type)\n",
        "    return total_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laoX2YvHMW40"
      },
      "source": [
        "def train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, features in enumerate(train_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        with amp.autocast(enabled=Config.amp):\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "            loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss = loss / config.gradient_accumulation_steps\n",
        "            \n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        else:\n",
        "            grad_norm = compute_grad_norm(model.parameters())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(train_loader) - 1):\n",
        "            print(\n",
        "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "                f\"Grad: {grad_norm:.4f} \"\n",
        "                f\"LR: {scheduler.get_lr()[0]:.6f} \"\n",
        "            )\n",
        "            # wandb.log({\n",
        "            #     \"step\": (epoch) * len(train_loader) + step,\n",
        "            #     f\"loss/fold{fold}\": losses.avg,\n",
        "            #     f\"grad/fold{fold}\": grad_norm,\n",
        "            #     f\"lr/fold{fold}\": scheduler.get_lr()[0],\n",
        "            # })\n",
        "\n",
        "    return losses.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4GZ8PcPpLt"
      },
      "source": [
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds_start = []\n",
        "    preds_end = []\n",
        "    start = time.time()\n",
        "\n",
        "    for step, features in enumerate(valid_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        preds_start.append(out_start.to(\"cpu\").numpy())\n",
        "        preds_end.append(out_end.to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(valid_loader) - 1):\n",
        "            print(\n",
        "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "            )\n",
        "\n",
        "    predictions_start = np.concatenate(preds_start)\n",
        "    predictions_end = np.concatenate(preds_end)\n",
        "    return losses.avg, predictions_start, predictions_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aS_0cqjWy5P"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5pkRxmW0z_"
      },
      "source": [
        "def postprocess_qa_predictions(examples, features, tokenizer, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    \n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    for example_index, example in examples.iterrows():\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "\n",
        "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
        "            context_index = 1\n",
        "\n",
        "            features[feature_index][\"offset_mapping\"] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
        "            ]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUqyC9I8xyd9"
      },
      "source": [
        "# https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765\n",
        "def postpurocess_by_nbroad(preds_df):\n",
        "    bad_starts = [\".\", \"(\", \")\", \"-\", \"‚Äì\", \",\", \";\"]\n",
        "    bad_endings = [\"...\", \"(\", \")\", \"-\", \"‚Äì\", \",\", \";\"]\n",
        "\n",
        "    tamil_ad = \"‡Æï‡Æø.‡Æ™‡Æø\"\n",
        "    tamil_bc = \"‡Æï‡Æø.‡ÆÆ‡ØÅ\"\n",
        "    tamil_km = \"‡Æï‡Æø.‡ÆÆ‡ØÄ\"\n",
        "    hindi_ad = \"‡§à\"\n",
        "    hindi_bc = \"‡§à.‡§™‡•Ç\"\n",
        "\n",
        "    cleaned_preds = []\n",
        "    for pred, context in preds_df[[\"prediction\", \"context\"]].to_numpy():\n",
        "        if pred == \"\":\n",
        "            cleaned_preds.append(pred)\n",
        "            continue\n",
        "        pred = pred.strip()\n",
        "        while any([pred.startswith(y) for y in bad_starts]):\n",
        "            pred = pred[1:]\n",
        "        while any([pred.endswith(y) for y in bad_endings]):\n",
        "            if pred.endswith(\"...\"):\n",
        "                pred = pred[:-3]\n",
        "            else:\n",
        "                pred = pred[:-1]\n",
        "\n",
        "        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n",
        "            pred = pred+\".\"\n",
        "\n",
        "        cleaned_preds.append(pred)\n",
        "\n",
        "    preds_df[\"prediction\"] = cleaned_preds\n",
        "\n",
        "    return preds_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vwcRHThRbcm"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmu1ZdXRdA7"
      },
      "source": [
        "def train_loop(df, fold):\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # Data Loader\n",
        "    # ====================================================\n",
        "    trn_idx = df[df[\"fold\"] != fold].index\n",
        "    val_idx = df[df[\"fold\"] == fold].index\n",
        "\n",
        "    train_folds = df.loc[trn_idx].reset_index(drop=True)\n",
        "    valid_folds = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = BaseDataset(train_folds, config.model_name)\n",
        "    valid_dataset = BaseDataset(valid_folds, config.model_name)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # ====================================================\n",
        "    # Optimizer\n",
        "    # ====================================================\n",
        "    def get_optimizer(model):\n",
        "        if config.optimizer == \"Adam\":\n",
        "            optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"AdamW\":\n",
        "            optimizer = T.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"BertAdamW\":\n",
        "            optimizer = bert_optimizer(model)\n",
        "        return optimizer\n",
        "\n",
        "    # ====================================================\n",
        "    # Scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(optimizer):\n",
        "        # num_data = len(train_folds)\n",
        "        num_data = len(train_dataset)\n",
        "        num_steps = num_data // (config.batch_size * config.gradient_accumulation_steps) * config.epochs\n",
        "\n",
        "        if config.scheduler == \"CosineAnnealingWarmRestarts\":\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=num_steps, T_mult=1, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingLR\":\n",
        "            scheduler = CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingWarmupRestarts\":\n",
        "            scheduler = CosineAnnealingWarmupRestarts(\n",
        "                optimizer, first_cycle_steps=num_steps, max_lr=config.lr, min_lr=config.min_lr, warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        elif config.scheduler == \"get_cosine_schedule_with_warmup\":\n",
        "            scheduler = T.get_cosine_schedule_with_warmup(\n",
        "                optimizer, num_training_steps=num_steps, num_warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        return scheduler\n",
        "\n",
        "    # ====================================================\n",
        "    # Model\n",
        "    # ====================================================\n",
        "    if config.model_class == \"bare\":\n",
        "        model = BaseModel(config.model_name)\n",
        "    elif config.model_class == \"qa\":\n",
        "        model = QAModel(config.model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = get_optimizer(model)\n",
        "    scaler = amp.GradScaler(enabled=Config.amp)\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "\n",
        "    # ====================================================\n",
        "    # Criterion\n",
        "    # ====================================================\n",
        "    def get_criterion():\n",
        "        if config.criterion == \"CrossEntropyLoss\":\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "        elif config.criterion == \"BCEWithLogitsLoss\":\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "        elif config.criterion == \"MSELoss\":\n",
        "            criterion = nn.MSELoss()\n",
        "        elif config.criterion == \"ChaiiCrossEntropyLoss\":\n",
        "            criterion = chaii_cross_entropy\n",
        "        return criterion\n",
        "\n",
        "    criterion = get_criterion()\n",
        "\n",
        "    # ====================================================\n",
        "    # Loop\n",
        "    # ====================================================\n",
        "    best_score = -1\n",
        "    best_loss = np.inf\n",
        "    best_preds = None\n",
        "\n",
        "    wandb.watch(model, log_freq=Config.print_freq)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, preds_start, preds_end = valid_fn(valid_loader, model, criterion, device)\n",
        "\n",
        "        # postprocess 1\n",
        "        predictions = postprocess_qa_predictions(\n",
        "            valid_folds, valid_dataset.features, valid_dataset.tokenizer, (preds_start, preds_end)\n",
        "        )\n",
        "        valid_folds[\"prediction\"] = valid_folds['id'].apply(lambda r: predictions[r])\n",
        "\n",
        "        # postprocess 2\n",
        "        valid_folds = postpurocess_by_nbroad(valid_folds)\n",
        "\n",
        "        # scoring\n",
        "        valid_folds['jaccard'] = valid_folds[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
        "        score = valid_folds[\"jaccard\"].mean()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {elapsed:.0f}s\")\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            f\"val_loss/fold{fold}\": avg_val_loss,\n",
        "            f\"score/fold{fold}\": score,\n",
        "        })\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_score = score\n",
        "            best_loss = avg_val_loss\n",
        "            best_preds = valid_folds[[\"prediction\", \"jaccard\"]].copy()\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Model. score: {best_score:.4f}, loss: {best_loss:.4f}\")\n",
        "\n",
        "            model_subdir = MODEL_DIR + f\"fold{fold}/\"\n",
        "            os.makedirs(model_subdir, exist_ok=True)\n",
        "            best_preds.to_csv(f\"{model_subdir}/best_preds.csv\")\n",
        "            torch.save(model.state_dict(), f\"{model_subdir}/pytorch_model.bin\")\n",
        "            model.auto_config.save_pretrained(model_subdir)\n",
        "            train_dataset.tokenizer.save_pretrained(model_subdir)\n",
        "\n",
        "    valid_folds[[\"prediction\", \"jaccard\"]] = best_preds\n",
        "\n",
        "    return valid_folds, best_score, best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znc9U4s9YPqs"
      },
      "source": [
        "# üöÄ Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPgK02eYRCX"
      },
      "source": [
        "def main():\n",
        "    # ====================================================\n",
        "    # Training\n",
        "    # ====================================================\n",
        "    if Config.train:\n",
        "        oof_df = pd.DataFrame()\n",
        "        oof_result = []\n",
        "        for fold in range(config.n_fold):\n",
        "            seed_torch(seed + fold)\n",
        "\n",
        "            _oof_df, score, loss = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            oof_result.append([fold, score, loss])\n",
        "\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df, fold)\n",
        "\n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        loss = statistics.mean([d[2] for d in oof_result])\n",
        "        wandb.log({\"loss\": loss})\n",
        "\n",
        "        table = wandb.Table(data=oof_result, columns = [\"fold\", \"score\", \"loss\"])\n",
        "        run.log({\"Fold Result\": table})\n",
        "        \n",
        "        # save result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "        wandb.save(OUTPUT_DIR + \"oof_df.csv\")\n",
        "\n",
        "        artifact = wandb.Artifact(config.model_name.replace('/', '-'), type='model')\n",
        "        artifact.add_dir(MODEL_DIR)\n",
        "        run.log_artifact(artifact)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Q3YuoeYiLS",
        "outputId": "466b04fc-c8cb-421d-8fe8-6f4368559047"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RemBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RemBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/8102] Elapsed 0m 1s (remain 169m 14s) Loss: 6.1292 Grad: 8.4091 LR: 0.000000 \n",
            "Epoch: [1][100/8102] Elapsed 1m 6s (remain 87m 8s) Loss: 5.5793 Grad: 8.4104 LR: 0.000061 \n",
            "Epoch: [1][200/8102] Elapsed 2m 10s (remain 85m 26s) Loss: 4.6887 Grad: 8.6200 LR: 0.000121 \n",
            "Epoch: [1][300/8102] Elapsed 3m 14s (remain 84m 11s) Loss: 4.1654 Grad: 9.8461 LR: 0.000186 \n",
            "Epoch: [1][400/8102] Elapsed 4m 19s (remain 82m 59s) Loss: 3.8244 Grad: 7.1687 LR: 0.000247 \n",
            "Epoch: [1][500/8102] Elapsed 5m 23s (remain 81m 52s) Loss: 3.5753 Grad: 32.9825 LR: 0.000307 \n",
            "Epoch: [1][600/8102] Elapsed 6m 28s (remain 80m 46s) Loss: 3.3889 Grad: 81.5098 LR: 0.000368 \n",
            "Epoch: [1][700/8102] Elapsed 7m 32s (remain 79m 41s) Loss: 3.2094 Grad: 3.9198 LR: 0.000433 \n",
            "Epoch: [1][800/8102] Elapsed 8m 37s (remain 78m 35s) Loss: 3.0531 Grad: 15.4862 LR: 0.000494 \n",
            "Epoch: [1][900/8102] Elapsed 9m 41s (remain 77m 30s) Loss: 2.8744 Grad: 23.8875 LR: 0.000554 \n",
            "Epoch: [1][1000/8102] Elapsed 10m 46s (remain 76m 24s) Loss: 2.7244 Grad: 280.5859 LR: 0.000619 \n",
            "Epoch: [1][1100/8102] Elapsed 11m 50s (remain 75m 19s) Loss: 2.5945 Grad: 21.6969 LR: 0.000680 \n",
            "Epoch: [1][1200/8102] Elapsed 12m 55s (remain 74m 13s) Loss: 2.4844 Grad: 10.4552 LR: 0.000740 \n",
            "Epoch: [1][1300/8102] Elapsed 13m 59s (remain 73m 8s) Loss: 2.3786 Grad: 32.1452 LR: 0.000801 \n",
            "Epoch: [1][1400/8102] Elapsed 15m 4s (remain 72m 4s) Loss: 2.2976 Grad: 27.0457 LR: 0.000866 \n",
            "Epoch: [1][1500/8102] Elapsed 16m 8s (remain 70m 59s) Loss: 2.2123 Grad: 11.7533 LR: 0.000926 \n",
            "Epoch: [1][1600/8102] Elapsed 17m 13s (remain 69m 54s) Loss: 2.1638 Grad: 18.9114 LR: 0.000987 \n",
            "Epoch: [1][1700/8102] Elapsed 18m 17s (remain 68m 50s) Loss: 2.1084 Grad: 32.3650 LR: 0.001000 \n",
            "Epoch: [1][1800/8102] Elapsed 19m 22s (remain 67m 45s) Loss: 2.0570 Grad: 4.3414 LR: 0.001000 \n",
            "Epoch: [1][1900/8102] Elapsed 20m 26s (remain 66m 40s) Loss: 2.0067 Grad: 14.9441 LR: 0.000999 \n",
            "Epoch: [1][2000/8102] Elapsed 21m 30s (remain 65m 36s) Loss: 1.9638 Grad: 21.0589 LR: 0.000998 \n",
            "Epoch: [1][2100/8102] Elapsed 22m 35s (remain 64m 31s) Loss: 1.9238 Grad: 11.0675 LR: 0.000997 \n",
            "Epoch: [1][2200/8102] Elapsed 23m 39s (remain 63m 26s) Loss: 1.8840 Grad: 4.6774 LR: 0.000996 \n",
            "Epoch: [1][2300/8102] Elapsed 24m 44s (remain 62m 22s) Loss: 1.8550 Grad: 16.0186 LR: 0.000995 \n",
            "Epoch: [1][2400/8102] Elapsed 25m 48s (remain 61m 17s) Loss: 1.8145 Grad: 21.6136 LR: 0.000993 \n",
            "Epoch: [1][2500/8102] Elapsed 26m 53s (remain 60m 12s) Loss: 1.7814 Grad: 40.3137 LR: 0.000991 \n",
            "Epoch: [1][2600/8102] Elapsed 27m 57s (remain 59m 8s) Loss: 1.7593 Grad: 10.9554 LR: 0.000989 \n",
            "Epoch: [1][2700/8102] Elapsed 29m 2s (remain 58m 3s) Loss: 1.7257 Grad: 16.7332 LR: 0.000987 \n",
            "Epoch: [1][2800/8102] Elapsed 30m 6s (remain 56m 59s) Loss: 1.6975 Grad: 1.8762 LR: 0.000984 \n",
            "Epoch: [1][2900/8102] Elapsed 31m 11s (remain 55m 54s) Loss: 1.6712 Grad: 9.0218 LR: 0.000981 \n",
            "Epoch: [1][3000/8102] Elapsed 32m 15s (remain 54m 50s) Loss: 1.6409 Grad: 18.1946 LR: 0.000978 \n",
            "Epoch: [1][3100/8102] Elapsed 33m 20s (remain 53m 45s) Loss: 1.6197 Grad: 16.9778 LR: 0.000975 \n",
            "Epoch: [1][3200/8102] Elapsed 34m 24s (remain 52m 41s) Loss: 1.6009 Grad: 8.1655 LR: 0.000971 \n",
            "Epoch: [1][3300/8102] Elapsed 35m 29s (remain 51m 36s) Loss: 1.5803 Grad: 10.6568 LR: 0.000968 \n",
            "Epoch: [1][3400/8102] Elapsed 36m 33s (remain 50m 32s) Loss: 1.5589 Grad: 9.1327 LR: 0.000964 \n",
            "Epoch: [1][3500/8102] Elapsed 37m 38s (remain 49m 27s) Loss: 1.5405 Grad: 1.0190 LR: 0.000959 \n",
            "Epoch: [1][3600/8102] Elapsed 38m 42s (remain 48m 23s) Loss: 1.5278 Grad: 3.8846 LR: 0.000955 \n",
            "Epoch: [1][3700/8102] Elapsed 39m 47s (remain 47m 18s) Loss: 1.5099 Grad: 32.1386 LR: 0.000951 \n",
            "Epoch: [1][3800/8102] Elapsed 40m 51s (remain 46m 14s) Loss: 1.4929 Grad: 41.8831 LR: 0.000946 \n",
            "Epoch: [1][3900/8102] Elapsed 41m 56s (remain 45m 9s) Loss: 1.4765 Grad: 7.6754 LR: 0.000941 \n",
            "Epoch: [1][4000/8102] Elapsed 43m 0s (remain 44m 5s) Loss: 1.4604 Grad: 4.9010 LR: 0.000936 \n",
            "Epoch: [1][4100/8102] Elapsed 44m 5s (remain 43m 0s) Loss: 1.4509 Grad: 11.7467 LR: 0.000930 \n",
            "Epoch: [1][4200/8102] Elapsed 45m 9s (remain 41m 56s) Loss: 1.4361 Grad: 5.1650 LR: 0.000925 \n",
            "Epoch: [1][4300/8102] Elapsed 46m 14s (remain 40m 51s) Loss: 1.4220 Grad: 13.9300 LR: 0.000919 \n",
            "Epoch: [1][4400/8102] Elapsed 47m 18s (remain 39m 47s) Loss: 1.4123 Grad: 30.0308 LR: 0.000913 \n",
            "Epoch: [1][4500/8102] Elapsed 48m 23s (remain 38m 42s) Loss: 1.3993 Grad: 14.0218 LR: 0.000907 \n",
            "Epoch: [1][4600/8102] Elapsed 49m 27s (remain 37m 38s) Loss: 1.3886 Grad: 6.0728 LR: 0.000900 \n",
            "Epoch: [1][4700/8102] Elapsed 50m 32s (remain 36m 33s) Loss: 1.3760 Grad: 10.3837 LR: 0.000894 \n",
            "Epoch: [1][4800/8102] Elapsed 51m 36s (remain 35m 29s) Loss: 1.3630 Grad: 30.8322 LR: 0.000887 \n",
            "Epoch: [1][4900/8102] Elapsed 52m 41s (remain 34m 24s) Loss: 1.3535 Grad: 0.0254 LR: 0.000880 \n",
            "Epoch: [1][5000/8102] Elapsed 53m 45s (remain 33m 20s) Loss: 1.3467 Grad: 5.6801 LR: 0.000873 \n",
            "Epoch: [1][5100/8102] Elapsed 54m 50s (remain 32m 15s) Loss: 1.3364 Grad: 8.4516 LR: 0.000866 \n",
            "Epoch: [1][5200/8102] Elapsed 55m 54s (remain 31m 11s) Loss: 1.3271 Grad: 9.0744 LR: 0.000858 \n",
            "Epoch: [1][5300/8102] Elapsed 56m 59s (remain 30m 6s) Loss: 1.3173 Grad: 4.8487 LR: 0.000851 \n",
            "Epoch: [1][5400/8102] Elapsed 58m 3s (remain 29m 2s) Loss: 1.3057 Grad: 3.3135 LR: 0.000843 \n",
            "Epoch: [1][5500/8102] Elapsed 59m 8s (remain 27m 57s) Loss: 1.2960 Grad: 6.3226 LR: 0.000835 \n",
            "Epoch: [1][5600/8102] Elapsed 60m 12s (remain 26m 53s) Loss: 1.2903 Grad: 9.5486 LR: 0.000827 \n",
            "Epoch: [1][5700/8102] Elapsed 61m 17s (remain 25m 48s) Loss: 1.2843 Grad: 9.4188 LR: 0.000819 \n",
            "Epoch: [1][5800/8102] Elapsed 62m 21s (remain 24m 44s) Loss: 1.2788 Grad: 12.6794 LR: 0.000811 \n",
            "Epoch: [1][5900/8102] Elapsed 63m 26s (remain 23m 39s) Loss: 1.2690 Grad: 17.7904 LR: 0.000802 \n",
            "Epoch: [1][6000/8102] Elapsed 64m 30s (remain 22m 35s) Loss: 1.2610 Grad: 2.6388 LR: 0.000793 \n",
            "Epoch: [1][6100/8102] Elapsed 65m 35s (remain 21m 30s) Loss: 1.2546 Grad: 8.3144 LR: 0.000785 \n",
            "Epoch: [1][6200/8102] Elapsed 66m 39s (remain 20m 26s) Loss: 1.2489 Grad: 12.5893 LR: 0.000776 \n",
            "Epoch: [1][6300/8102] Elapsed 67m 44s (remain 19m 21s) Loss: 1.2436 Grad: 4.7182 LR: 0.000766 \n",
            "Epoch: [1][6400/8102] Elapsed 68m 48s (remain 18m 17s) Loss: 1.2381 Grad: 16.7351 LR: 0.000757 \n",
            "Epoch: [1][6500/8102] Elapsed 69m 53s (remain 17m 12s) Loss: 1.2318 Grad: 6.5866 LR: 0.000748 \n",
            "Epoch: [1][6600/8102] Elapsed 70m 57s (remain 16m 8s) Loss: 1.2272 Grad: 11.2837 LR: 0.000738 \n",
            "Epoch: [1][6700/8102] Elapsed 72m 2s (remain 15m 3s) Loss: 1.2212 Grad: 8.9501 LR: 0.000729 \n",
            "Epoch: [1][6800/8102] Elapsed 73m 6s (remain 13m 59s) Loss: 1.2160 Grad: 6.8133 LR: 0.000720 \n",
            "Epoch: [1][6900/8102] Elapsed 74m 11s (remain 12m 54s) Loss: 1.2095 Grad: 6.0311 LR: 0.000710 \n",
            "Epoch: [1][7000/8102] Elapsed 75m 15s (remain 11m 50s) Loss: 1.2028 Grad: 5.1863 LR: 0.000700 \n",
            "Epoch: [1][7100/8102] Elapsed 76m 20s (remain 10m 45s) Loss: 1.1954 Grad: 10.6947 LR: 0.000690 \n",
            "Epoch: [1][7200/8102] Elapsed 77m 24s (remain 9m 41s) Loss: 1.1894 Grad: 10.0929 LR: 0.000680 \n",
            "Epoch: [1][7300/8102] Elapsed 78m 29s (remain 8m 36s) Loss: 1.1845 Grad: 14.8260 LR: 0.000670 \n",
            "Epoch: [1][7400/8102] Elapsed 79m 33s (remain 7m 32s) Loss: 1.1786 Grad: 13.0966 LR: 0.000660 \n",
            "Epoch: [1][7500/8102] Elapsed 80m 38s (remain 6m 27s) Loss: 1.1735 Grad: 7.5686 LR: 0.000650 \n",
            "Epoch: [1][7600/8102] Elapsed 81m 42s (remain 5m 23s) Loss: 1.1682 Grad: 9.8104 LR: 0.000640 \n",
            "Epoch: [1][7700/8102] Elapsed 82m 47s (remain 4m 18s) Loss: 1.1634 Grad: 2.6546 LR: 0.000629 \n",
            "Epoch: [1][7800/8102] Elapsed 83m 51s (remain 3m 14s) Loss: 1.1577 Grad: 1.1752 LR: 0.000618 \n",
            "Epoch: [1][7900/8102] Elapsed 84m 56s (remain 2m 9s) Loss: 1.1531 Grad: 10.7693 LR: 0.000608 \n",
            "Epoch: [1][8000/8102] Elapsed 86m 0s (remain 1m 5s) Loss: 1.1483 Grad: 12.6066 LR: 0.000597 \n",
            "Epoch: [1][8100/8102] Elapsed 87m 5s (remain 0m 0s) Loss: 1.1443 Grad: 8.6488 LR: 0.000587 \n",
            "Epoch: [1][8101/8102] Elapsed 87m 5s (remain 0m 0s) Loss: 1.1442 Grad: 8.7857 LR: 0.000587 \n",
            "EVAL: [0/1014] Elapsed 0m 0s (remain 13m 35s) Loss: 0.1764 \n",
            "EVAL: [100/1014] Elapsed 0m 21s (remain 3m 14s) Loss: 0.2836 \n",
            "EVAL: [200/1014] Elapsed 0m 42s (remain 2m 50s) Loss: 0.2877 \n",
            "EVAL: [300/1014] Elapsed 1m 2s (remain 2m 29s) Loss: 0.2431 \n",
            "EVAL: [400/1014] Elapsed 1m 23s (remain 2m 7s) Loss: 0.2475 \n",
            "EVAL: [500/1014] Elapsed 1m 44s (remain 1m 46s) Loss: 0.2580 \n",
            "EVAL: [600/1014] Elapsed 2m 5s (remain 1m 26s) Loss: 0.2480 \n",
            "EVAL: [700/1014] Elapsed 2m 25s (remain 1m 5s) Loss: 0.2394 \n",
            "EVAL: [800/1014] Elapsed 2m 46s (remain 0m 44s) Loss: 0.2470 \n",
            "EVAL: [900/1014] Elapsed 3m 7s (remain 0m 23s) Loss: 0.2473 \n",
            "EVAL: [1000/1014] Elapsed 3m 28s (remain 0m 2s) Loss: 0.2440 \n",
            "EVAL: [1013/1014] Elapsed 3m 30s (remain 0m 0s) Loss: 0.2475 \n",
            "Post-processing 223 example predictions split into 3040 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6327051747455334, Train Loss: 1.1442, Val Loss: 0.2475, Time: 5438s\n",
            "Epoch 1 - Save Best Model. score: 0.6327, loss: 0.2475\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/8102] Elapsed 0m 1s (remain 169m 18s) Loss: 0.4084 Grad: 3.1091 LR: 0.000587 \n",
            "Epoch: [2][100/8102] Elapsed 1m 5s (remain 86m 55s) Loss: 0.4519 Grad: 8.0088 LR: 0.000576 \n",
            "Epoch: [2][200/8102] Elapsed 2m 10s (remain 85m 23s) Loss: 0.4660 Grad: 15.7312 LR: 0.000566 \n",
            "Epoch: [2][300/8102] Elapsed 3m 14s (remain 84m 12s) Loss: 0.4739 Grad: 11.7190 LR: 0.000555 \n",
            "Epoch: [2][400/8102] Elapsed 4m 19s (remain 83m 2s) Loss: 0.4805 Grad: 6.2941 LR: 0.000544 \n",
            "Epoch: [2][500/8102] Elapsed 5m 23s (remain 81m 53s) Loss: 0.4547 Grad: 10.0011 LR: 0.000534 \n",
            "Epoch: [2][600/8102] Elapsed 6m 28s (remain 80m 45s) Loss: 0.4516 Grad: 8.4872 LR: 0.000523 \n",
            "Epoch: [2][700/8102] Elapsed 7m 32s (remain 79m 41s) Loss: 0.4546 Grad: 0.4288 LR: 0.000512 \n",
            "Epoch: [2][800/8102] Elapsed 8m 37s (remain 78m 34s) Loss: 0.4617 Grad: 13.2608 LR: 0.000501 \n",
            "Epoch: [2][900/8102] Elapsed 9m 41s (remain 77m 28s) Loss: 0.4514 Grad: 13.8560 LR: 0.000491 \n",
            "Epoch: [2][1000/8102] Elapsed 10m 46s (remain 76m 24s) Loss: 0.4542 Grad: 25.5120 LR: 0.000479 \n",
            "Epoch: [2][1100/8102] Elapsed 11m 50s (remain 75m 19s) Loss: 0.4608 Grad: 2.9193 LR: 0.000469 \n",
            "Epoch: [2][1200/8102] Elapsed 12m 55s (remain 74m 14s) Loss: 0.4602 Grad: 12.4563 LR: 0.000458 \n",
            "Epoch: [2][1300/8102] Elapsed 13m 59s (remain 73m 9s) Loss: 0.4563 Grad: 21.1243 LR: 0.000448 \n",
            "Epoch: [2][1400/8102] Elapsed 15m 4s (remain 72m 4s) Loss: 0.4667 Grad: 1.2439 LR: 0.000436 \n",
            "Epoch: [2][1500/8102] Elapsed 16m 8s (remain 70m 59s) Loss: 0.4619 Grad: 7.0409 LR: 0.000426 \n",
            "Epoch: [2][1600/8102] Elapsed 17m 13s (remain 69m 54s) Loss: 0.4621 Grad: 9.7738 LR: 0.000416 \n",
            "Epoch: [2][1700/8102] Elapsed 18m 17s (remain 68m 49s) Loss: 0.4599 Grad: 15.8471 LR: 0.000404 \n",
            "Epoch: [2][1800/8102] Elapsed 19m 21s (remain 67m 45s) Loss: 0.4579 Grad: 1.6320 LR: 0.000394 \n",
            "Epoch: [2][1900/8102] Elapsed 20m 26s (remain 66m 40s) Loss: 0.4556 Grad: 11.1892 LR: 0.000384 \n",
            "Epoch: [2][2000/8102] Elapsed 21m 30s (remain 65m 35s) Loss: 0.4554 Grad: 10.5010 LR: 0.000374 \n",
            "Epoch: [2][2100/8102] Elapsed 22m 35s (remain 64m 31s) Loss: 0.4523 Grad: 0.3890 LR: 0.000363 \n",
            "Epoch: [2][2200/8102] Elapsed 23m 39s (remain 63m 26s) Loss: 0.4521 Grad: 4.9404 LR: 0.000353 \n",
            "Epoch: [2][2300/8102] Elapsed 24m 44s (remain 62m 21s) Loss: 0.4496 Grad: 7.4745 LR: 0.000342 \n",
            "Epoch: [2][2400/8102] Elapsed 25m 48s (remain 61m 17s) Loss: 0.4450 Grad: 16.3052 LR: 0.000332 \n",
            "Epoch: [2][2500/8102] Elapsed 26m 53s (remain 60m 12s) Loss: 0.4441 Grad: 4.7959 LR: 0.000322 \n",
            "Epoch: [2][2600/8102] Elapsed 27m 57s (remain 59m 8s) Loss: 0.4449 Grad: 10.7513 LR: 0.000312 \n",
            "Epoch: [2][2700/8102] Elapsed 29m 2s (remain 58m 3s) Loss: 0.4413 Grad: 5.3260 LR: 0.000302 \n",
            "Epoch: [2][2800/8102] Elapsed 30m 6s (remain 56m 59s) Loss: 0.4431 Grad: 6.3807 LR: 0.000292 \n",
            "Epoch: [2][2900/8102] Elapsed 31m 11s (remain 55m 54s) Loss: 0.4430 Grad: 0.9270 LR: 0.000282 \n",
            "Epoch: [2][3000/8102] Elapsed 32m 15s (remain 54m 49s) Loss: 0.4415 Grad: 10.7998 LR: 0.000273 \n",
            "Epoch: [2][3100/8102] Elapsed 33m 19s (remain 53m 45s) Loss: 0.4378 Grad: 17.0494 LR: 0.000263 \n",
            "Epoch: [2][3200/8102] Elapsed 34m 24s (remain 52m 40s) Loss: 0.4352 Grad: 2.5241 LR: 0.000254 \n",
            "Epoch: [2][3300/8102] Elapsed 35m 28s (remain 51m 36s) Loss: 0.4335 Grad: 7.5078 LR: 0.000245 \n",
            "Epoch: [2][3400/8102] Elapsed 36m 33s (remain 50m 31s) Loss: 0.4324 Grad: 5.9741 LR: 0.000236 \n",
            "Epoch: [2][3500/8102] Elapsed 37m 37s (remain 49m 27s) Loss: 0.4334 Grad: 1.5558 LR: 0.000226 \n",
            "Epoch: [2][3600/8102] Elapsed 38m 42s (remain 48m 22s) Loss: 0.4315 Grad: 16.5399 LR: 0.000217 \n",
            "Epoch: [2][3700/8102] Elapsed 39m 46s (remain 47m 18s) Loss: 0.4332 Grad: 17.3288 LR: 0.000209 \n",
            "Epoch: [2][3800/8102] Elapsed 40m 51s (remain 46m 13s) Loss: 0.4311 Grad: 6.1181 LR: 0.000199 \n",
            "Epoch: [2][3900/8102] Elapsed 41m 55s (remain 45m 8s) Loss: 0.4282 Grad: 3.7356 LR: 0.000191 \n",
            "Epoch: [2][4000/8102] Elapsed 42m 59s (remain 44m 4s) Loss: 0.4288 Grad: 0.7326 LR: 0.000183 \n",
            "Epoch: [2][4100/8102] Elapsed 44m 4s (remain 42m 59s) Loss: 0.4275 Grad: 7.6667 LR: 0.000175 \n",
            "Epoch: [2][4200/8102] Elapsed 45m 8s (remain 41m 55s) Loss: 0.4288 Grad: 3.0883 LR: 0.000166 \n",
            "Epoch: [2][4300/8102] Elapsed 46m 13s (remain 40m 50s) Loss: 0.4297 Grad: 10.6173 LR: 0.000159 \n",
            "Epoch: [2][4400/8102] Elapsed 47m 17s (remain 39m 46s) Loss: 0.4275 Grad: 5.9626 LR: 0.000151 \n",
            "Epoch: [2][4500/8102] Elapsed 48m 22s (remain 38m 41s) Loss: 0.4277 Grad: 9.2582 LR: 0.000143 \n",
            "Epoch: [2][4600/8102] Elapsed 49m 26s (remain 37m 37s) Loss: 0.4257 Grad: 2.4576 LR: 0.000136 \n",
            "Epoch: [2][4700/8102] Elapsed 50m 31s (remain 36m 32s) Loss: 0.4246 Grad: 7.7252 LR: 0.000128 \n",
            "Epoch: [2][4800/8102] Elapsed 51m 35s (remain 35m 28s) Loss: 0.4244 Grad: 6.3897 LR: 0.000121 \n",
            "Epoch: [2][4900/8102] Elapsed 52m 40s (remain 34m 23s) Loss: 0.4212 Grad: 1.4615 LR: 0.000114 \n",
            "Epoch: [2][5000/8102] Elapsed 53m 44s (remain 33m 19s) Loss: 0.4217 Grad: 6.7441 LR: 0.000108 \n",
            "Epoch: [2][5100/8102] Elapsed 54m 48s (remain 32m 14s) Loss: 0.4216 Grad: 13.0837 LR: 0.000101 \n",
            "Epoch: [2][5200/8102] Elapsed 55m 53s (remain 31m 10s) Loss: 0.4222 Grad: 14.7243 LR: 0.000094 \n",
            "Epoch: [2][5300/8102] Elapsed 56m 57s (remain 30m 5s) Loss: 0.4205 Grad: 0.4363 LR: 0.000088 \n",
            "Epoch: [2][5400/8102] Elapsed 58m 2s (remain 29m 1s) Loss: 0.4184 Grad: 8.4503 LR: 0.000082 \n",
            "Epoch: [2][5500/8102] Elapsed 59m 6s (remain 27m 56s) Loss: 0.4170 Grad: 8.7036 LR: 0.000077 \n",
            "Epoch: [2][5600/8102] Elapsed 60m 11s (remain 26m 52s) Loss: 0.4157 Grad: 2.5389 LR: 0.000071 \n",
            "Epoch: [2][5700/8102] Elapsed 61m 15s (remain 25m 47s) Loss: 0.4157 Grad: 4.6920 LR: 0.000065 \n",
            "Epoch: [2][5800/8102] Elapsed 62m 19s (remain 24m 43s) Loss: 0.4142 Grad: 8.1259 LR: 0.000060 \n",
            "Epoch: [2][5900/8102] Elapsed 63m 24s (remain 23m 38s) Loss: 0.4135 Grad: 11.2814 LR: 0.000055 \n",
            "Epoch: [2][6000/8102] Elapsed 64m 28s (remain 22m 34s) Loss: 0.4109 Grad: 1.5612 LR: 0.000050 \n",
            "Epoch: [2][6100/8102] Elapsed 65m 33s (remain 21m 29s) Loss: 0.4113 Grad: 13.8020 LR: 0.000046 \n",
            "Epoch: [2][6200/8102] Elapsed 66m 37s (remain 20m 25s) Loss: 0.4113 Grad: 15.9903 LR: 0.000041 \n",
            "Epoch: [2][6300/8102] Elapsed 67m 42s (remain 19m 21s) Loss: 0.4111 Grad: 9.8495 LR: 0.000037 \n",
            "Epoch: [2][6400/8102] Elapsed 68m 46s (remain 18m 16s) Loss: 0.4107 Grad: 5.9360 LR: 0.000033 \n",
            "Epoch: [2][6500/8102] Elapsed 69m 50s (remain 17m 12s) Loss: 0.4100 Grad: 9.1231 LR: 0.000030 \n",
            "Epoch: [2][6600/8102] Elapsed 70m 55s (remain 16m 7s) Loss: 0.4091 Grad: 10.8783 LR: 0.000026 \n",
            "Epoch: [2][6700/8102] Elapsed 71m 59s (remain 15m 3s) Loss: 0.4086 Grad: 9.7936 LR: 0.000023 \n",
            "Epoch: [2][6800/8102] Elapsed 73m 4s (remain 13m 58s) Loss: 0.4072 Grad: 9.0237 LR: 0.000020 \n",
            "Epoch: [2][6900/8102] Elapsed 74m 8s (remain 12m 54s) Loss: 0.4057 Grad: 11.2169 LR: 0.000017 \n",
            "Epoch: [2][7000/8102] Elapsed 75m 13s (remain 11m 49s) Loss: 0.4058 Grad: 0.5674 LR: 0.000014 \n",
            "Epoch: [2][7100/8102] Elapsed 76m 17s (remain 10m 45s) Loss: 0.4065 Grad: 9.5967 LR: 0.000012 \n",
            "Epoch: [2][7200/8102] Elapsed 77m 22s (remain 9m 40s) Loss: 0.4067 Grad: 4.5862 LR: 0.000009 \n",
            "Epoch: [2][7300/8102] Elapsed 78m 26s (remain 8m 36s) Loss: 0.4061 Grad: 11.1455 LR: 0.000007 \n",
            "Epoch: [2][7400/8102] Elapsed 79m 31s (remain 7m 31s) Loss: 0.4049 Grad: 6.9944 LR: 0.000006 \n",
            "Epoch: [2][7500/8102] Elapsed 80m 35s (remain 6m 27s) Loss: 0.4041 Grad: 3.9403 LR: 0.000004 \n",
            "Epoch: [2][7600/8102] Elapsed 81m 39s (remain 5m 22s) Loss: 0.4046 Grad: 5.1689 LR: 0.000003 \n",
            "Epoch: [2][7700/8102] Elapsed 82m 44s (remain 4m 18s) Loss: 0.4042 Grad: 1.1475 LR: 0.000002 \n",
            "Epoch: [2][7800/8102] Elapsed 83m 48s (remain 3m 14s) Loss: 0.4044 Grad: 11.6721 LR: 0.000001 \n",
            "Epoch: [2][7900/8102] Elapsed 84m 53s (remain 2m 9s) Loss: 0.4038 Grad: 10.0780 LR: 0.000000 \n",
            "Epoch: [2][8000/8102] Elapsed 85m 57s (remain 1m 5s) Loss: 0.4038 Grad: 16.1128 LR: 0.000000 \n",
            "Epoch: [2][8100/8102] Elapsed 87m 2s (remain 0m 0s) Loss: 0.4030 Grad: 3.8680 LR: 0.000000 \n",
            "Epoch: [2][8101/8102] Elapsed 87m 2s (remain 0m 0s) Loss: 0.4030 Grad: 7.0810 LR: 0.000000 \n",
            "EVAL: [0/1014] Elapsed 0m 0s (remain 13m 36s) Loss: 0.0404 \n",
            "EVAL: [100/1014] Elapsed 0m 21s (remain 3m 14s) Loss: 0.2549 \n",
            "EVAL: [200/1014] Elapsed 0m 42s (remain 2m 50s) Loss: 0.2791 \n",
            "EVAL: [300/1014] Elapsed 1m 3s (remain 2m 29s) Loss: 0.2311 \n",
            "EVAL: [400/1014] Elapsed 1m 23s (remain 2m 8s) Loss: 0.2373 \n",
            "EVAL: [500/1014] Elapsed 1m 44s (remain 1m 46s) Loss: 0.2503 \n",
            "EVAL: [600/1014] Elapsed 2m 5s (remain 1m 26s) Loss: 0.2380 \n",
            "EVAL: [700/1014] Elapsed 2m 25s (remain 1m 5s) Loss: 0.2314 \n",
            "EVAL: [800/1014] Elapsed 2m 46s (remain 0m 44s) Loss: 0.2387 \n",
            "EVAL: [900/1014] Elapsed 3m 7s (remain 0m 23s) Loss: 0.2433 \n",
            "EVAL: [1000/1014] Elapsed 3m 28s (remain 0m 2s) Loss: 0.2439 \n",
            "EVAL: [1013/1014] Elapsed 3m 30s (remain 0m 0s) Loss: 0.2481 \n",
            "Post-processing 223 example predictions split into 3040 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.6758844045839562, Train Loss: 0.4030, Val Loss: 0.2481, Time: 5435s\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.62448\n",
            "========== fold: 1 training ==========\n",
            "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RemBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RemBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/8125] Elapsed 0m 1s (remain 167m 58s) Loss: 5.8760 Grad: 4.7725 LR: 0.000000 \n",
            "Epoch: [1][100/8125] Elapsed 1m 5s (remain 86m 59s) Loss: 5.6453 Grad: 7.5866 LR: 0.000060 \n",
            "Epoch: [1][200/8125] Elapsed 2m 10s (remain 85m 29s) Loss: 4.6773 Grad: 11.8688 LR: 0.000121 \n",
            "Epoch: [1][300/8125] Elapsed 3m 14s (remain 84m 20s) Loss: 4.1481 Grad: 16.6983 LR: 0.000185 \n",
            "Epoch: [1][400/8125] Elapsed 4m 19s (remain 83m 10s) Loss: 3.7505 Grad: 6.0406 LR: 0.000246 \n",
            "Epoch: [1][500/8125] Elapsed 5m 23s (remain 82m 4s) Loss: 3.4759 Grad: 17.6416 LR: 0.000306 \n",
            "Epoch: [1][600/8125] Elapsed 6m 28s (remain 80m 57s) Loss: 3.2630 Grad: 44.9304 LR: 0.000366 \n",
            "Epoch: [1][700/8125] Elapsed 7m 32s (remain 79m 52s) Loss: 3.1298 Grad: 6.1753 LR: 0.000431 \n",
            "Epoch: [1][800/8125] Elapsed 8m 36s (remain 78m 46s) Loss: 3.0381 Grad: 18.6347 LR: 0.000491 \n",
            "Epoch: [1][900/8125] Elapsed 9m 41s (remain 77m 41s) Loss: 2.8926 Grad: 14.0206 LR: 0.000552 \n",
            "Epoch: [1][1000/8125] Elapsed 10m 45s (remain 76m 37s) Loss: 2.7761 Grad: 51.5051 LR: 0.000616 \n",
            "Epoch: [1][1100/8125] Elapsed 11m 50s (remain 75m 31s) Loss: 2.6291 Grad: 12.0419 LR: 0.000677 \n",
            "Epoch: [1][1200/8125] Elapsed 12m 54s (remain 74m 26s) Loss: 2.4985 Grad: 7.6713 LR: 0.000737 \n",
            "Epoch: [1][1300/8125] Elapsed 13m 59s (remain 73m 21s) Loss: 2.3938 Grad: 12.4640 LR: 0.000797 \n",
            "Epoch: [1][1400/8125] Elapsed 15m 3s (remain 72m 17s) Loss: 2.3008 Grad: 7.7244 LR: 0.000862 \n",
            "Epoch: [1][1500/8125] Elapsed 16m 8s (remain 71m 12s) Loss: 2.2301 Grad: 5.5519 LR: 0.000922 \n",
            "Epoch: [1][1600/8125] Elapsed 17m 12s (remain 70m 8s) Loss: 2.1710 Grad: 12.6455 LR: 0.000983 \n",
            "Epoch: [1][1700/8125] Elapsed 18m 17s (remain 69m 3s) Loss: 2.1192 Grad: 13.4136 LR: 0.001000 \n",
            "Epoch: [1][1800/8125] Elapsed 19m 21s (remain 67m 58s) Loss: 2.0610 Grad: 13.4314 LR: 0.001000 \n",
            "Epoch: [1][1900/8125] Elapsed 20m 25s (remain 66m 53s) Loss: 2.0061 Grad: 14.4809 LR: 0.000999 \n",
            "Epoch: [1][2000/8125] Elapsed 21m 30s (remain 65m 49s) Loss: 1.9610 Grad: 11.2764 LR: 0.000998 \n",
            "Epoch: [1][2100/8125] Elapsed 22m 35s (remain 64m 45s) Loss: 1.9154 Grad: 2.4910 LR: 0.000997 \n",
            "Epoch: [1][2200/8125] Elapsed 23m 39s (remain 63m 40s) Loss: 1.8788 Grad: 6.6991 LR: 0.000996 \n",
            "Epoch: [1][2300/8125] Elapsed 24m 43s (remain 62m 35s) Loss: 1.8350 Grad: 13.4284 LR: 0.000995 \n",
            "Epoch: [1][2400/8125] Elapsed 25m 48s (remain 61m 31s) Loss: 1.8012 Grad: 14.3040 LR: 0.000993 \n",
            "Epoch: [1][2500/8125] Elapsed 26m 52s (remain 60m 26s) Loss: 1.7692 Grad: 8.1265 LR: 0.000991 \n",
            "Epoch: [1][2600/8125] Elapsed 27m 57s (remain 59m 21s) Loss: 1.7413 Grad: 17.6420 LR: 0.000989 \n",
            "Epoch: [1][2700/8125] Elapsed 29m 1s (remain 58m 17s) Loss: 1.7165 Grad: 11.4779 LR: 0.000987 \n",
            "Epoch: [1][2800/8125] Elapsed 30m 6s (remain 57m 12s) Loss: 1.6917 Grad: 6.2577 LR: 0.000984 \n",
            "Epoch: [1][2900/8125] Elapsed 31m 10s (remain 56m 8s) Loss: 1.6690 Grad: 11.6748 LR: 0.000981 \n",
            "Epoch: [1][3000/8125] Elapsed 32m 15s (remain 55m 3s) Loss: 1.6482 Grad: 16.5278 LR: 0.000978 \n",
            "Epoch: [1][3100/8125] Elapsed 33m 19s (remain 53m 59s) Loss: 1.6306 Grad: 22.8413 LR: 0.000975 \n",
            "Epoch: [1][3200/8125] Elapsed 34m 24s (remain 52m 55s) Loss: 1.6110 Grad: 13.3193 LR: 0.000972 \n",
            "Epoch: [1][3300/8125] Elapsed 35m 28s (remain 51m 50s) Loss: 1.5916 Grad: 11.5910 LR: 0.000968 \n",
            "Epoch: [1][3400/8125] Elapsed 36m 32s (remain 50m 46s) Loss: 1.5673 Grad: 9.4311 LR: 0.000964 \n",
            "Epoch: [1][3500/8125] Elapsed 37m 37s (remain 49m 41s) Loss: 1.5491 Grad: 1.0946 LR: 0.000960 \n",
            "Epoch: [1][3600/8125] Elapsed 38m 41s (remain 48m 37s) Loss: 1.5277 Grad: 4.2669 LR: 0.000956 \n",
            "Epoch: [1][3700/8125] Elapsed 39m 46s (remain 47m 32s) Loss: 1.5098 Grad: 5.6050 LR: 0.000951 \n",
            "Epoch: [1][3800/8125] Elapsed 40m 50s (remain 46m 28s) Loss: 1.4904 Grad: 5.9156 LR: 0.000946 \n",
            "Epoch: [1][3900/8125] Elapsed 41m 55s (remain 45m 23s) Loss: 1.4740 Grad: 7.2743 LR: 0.000941 \n",
            "Epoch: [1][4000/8125] Elapsed 42m 59s (remain 44m 19s) Loss: 1.4595 Grad: 9.1957 LR: 0.000936 \n",
            "Epoch: [1][4100/8125] Elapsed 44m 4s (remain 43m 14s) Loss: 1.4470 Grad: 18.3346 LR: 0.000931 \n",
            "Epoch: [1][4200/8125] Elapsed 45m 8s (remain 42m 10s) Loss: 1.4332 Grad: 1.9085 LR: 0.000925 \n",
            "Epoch: [1][4300/8125] Elapsed 46m 13s (remain 41m 5s) Loss: 1.4210 Grad: 9.0381 LR: 0.000920 \n",
            "Epoch: [1][4400/8125] Elapsed 47m 17s (remain 40m 1s) Loss: 1.4082 Grad: 6.4730 LR: 0.000914 \n",
            "Epoch: [1][4500/8125] Elapsed 48m 22s (remain 38m 56s) Loss: 1.3989 Grad: 20.8191 LR: 0.000907 \n",
            "Epoch: [1][4600/8125] Elapsed 49m 26s (remain 37m 52s) Loss: 1.3851 Grad: 9.2069 LR: 0.000901 \n",
            "Epoch: [1][4700/8125] Elapsed 50m 31s (remain 36m 47s) Loss: 1.3734 Grad: 21.1796 LR: 0.000895 \n",
            "Epoch: [1][4800/8125] Elapsed 51m 35s (remain 35m 43s) Loss: 1.3642 Grad: 20.7575 LR: 0.000888 \n",
            "Epoch: [1][4900/8125] Elapsed 52m 39s (remain 34m 38s) Loss: 1.3550 Grad: 1.4185 LR: 0.000881 \n",
            "Epoch: [1][5000/8125] Elapsed 53m 44s (remain 33m 34s) Loss: 1.3471 Grad: 9.2269 LR: 0.000874 \n",
            "Epoch: [1][5100/8125] Elapsed 54m 48s (remain 32m 29s) Loss: 1.3371 Grad: 11.2342 LR: 0.000867 \n",
            "Epoch: [1][5200/8125] Elapsed 55m 53s (remain 31m 25s) Loss: 1.3279 Grad: 7.9721 LR: 0.000859 \n",
            "Epoch: [1][5300/8125] Elapsed 56m 57s (remain 30m 20s) Loss: 1.3203 Grad: 1.6592 LR: 0.000852 \n",
            "Epoch: [1][5400/8125] Elapsed 58m 2s (remain 29m 16s) Loss: 1.3134 Grad: 4.2714 LR: 0.000844 \n",
            "Epoch: [1][5500/8125] Elapsed 59m 6s (remain 28m 11s) Loss: 1.3032 Grad: 13.2757 LR: 0.000837 \n",
            "Epoch: [1][5600/8125] Elapsed 60m 11s (remain 27m 7s) Loss: 1.2938 Grad: 1.0756 LR: 0.000828 \n",
            "Epoch: [1][5700/8125] Elapsed 61m 15s (remain 26m 2s) Loss: 1.2836 Grad: 9.2648 LR: 0.000820 \n",
            "Epoch: [1][5800/8125] Elapsed 62m 20s (remain 24m 58s) Loss: 1.2781 Grad: 5.0171 LR: 0.000812 \n",
            "Epoch: [1][5900/8125] Elapsed 63m 24s (remain 23m 53s) Loss: 1.2699 Grad: 20.6758 LR: 0.000803 \n",
            "Epoch: [1][6000/8125] Elapsed 64m 29s (remain 22m 49s) Loss: 1.2645 Grad: 3.7314 LR: 0.000795 \n",
            "Epoch: [1][6100/8125] Elapsed 65m 33s (remain 21m 44s) Loss: 1.2563 Grad: 7.0231 LR: 0.000786 \n",
            "Epoch: [1][6200/8125] Elapsed 66m 38s (remain 20m 40s) Loss: 1.2532 Grad: 7.3306 LR: 0.000777 \n",
            "Epoch: [1][6300/8125] Elapsed 67m 42s (remain 19m 36s) Loss: 1.2468 Grad: 2.8322 LR: 0.000768 \n",
            "Epoch: [1][6400/8125] Elapsed 68m 47s (remain 18m 31s) Loss: 1.2395 Grad: 10.3345 LR: 0.000759 \n",
            "Epoch: [1][6500/8125] Elapsed 69m 51s (remain 17m 27s) Loss: 1.2330 Grad: 11.6213 LR: 0.000750 \n",
            "Epoch: [1][6600/8125] Elapsed 70m 56s (remain 16m 22s) Loss: 1.2250 Grad: 12.1964 LR: 0.000740 \n",
            "Epoch: [1][6700/8125] Elapsed 72m 0s (remain 15m 18s) Loss: 1.2181 Grad: 9.7947 LR: 0.000731 \n",
            "Epoch: [1][6800/8125] Elapsed 73m 4s (remain 14m 13s) Loss: 1.2113 Grad: 18.0594 LR: 0.000721 \n",
            "Epoch: [1][6900/8125] Elapsed 74m 9s (remain 13m 9s) Loss: 1.2065 Grad: 5.4020 LR: 0.000712 \n",
            "Epoch: [1][7000/8125] Elapsed 75m 14s (remain 12m 4s) Loss: 1.2013 Grad: 4.2442 LR: 0.000702 \n",
            "Epoch: [1][7100/8125] Elapsed 76m 18s (remain 11m 0s) Loss: 1.1966 Grad: 9.3294 LR: 0.000692 \n",
            "Epoch: [1][7200/8125] Elapsed 77m 22s (remain 9m 55s) Loss: 1.1920 Grad: 5.1510 LR: 0.000682 \n",
            "Epoch: [1][7300/8125] Elapsed 78m 27s (remain 8m 51s) Loss: 1.1858 Grad: 14.9790 LR: 0.000672 \n",
            "Epoch: [1][7400/8125] Elapsed 79m 32s (remain 7m 46s) Loss: 1.1808 Grad: 4.0363 LR: 0.000662 \n",
            "Epoch: [1][7500/8125] Elapsed 80m 36s (remain 6m 42s) Loss: 1.1736 Grad: 12.0742 LR: 0.000652 \n",
            "Epoch: [1][7600/8125] Elapsed 81m 40s (remain 5m 37s) Loss: 1.1709 Grad: 18.9658 LR: 0.000642 \n",
            "Epoch: [1][7700/8125] Elapsed 82m 45s (remain 4m 33s) Loss: 1.1650 Grad: 0.2555 LR: 0.000631 \n",
            "Epoch: [1][7800/8125] Elapsed 83m 49s (remain 3m 28s) Loss: 1.1610 Grad: 6.9890 LR: 0.000621 \n",
            "Epoch: [1][7900/8125] Elapsed 84m 54s (remain 2m 24s) Loss: 1.1555 Grad: 8.2649 LR: 0.000610 \n",
            "Epoch: [1][8000/8125] Elapsed 85m 58s (remain 1m 19s) Loss: 1.1500 Grad: 12.5968 LR: 0.000599 \n",
            "Epoch: [1][8100/8125] Elapsed 87m 3s (remain 0m 15s) Loss: 1.1445 Grad: 1.7397 LR: 0.000589 \n",
            "Epoch: [1][8124/8125] Elapsed 87m 18s (remain 0m 0s) Loss: 1.1441 Grad: 17.8117 LR: 0.000587 \n",
            "EVAL: [0/990] Elapsed 0m 0s (remain 13m 25s) Loss: 0.8719 \n",
            "EVAL: [100/990] Elapsed 0m 21s (remain 3m 9s) Loss: 0.3073 \n",
            "EVAL: [200/990] Elapsed 0m 42s (remain 2m 45s) Loss: 0.3427 \n",
            "EVAL: [300/990] Elapsed 1m 2s (remain 2m 24s) Loss: 0.3101 \n",
            "EVAL: [400/990] Elapsed 1m 23s (remain 2m 2s) Loss: 0.3071 \n",
            "EVAL: [500/990] Elapsed 1m 44s (remain 1m 41s) Loss: 0.2857 \n",
            "EVAL: [600/990] Elapsed 2m 5s (remain 1m 21s) Loss: 0.2727 \n",
            "EVAL: [700/990] Elapsed 2m 25s (remain 1m 0s) Loss: 0.2651 \n",
            "EVAL: [800/990] Elapsed 2m 46s (remain 0m 39s) Loss: 0.2542 \n",
            "EVAL: [900/990] Elapsed 3m 7s (remain 0m 18s) Loss: 0.2514 \n",
            "EVAL: [989/990] Elapsed 3m 25s (remain 0m 0s) Loss: 0.2408 \n",
            "Post-processing 223 example predictions split into 2970 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6815271549576482, Train Loss: 1.1441, Val Loss: 0.2408, Time: 5446s\n",
            "Epoch 1 - Save Best Model. score: 0.6815, loss: 0.2408\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/8125] Elapsed 0m 1s (remain 170m 42s) Loss: 0.4696 Grad: 2.7138 LR: 0.000587 \n",
            "Epoch: [2][100/8125] Elapsed 1m 5s (remain 87m 4s) Loss: 0.4916 Grad: 1.3426 LR: 0.000576 \n",
            "Epoch: [2][200/8125] Elapsed 2m 10s (remain 85m 32s) Loss: 0.4827 Grad: 13.9217 LR: 0.000566 \n",
            "Epoch: [2][300/8125] Elapsed 3m 14s (remain 84m 22s) Loss: 0.4368 Grad: 2.9338 LR: 0.000555 \n",
            "Epoch: [2][400/8125] Elapsed 4m 19s (remain 83m 13s) Loss: 0.4307 Grad: 1.7375 LR: 0.000544 \n",
            "Epoch: [2][500/8125] Elapsed 5m 23s (remain 82m 6s) Loss: 0.4388 Grad: 7.8210 LR: 0.000534 \n",
            "Epoch: [2][600/8125] Elapsed 6m 28s (remain 80m 59s) Loss: 0.4385 Grad: 7.0106 LR: 0.000523 \n",
            "Epoch: [2][700/8125] Elapsed 7m 32s (remain 79m 53s) Loss: 0.4413 Grad: 0.2561 LR: 0.000512 \n",
            "Epoch: [2][800/8125] Elapsed 8m 37s (remain 78m 47s) Loss: 0.4373 Grad: 4.8223 LR: 0.000502 \n",
            "Epoch: [2][900/8125] Elapsed 9m 41s (remain 77m 42s) Loss: 0.4388 Grad: 8.1945 LR: 0.000491 \n",
            "Epoch: [2][1000/8125] Elapsed 10m 46s (remain 76m 37s) Loss: 0.4363 Grad: 6.4140 LR: 0.000480 \n",
            "Epoch: [2][1100/8125] Elapsed 11m 50s (remain 75m 32s) Loss: 0.4385 Grad: 0.6343 LR: 0.000469 \n",
            "Epoch: [2][1200/8125] Elapsed 12m 54s (remain 74m 27s) Loss: 0.4366 Grad: 13.1601 LR: 0.000459 \n",
            "Epoch: [2][1300/8125] Elapsed 13m 59s (remain 73m 22s) Loss: 0.4439 Grad: 8.0901 LR: 0.000448 \n",
            "Epoch: [2][1400/8125] Elapsed 15m 4s (remain 72m 18s) Loss: 0.4558 Grad: 4.6119 LR: 0.000437 \n",
            "Epoch: [2][1500/8125] Elapsed 16m 8s (remain 71m 13s) Loss: 0.4512 Grad: 16.9765 LR: 0.000427 \n",
            "Epoch: [2][1600/8125] Elapsed 17m 12s (remain 70m 9s) Loss: 0.4468 Grad: 8.3103 LR: 0.000416 \n",
            "Epoch: [2][1700/8125] Elapsed 18m 17s (remain 69m 4s) Loss: 0.4450 Grad: 12.0596 LR: 0.000405 \n",
            "Epoch: [2][1800/8125] Elapsed 19m 21s (remain 67m 59s) Loss: 0.4415 Grad: 0.9581 LR: 0.000395 \n",
            "Epoch: [2][1900/8125] Elapsed 20m 26s (remain 66m 55s) Loss: 0.4376 Grad: 11.7765 LR: 0.000384 \n",
            "Epoch: [2][2000/8125] Elapsed 21m 30s (remain 65m 50s) Loss: 0.4327 Grad: 2.7374 LR: 0.000374 \n",
            "Epoch: [2][2100/8125] Elapsed 22m 35s (remain 64m 46s) Loss: 0.4306 Grad: 6.3255 LR: 0.000363 \n",
            "Epoch: [2][2200/8125] Elapsed 23m 39s (remain 63m 41s) Loss: 0.4265 Grad: 12.8214 LR: 0.000353 \n",
            "Epoch: [2][2300/8125] Elapsed 24m 44s (remain 62m 36s) Loss: 0.4280 Grad: 11.4828 LR: 0.000343 \n",
            "Epoch: [2][2400/8125] Elapsed 25m 48s (remain 61m 32s) Loss: 0.4241 Grad: 13.0958 LR: 0.000333 \n",
            "Epoch: [2][2500/8125] Elapsed 26m 53s (remain 60m 27s) Loss: 0.4224 Grad: 6.6144 LR: 0.000323 \n",
            "Epoch: [2][2600/8125] Elapsed 27m 57s (remain 59m 23s) Loss: 0.4184 Grad: 3.1420 LR: 0.000313 \n",
            "Epoch: [2][2700/8125] Elapsed 29m 2s (remain 58m 18s) Loss: 0.4175 Grad: 11.1324 LR: 0.000303 \n",
            "Epoch: [2][2800/8125] Elapsed 30m 6s (remain 57m 14s) Loss: 0.4191 Grad: 18.2397 LR: 0.000293 \n",
            "Epoch: [2][2900/8125] Elapsed 31m 11s (remain 56m 9s) Loss: 0.4204 Grad: 1.6181 LR: 0.000283 \n",
            "Epoch: [2][3000/8125] Elapsed 32m 15s (remain 55m 5s) Loss: 0.4178 Grad: 5.2977 LR: 0.000274 \n",
            "Epoch: [2][3100/8125] Elapsed 33m 20s (remain 54m 0s) Loss: 0.4161 Grad: 7.1307 LR: 0.000264 \n",
            "Epoch: [2][3200/8125] Elapsed 34m 24s (remain 52m 56s) Loss: 0.4197 Grad: 7.2553 LR: 0.000255 \n",
            "Epoch: [2][3300/8125] Elapsed 35m 29s (remain 51m 51s) Loss: 0.4204 Grad: 12.8057 LR: 0.000245 \n",
            "Epoch: [2][3400/8125] Elapsed 36m 33s (remain 50m 47s) Loss: 0.4231 Grad: 10.4778 LR: 0.000236 \n",
            "Epoch: [2][3500/8125] Elapsed 37m 38s (remain 49m 42s) Loss: 0.4221 Grad: 3.2998 LR: 0.000227 \n",
            "Epoch: [2][3600/8125] Elapsed 38m 42s (remain 48m 38s) Loss: 0.4203 Grad: 10.6706 LR: 0.000218 \n",
            "Epoch: [2][3700/8125] Elapsed 39m 47s (remain 47m 33s) Loss: 0.4209 Grad: 12.9521 LR: 0.000210 \n",
            "Epoch: [2][3800/8125] Elapsed 40m 51s (remain 46m 29s) Loss: 0.4207 Grad: 10.5222 LR: 0.000200 \n",
            "Epoch: [2][3900/8125] Elapsed 41m 56s (remain 45m 24s) Loss: 0.4227 Grad: 6.4880 LR: 0.000192 \n",
            "Epoch: [2][4000/8125] Elapsed 43m 0s (remain 44m 20s) Loss: 0.4242 Grad: 9.2105 LR: 0.000184 \n",
            "Epoch: [2][4100/8125] Elapsed 44m 5s (remain 43m 15s) Loss: 0.4248 Grad: 6.8456 LR: 0.000176 \n",
            "Epoch: [2][4200/8125] Elapsed 45m 9s (remain 42m 11s) Loss: 0.4232 Grad: 0.0386 LR: 0.000167 \n",
            "Epoch: [2][4300/8125] Elapsed 46m 14s (remain 41m 6s) Loss: 0.4231 Grad: 6.0362 LR: 0.000159 \n",
            "Epoch: [2][4400/8125] Elapsed 47m 18s (remain 40m 2s) Loss: 0.4235 Grad: 14.1297 LR: 0.000152 \n",
            "Epoch: [2][4500/8125] Elapsed 48m 23s (remain 38m 57s) Loss: 0.4226 Grad: 9.1907 LR: 0.000144 \n",
            "Epoch: [2][4600/8125] Elapsed 49m 27s (remain 37m 53s) Loss: 0.4218 Grad: 5.2421 LR: 0.000136 \n",
            "Epoch: [2][4700/8125] Elapsed 50m 32s (remain 36m 48s) Loss: 0.4200 Grad: 17.5575 LR: 0.000129 \n",
            "Epoch: [2][4800/8125] Elapsed 51m 36s (remain 35m 44s) Loss: 0.4213 Grad: 6.8455 LR: 0.000122 \n",
            "Epoch: [2][4900/8125] Elapsed 52m 41s (remain 34m 39s) Loss: 0.4201 Grad: 2.2539 LR: 0.000115 \n",
            "Epoch: [2][5000/8125] Elapsed 53m 45s (remain 33m 35s) Loss: 0.4180 Grad: 12.0495 LR: 0.000108 \n",
            "Epoch: [2][5100/8125] Elapsed 54m 50s (remain 32m 30s) Loss: 0.4156 Grad: 13.5103 LR: 0.000102 \n",
            "Epoch: [2][5200/8125] Elapsed 55m 55s (remain 31m 26s) Loss: 0.4154 Grad: 8.6906 LR: 0.000095 \n",
            "Epoch: [2][5300/8125] Elapsed 56m 59s (remain 30m 21s) Loss: 0.4154 Grad: 2.7055 LR: 0.000089 \n",
            "Epoch: [2][5400/8125] Elapsed 58m 4s (remain 29m 17s) Loss: 0.4143 Grad: 0.4000 LR: 0.000083 \n",
            "Epoch: [2][5500/8125] Elapsed 59m 8s (remain 28m 12s) Loss: 0.4138 Grad: 14.6971 LR: 0.000077 \n",
            "Epoch: [2][5600/8125] Elapsed 60m 13s (remain 27m 8s) Loss: 0.4139 Grad: 1.9319 LR: 0.000072 \n",
            "Epoch: [2][5700/8125] Elapsed 61m 17s (remain 26m 3s) Loss: 0.4134 Grad: 7.6978 LR: 0.000066 \n",
            "Epoch: [2][5800/8125] Elapsed 62m 22s (remain 24m 59s) Loss: 0.4115 Grad: 1.3731 LR: 0.000061 \n",
            "Epoch: [2][5900/8125] Elapsed 63m 26s (remain 23m 54s) Loss: 0.4106 Grad: 18.0444 LR: 0.000056 \n",
            "Epoch: [2][6000/8125] Elapsed 64m 31s (remain 22m 50s) Loss: 0.4093 Grad: 0.0058 LR: 0.000051 \n",
            "Epoch: [2][6100/8125] Elapsed 65m 35s (remain 21m 45s) Loss: 0.4095 Grad: 3.8043 LR: 0.000047 \n",
            "Epoch: [2][6200/8125] Elapsed 66m 40s (remain 20m 41s) Loss: 0.4082 Grad: 10.4991 LR: 0.000042 \n",
            "Epoch: [2][6300/8125] Elapsed 67m 44s (remain 19m 36s) Loss: 0.4072 Grad: 5.9701 LR: 0.000038 \n",
            "Epoch: [2][6400/8125] Elapsed 68m 49s (remain 18m 32s) Loss: 0.4078 Grad: 8.0982 LR: 0.000034 \n",
            "Epoch: [2][6500/8125] Elapsed 69m 53s (remain 17m 27s) Loss: 0.4063 Grad: 4.9798 LR: 0.000030 \n",
            "Epoch: [2][6600/8125] Elapsed 70m 58s (remain 16m 23s) Loss: 0.4077 Grad: 26.4309 LR: 0.000026 \n",
            "Epoch: [2][6700/8125] Elapsed 72m 2s (remain 15m 18s) Loss: 0.4074 Grad: 4.2450 LR: 0.000023 \n",
            "Epoch: [2][6800/8125] Elapsed 73m 7s (remain 14m 14s) Loss: 0.4066 Grad: 15.1579 LR: 0.000020 \n",
            "Epoch: [2][6900/8125] Elapsed 74m 11s (remain 13m 9s) Loss: 0.4053 Grad: 7.5146 LR: 0.000017 \n",
            "Epoch: [2][7000/8125] Elapsed 75m 16s (remain 12m 5s) Loss: 0.4043 Grad: 7.3866 LR: 0.000014 \n",
            "Epoch: [2][7100/8125] Elapsed 76m 20s (remain 11m 0s) Loss: 0.4034 Grad: 13.1156 LR: 0.000012 \n",
            "Epoch: [2][7200/8125] Elapsed 77m 25s (remain 9m 56s) Loss: 0.4037 Grad: 2.2385 LR: 0.000010 \n",
            "Epoch: [2][7300/8125] Elapsed 78m 29s (remain 8m 51s) Loss: 0.4040 Grad: 13.5436 LR: 0.000008 \n",
            "Epoch: [2][7400/8125] Elapsed 79m 34s (remain 7m 47s) Loss: 0.4025 Grad: 1.3553 LR: 0.000006 \n",
            "Epoch: [2][7500/8125] Elapsed 80m 38s (remain 6m 42s) Loss: 0.4007 Grad: 14.1494 LR: 0.000004 \n",
            "Epoch: [2][7600/8125] Elapsed 81m 43s (remain 5m 38s) Loss: 0.4015 Grad: 9.6818 LR: 0.000003 \n",
            "Epoch: [2][7700/8125] Elapsed 82m 47s (remain 4m 33s) Loss: 0.4018 Grad: 4.9787 LR: 0.000002 \n",
            "Epoch: [2][7800/8125] Elapsed 83m 52s (remain 3m 29s) Loss: 0.4012 Grad: 26.6843 LR: 0.000001 \n",
            "Epoch: [2][7900/8125] Elapsed 84m 56s (remain 2m 24s) Loss: 0.3997 Grad: 11.7592 LR: 0.000001 \n",
            "Epoch: [2][8000/8125] Elapsed 86m 1s (remain 1m 19s) Loss: 0.3983 Grad: 4.3815 LR: 0.000000 \n",
            "Epoch: [2][8100/8125] Elapsed 87m 5s (remain 0m 15s) Loss: 0.3981 Grad: 2.5441 LR: 0.000000 \n",
            "Epoch: [2][8124/8125] Elapsed 87m 20s (remain 0m 0s) Loss: 0.3979 Grad: 1.4307 LR: 0.000000 \n",
            "EVAL: [0/990] Elapsed 0m 0s (remain 13m 55s) Loss: 0.6616 \n",
            "EVAL: [100/990] Elapsed 0m 21s (remain 3m 10s) Loss: 0.2628 \n",
            "EVAL: [200/990] Elapsed 0m 42s (remain 2m 46s) Loss: 0.3254 \n",
            "EVAL: [300/990] Elapsed 1m 3s (remain 2m 24s) Loss: 0.2798 \n",
            "EVAL: [400/990] Elapsed 1m 23s (remain 2m 3s) Loss: 0.2748 \n",
            "EVAL: [500/990] Elapsed 1m 44s (remain 1m 42s) Loss: 0.2600 \n",
            "EVAL: [600/990] Elapsed 2m 5s (remain 1m 21s) Loss: 0.2553 \n",
            "EVAL: [700/990] Elapsed 2m 26s (remain 1m 0s) Loss: 0.2499 \n",
            "EVAL: [800/990] Elapsed 2m 46s (remain 0m 39s) Loss: 0.2401 \n",
            "EVAL: [900/990] Elapsed 3m 7s (remain 0m 18s) Loss: 0.2378 \n",
            "EVAL: [989/990] Elapsed 3m 26s (remain 0m 0s) Loss: 0.2271 \n",
            "Post-processing 223 example predictions split into 2970 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.7006833226564168, Train Loss: 0.3979, Val Loss: 0.2271, Time: 5448s\n",
            "Epoch 2 - Save Best Model. score: 0.7007, loss: 0.2271\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.68648\n",
            "========== fold: 2 training ==========\n",
            "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RemBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RemBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/8091] Elapsed 0m 1s (remain 168m 16s) Loss: 6.1936 Grad: 4.1734 LR: 0.000000 \n",
            "Epoch: [1][100/8091] Elapsed 1m 5s (remain 86m 42s) Loss: 5.7953 Grad: 11.9863 LR: 0.000061 \n",
            "Epoch: [1][200/8091] Elapsed 2m 10s (remain 85m 14s) Loss: 4.9208 Grad: 39.1792 LR: 0.000121 \n",
            "Epoch: [1][300/8091] Elapsed 3m 14s (remain 84m 3s) Loss: 4.3081 Grad: 11.7337 LR: 0.000186 \n",
            "Epoch: [1][400/8091] Elapsed 4m 19s (remain 82m 53s) Loss: 3.9190 Grad: 6.3266 LR: 0.000247 \n",
            "Epoch: [1][500/8091] Elapsed 5m 23s (remain 81m 46s) Loss: 3.6031 Grad: 10.1611 LR: 0.000307 \n",
            "Epoch: [1][600/8091] Elapsed 6m 28s (remain 80m 38s) Loss: 3.4051 Grad: 13.1831 LR: 0.000368 \n",
            "Epoch: [1][700/8091] Elapsed 7m 32s (remain 79m 33s) Loss: 3.2278 Grad: 1.1667 LR: 0.000433 \n",
            "Epoch: [1][800/8091] Elapsed 8m 37s (remain 78m 28s) Loss: 3.0820 Grad: 6.5362 LR: 0.000494 \n",
            "Epoch: [1][900/8091] Elapsed 9m 41s (remain 77m 22s) Loss: 2.9645 Grad: 17.2495 LR: 0.000554 \n",
            "Epoch: [1][1000/8091] Elapsed 10m 46s (remain 76m 18s) Loss: 2.8064 Grad: 17.4157 LR: 0.000619 \n",
            "Epoch: [1][1100/8091] Elapsed 11m 50s (remain 75m 13s) Loss: 2.6530 Grad: 6.1089 LR: 0.000680 \n",
            "Epoch: [1][1200/8091] Elapsed 12m 57s (remain 74m 18s) Loss: 2.5365 Grad: 13.5858 LR: 0.000740 \n",
            "Epoch: [1][1300/8091] Elapsed 14m 1s (remain 73m 12s) Loss: 2.4289 Grad: 25.7328 LR: 0.000801 \n",
            "Epoch: [1][1400/8091] Elapsed 15m 6s (remain 72m 7s) Loss: 2.3283 Grad: 0.2356 LR: 0.000866 \n",
            "Epoch: [1][1500/8091] Elapsed 16m 10s (remain 71m 1s) Loss: 2.2339 Grad: 7.6343 LR: 0.000926 \n",
            "Epoch: [1][1600/8091] Elapsed 17m 15s (remain 69m 56s) Loss: 2.1578 Grad: 7.4390 LR: 0.000987 \n",
            "Epoch: [1][1700/8091] Elapsed 18m 19s (remain 68m 51s) Loss: 2.0886 Grad: 26.0314 LR: 0.001000 \n",
            "Epoch: [1][1800/8091] Elapsed 19m 24s (remain 67m 45s) Loss: 2.0324 Grad: 17.4383 LR: 0.001000 \n",
            "Epoch: [1][1900/8091] Elapsed 20m 28s (remain 66m 40s) Loss: 1.9796 Grad: 15.6757 LR: 0.000999 \n",
            "Epoch: [1][2000/8091] Elapsed 21m 33s (remain 65m 35s) Loss: 1.9344 Grad: 14.4312 LR: 0.000998 \n",
            "Epoch: [1][2100/8091] Elapsed 22m 37s (remain 64m 30s) Loss: 1.8841 Grad: 1.6571 LR: 0.000997 \n",
            "Epoch: [1][2200/8091] Elapsed 23m 42s (remain 63m 25s) Loss: 1.8510 Grad: 21.4264 LR: 0.000996 \n",
            "Epoch: [1][2300/8091] Elapsed 24m 46s (remain 62m 20s) Loss: 1.8154 Grad: 6.6448 LR: 0.000995 \n",
            "Epoch: [1][2400/8091] Elapsed 25m 51s (remain 61m 15s) Loss: 1.7773 Grad: 20.8787 LR: 0.000993 \n",
            "Epoch: [1][2500/8091] Elapsed 26m 55s (remain 60m 10s) Loss: 1.7396 Grad: 7.1921 LR: 0.000991 \n",
            "Epoch: [1][2600/8091] Elapsed 28m 0s (remain 59m 6s) Loss: 1.7120 Grad: 8.9531 LR: 0.000989 \n",
            "Epoch: [1][2700/8091] Elapsed 29m 4s (remain 58m 1s) Loss: 1.6838 Grad: 7.3949 LR: 0.000987 \n",
            "Epoch: [1][2800/8091] Elapsed 30m 9s (remain 56m 56s) Loss: 1.6649 Grad: 0.9748 LR: 0.000984 \n",
            "Epoch: [1][2900/8091] Elapsed 31m 13s (remain 55m 52s) Loss: 1.6400 Grad: 1.4053 LR: 0.000981 \n",
            "Epoch: [1][3000/8091] Elapsed 32m 18s (remain 54m 47s) Loss: 1.6179 Grad: 33.7236 LR: 0.000978 \n",
            "Epoch: [1][3100/8091] Elapsed 33m 22s (remain 53m 42s) Loss: 1.5915 Grad: 21.6470 LR: 0.000975 \n",
            "Epoch: [1][3200/8091] Elapsed 34m 27s (remain 52m 38s) Loss: 1.5710 Grad: 13.0253 LR: 0.000971 \n",
            "Epoch: [1][3300/8091] Elapsed 35m 31s (remain 51m 33s) Loss: 1.5491 Grad: 9.6617 LR: 0.000967 \n",
            "Epoch: [1][3400/8091] Elapsed 36m 36s (remain 50m 28s) Loss: 1.5321 Grad: 14.4944 LR: 0.000964 \n",
            "Epoch: [1][3500/8091] Elapsed 37m 41s (remain 49m 24s) Loss: 1.5177 Grad: 6.5932 LR: 0.000959 \n",
            "Epoch: [1][3600/8091] Elapsed 38m 45s (remain 48m 19s) Loss: 1.5000 Grad: 15.9041 LR: 0.000955 \n",
            "Epoch: [1][3700/8091] Elapsed 39m 49s (remain 47m 14s) Loss: 1.4842 Grad: 15.6765 LR: 0.000950 \n",
            "Epoch: [1][3800/8091] Elapsed 40m 54s (remain 46m 10s) Loss: 1.4684 Grad: 18.5397 LR: 0.000945 \n",
            "Epoch: [1][3900/8091] Elapsed 41m 59s (remain 45m 5s) Loss: 1.4531 Grad: 8.6902 LR: 0.000941 \n",
            "Epoch: [1][4000/8091] Elapsed 43m 3s (remain 44m 1s) Loss: 1.4398 Grad: 12.7144 LR: 0.000935 \n",
            "Epoch: [1][4100/8091] Elapsed 44m 8s (remain 42m 56s) Loss: 1.4284 Grad: 16.1307 LR: 0.000930 \n",
            "Epoch: [1][4200/8091] Elapsed 45m 12s (remain 41m 51s) Loss: 1.4160 Grad: 0.8049 LR: 0.000924 \n",
            "Epoch: [1][4300/8091] Elapsed 46m 17s (remain 40m 47s) Loss: 1.4030 Grad: 17.1636 LR: 0.000919 \n",
            "Epoch: [1][4400/8091] Elapsed 47m 21s (remain 39m 42s) Loss: 1.3906 Grad: 18.9710 LR: 0.000913 \n",
            "Epoch: [1][4500/8091] Elapsed 48m 26s (remain 38m 38s) Loss: 1.3767 Grad: 15.1834 LR: 0.000906 \n",
            "Epoch: [1][4600/8091] Elapsed 49m 30s (remain 37m 33s) Loss: 1.3661 Grad: 8.5832 LR: 0.000900 \n",
            "Epoch: [1][4700/8091] Elapsed 50m 35s (remain 36m 28s) Loss: 1.3566 Grad: 10.8589 LR: 0.000893 \n",
            "Epoch: [1][4800/8091] Elapsed 51m 39s (remain 35m 24s) Loss: 1.3414 Grad: 14.3714 LR: 0.000887 \n",
            "Epoch: [1][4900/8091] Elapsed 52m 44s (remain 34m 19s) Loss: 1.3332 Grad: 0.3530 LR: 0.000880 \n",
            "Epoch: [1][5000/8091] Elapsed 53m 48s (remain 33m 15s) Loss: 1.3224 Grad: 7.0670 LR: 0.000873 \n",
            "Epoch: [1][5100/8091] Elapsed 54m 53s (remain 32m 10s) Loss: 1.3150 Grad: 12.6083 LR: 0.000865 \n",
            "Epoch: [1][5200/8091] Elapsed 55m 58s (remain 31m 5s) Loss: 1.3055 Grad: 16.6235 LR: 0.000858 \n",
            "Epoch: [1][5300/8091] Elapsed 57m 2s (remain 30m 1s) Loss: 1.2999 Grad: 5.2576 LR: 0.000850 \n",
            "Epoch: [1][5400/8091] Elapsed 58m 7s (remain 28m 56s) Loss: 1.2912 Grad: 15.1159 LR: 0.000843 \n",
            "Epoch: [1][5500/8091] Elapsed 59m 11s (remain 27m 52s) Loss: 1.2837 Grad: 89.9808 LR: 0.000835 \n",
            "Epoch: [1][5600/8091] Elapsed 60m 16s (remain 26m 47s) Loss: 1.2745 Grad: 3.0034 LR: 0.000826 \n",
            "Epoch: [1][5700/8091] Elapsed 61m 20s (remain 25m 43s) Loss: 1.2685 Grad: 10.5294 LR: 0.000818 \n",
            "Epoch: [1][5800/8091] Elapsed 62m 25s (remain 24m 38s) Loss: 1.2615 Grad: 13.1473 LR: 0.000810 \n",
            "Epoch: [1][5900/8091] Elapsed 63m 30s (remain 23m 34s) Loss: 1.2520 Grad: 19.3918 LR: 0.000801 \n",
            "Epoch: [1][6000/8091] Elapsed 64m 34s (remain 22m 29s) Loss: 1.2433 Grad: 7.3668 LR: 0.000792 \n",
            "Epoch: [1][6100/8091] Elapsed 65m 39s (remain 21m 24s) Loss: 1.2364 Grad: 10.1148 LR: 0.000784 \n",
            "Epoch: [1][6200/8091] Elapsed 66m 43s (remain 20m 20s) Loss: 1.2341 Grad: 13.4854 LR: 0.000775 \n",
            "Epoch: [1][6300/8091] Elapsed 67m 48s (remain 19m 15s) Loss: 1.2284 Grad: 6.9110 LR: 0.000766 \n",
            "Epoch: [1][6400/8091] Elapsed 68m 52s (remain 18m 11s) Loss: 1.2202 Grad: 10.0012 LR: 0.000757 \n",
            "Epoch: [1][6500/8091] Elapsed 69m 57s (remain 17m 6s) Loss: 1.2148 Grad: 11.4846 LR: 0.000747 \n",
            "Epoch: [1][6600/8091] Elapsed 71m 1s (remain 16m 2s) Loss: 1.2084 Grad: 7.0129 LR: 0.000737 \n",
            "Epoch: [1][6700/8091] Elapsed 72m 6s (remain 14m 57s) Loss: 1.2015 Grad: 2.3126 LR: 0.000728 \n",
            "Epoch: [1][6800/8091] Elapsed 73m 10s (remain 13m 52s) Loss: 1.1970 Grad: 16.5921 LR: 0.000719 \n",
            "Epoch: [1][6900/8091] Elapsed 74m 15s (remain 12m 48s) Loss: 1.1914 Grad: 15.7037 LR: 0.000709 \n",
            "Epoch: [1][7000/8091] Elapsed 75m 20s (remain 11m 43s) Loss: 1.1858 Grad: 0.0362 LR: 0.000699 \n",
            "Epoch: [1][7100/8091] Elapsed 76m 24s (remain 10m 39s) Loss: 1.1787 Grad: 9.1855 LR: 0.000689 \n",
            "Epoch: [1][7200/8091] Elapsed 77m 29s (remain 9m 34s) Loss: 1.1741 Grad: 8.1479 LR: 0.000679 \n",
            "Epoch: [1][7300/8091] Elapsed 78m 33s (remain 8m 30s) Loss: 1.1699 Grad: 13.9329 LR: 0.000669 \n",
            "Epoch: [1][7400/8091] Elapsed 79m 38s (remain 7m 25s) Loss: 1.1659 Grad: 5.1449 LR: 0.000659 \n",
            "Epoch: [1][7500/8091] Elapsed 80m 42s (remain 6m 20s) Loss: 1.1598 Grad: 5.6165 LR: 0.000648 \n",
            "Epoch: [1][7600/8091] Elapsed 81m 47s (remain 5m 16s) Loss: 1.1553 Grad: 9.1055 LR: 0.000638 \n",
            "Epoch: [1][7700/8091] Elapsed 82m 51s (remain 4m 11s) Loss: 1.1501 Grad: 3.3971 LR: 0.000627 \n",
            "Epoch: [1][7800/8091] Elapsed 83m 56s (remain 3m 7s) Loss: 1.1449 Grad: 8.8791 LR: 0.000617 \n",
            "Epoch: [1][7900/8091] Elapsed 85m 0s (remain 2m 2s) Loss: 1.1395 Grad: 5.5538 LR: 0.000607 \n",
            "Epoch: [1][8000/8091] Elapsed 86m 5s (remain 0m 58s) Loss: 1.1328 Grad: 24.1009 LR: 0.000596 \n",
            "Epoch: [1][8090/8091] Elapsed 87m 2s (remain 0m 0s) Loss: 1.1280 Grad: 5.3312 LR: 0.000587 \n",
            "EVAL: [0/1024] Elapsed 0m 0s (remain 14m 11s) Loss: 0.3827 \n",
            "EVAL: [100/1024] Elapsed 0m 21s (remain 3m 17s) Loss: 0.3086 \n",
            "EVAL: [200/1024] Elapsed 0m 42s (remain 2m 53s) Loss: 0.3277 \n",
            "EVAL: [300/1024] Elapsed 1m 3s (remain 2m 31s) Loss: 0.2964 \n",
            "EVAL: [400/1024] Elapsed 1m 23s (remain 2m 10s) Loss: 0.2978 \n",
            "EVAL: [500/1024] Elapsed 1m 44s (remain 1m 49s) Loss: 0.2894 \n",
            "EVAL: [600/1024] Elapsed 2m 5s (remain 1m 28s) Loss: 0.2737 \n",
            "EVAL: [700/1024] Elapsed 2m 26s (remain 1m 7s) Loss: 0.2648 \n",
            "EVAL: [800/1024] Elapsed 2m 46s (remain 0m 46s) Loss: 0.2637 \n",
            "EVAL: [900/1024] Elapsed 3m 7s (remain 0m 25s) Loss: 0.2490 \n",
            "EVAL: [1000/1024] Elapsed 3m 28s (remain 0m 4s) Loss: 0.2469 \n",
            "EVAL: [1023/1024] Elapsed 3m 33s (remain 0m 0s) Loss: 0.2453 \n",
            "Post-processing 223 example predictions split into 3071 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6432263551097632, Train Loss: 1.1280, Val Loss: 0.2453, Time: 5436s\n",
            "Epoch 1 - Save Best Model. score: 0.6432, loss: 0.2453\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/8091] Elapsed 0m 1s (remain 171m 34s) Loss: 0.2778 Grad: 1.5880 LR: 0.000587 \n",
            "Epoch: [2][100/8091] Elapsed 1m 5s (remain 86m 46s) Loss: 0.3803 Grad: 5.5531 LR: 0.000576 \n",
            "Epoch: [2][200/8091] Elapsed 2m 10s (remain 85m 15s) Loss: 0.3800 Grad: 6.4778 LR: 0.000566 \n",
            "Epoch: [2][300/8091] Elapsed 3m 14s (remain 84m 5s) Loss: 0.4228 Grad: 14.3531 LR: 0.000555 \n",
            "Epoch: [2][400/8091] Elapsed 4m 19s (remain 82m 56s) Loss: 0.4348 Grad: 4.4156 LR: 0.000544 \n",
            "Epoch: [2][500/8091] Elapsed 5m 24s (remain 81m 49s) Loss: 0.4495 Grad: 8.7375 LR: 0.000534 \n",
            "Epoch: [2][600/8091] Elapsed 6m 28s (remain 80m 44s) Loss: 0.4320 Grad: 5.0969 LR: 0.000523 \n",
            "Epoch: [2][700/8091] Elapsed 7m 33s (remain 79m 38s) Loss: 0.4240 Grad: 0.1351 LR: 0.000512 \n",
            "Epoch: [2][800/8091] Elapsed 8m 37s (remain 78m 32s) Loss: 0.4240 Grad: 2.8327 LR: 0.000501 \n",
            "Epoch: [2][900/8091] Elapsed 9m 42s (remain 77m 26s) Loss: 0.4197 Grad: 4.9337 LR: 0.000491 \n",
            "Epoch: [2][1000/8091] Elapsed 10m 46s (remain 76m 21s) Loss: 0.4280 Grad: 8.0695 LR: 0.000479 \n",
            "Epoch: [2][1100/8091] Elapsed 11m 51s (remain 75m 16s) Loss: 0.4303 Grad: 2.3013 LR: 0.000469 \n",
            "Epoch: [2][1200/8091] Elapsed 12m 55s (remain 74m 11s) Loss: 0.4216 Grad: 7.7585 LR: 0.000458 \n",
            "Epoch: [2][1300/8091] Elapsed 14m 0s (remain 73m 5s) Loss: 0.4237 Grad: 25.2230 LR: 0.000448 \n",
            "Epoch: [2][1400/8091] Elapsed 15m 4s (remain 72m 1s) Loss: 0.4260 Grad: 0.3385 LR: 0.000436 \n",
            "Epoch: [2][1500/8091] Elapsed 16m 9s (remain 70m 56s) Loss: 0.4290 Grad: 2.8421 LR: 0.000426 \n",
            "Epoch: [2][1600/8091] Elapsed 17m 14s (remain 69m 51s) Loss: 0.4281 Grad: 4.7459 LR: 0.000415 \n",
            "Epoch: [2][1700/8091] Elapsed 18m 18s (remain 68m 46s) Loss: 0.4256 Grad: 4.9961 LR: 0.000404 \n",
            "Epoch: [2][1800/8091] Elapsed 19m 23s (remain 67m 42s) Loss: 0.4223 Grad: 2.0420 LR: 0.000394 \n",
            "Epoch: [2][1900/8091] Elapsed 20m 27s (remain 66m 36s) Loss: 0.4172 Grad: 11.9258 LR: 0.000384 \n",
            "Epoch: [2][2000/8091] Elapsed 21m 31s (remain 65m 32s) Loss: 0.4119 Grad: 17.5649 LR: 0.000373 \n",
            "Epoch: [2][2100/8091] Elapsed 22m 36s (remain 64m 27s) Loss: 0.4167 Grad: 0.6072 LR: 0.000362 \n",
            "Epoch: [2][2200/8091] Elapsed 23m 40s (remain 63m 22s) Loss: 0.4152 Grad: 10.0620 LR: 0.000352 \n",
            "Epoch: [2][2300/8091] Elapsed 24m 45s (remain 62m 17s) Loss: 0.4128 Grad: 20.9138 LR: 0.000342 \n",
            "Epoch: [2][2400/8091] Elapsed 25m 50s (remain 61m 13s) Loss: 0.4147 Grad: 12.4196 LR: 0.000331 \n",
            "Epoch: [2][2500/8091] Elapsed 26m 54s (remain 60m 8s) Loss: 0.4147 Grad: 3.4587 LR: 0.000322 \n",
            "Epoch: [2][2600/8091] Elapsed 27m 58s (remain 59m 3s) Loss: 0.4107 Grad: 3.9047 LR: 0.000312 \n",
            "Epoch: [2][2700/8091] Elapsed 29m 3s (remain 57m 59s) Loss: 0.4068 Grad: 17.2791 LR: 0.000302 \n",
            "Epoch: [2][2800/8091] Elapsed 30m 8s (remain 56m 54s) Loss: 0.4074 Grad: 0.4205 LR: 0.000292 \n",
            "Epoch: [2][2900/8091] Elapsed 31m 12s (remain 55m 50s) Loss: 0.4093 Grad: 2.6772 LR: 0.000282 \n",
            "Epoch: [2][3000/8091] Elapsed 32m 16s (remain 54m 45s) Loss: 0.4113 Grad: 3.1818 LR: 0.000273 \n",
            "Epoch: [2][3100/8091] Elapsed 33m 21s (remain 53m 40s) Loss: 0.4163 Grad: 12.5019 LR: 0.000263 \n",
            "Epoch: [2][3200/8091] Elapsed 34m 26s (remain 52m 36s) Loss: 0.4170 Grad: 2.9798 LR: 0.000253 \n",
            "Epoch: [2][3300/8091] Elapsed 35m 30s (remain 51m 31s) Loss: 0.4160 Grad: 16.1639 LR: 0.000244 \n",
            "Epoch: [2][3400/8091] Elapsed 36m 35s (remain 50m 27s) Loss: 0.4158 Grad: 19.5940 LR: 0.000235 \n",
            "Epoch: [2][3500/8091] Elapsed 37m 39s (remain 49m 22s) Loss: 0.4162 Grad: 0.3452 LR: 0.000226 \n",
            "Epoch: [2][3600/8091] Elapsed 38m 44s (remain 48m 18s) Loss: 0.4125 Grad: 5.0231 LR: 0.000217 \n",
            "Epoch: [2][3700/8091] Elapsed 39m 48s (remain 47m 13s) Loss: 0.4134 Grad: 6.9566 LR: 0.000208 \n",
            "Epoch: [2][3800/8091] Elapsed 40m 53s (remain 46m 9s) Loss: 0.4133 Grad: 14.3175 LR: 0.000199 \n",
            "Epoch: [2][3900/8091] Elapsed 41m 57s (remain 45m 4s) Loss: 0.4092 Grad: 14.8094 LR: 0.000191 \n",
            "Epoch: [2][4000/8091] Elapsed 43m 2s (remain 43m 59s) Loss: 0.4105 Grad: 9.9527 LR: 0.000182 \n",
            "Epoch: [2][4100/8091] Elapsed 44m 6s (remain 42m 55s) Loss: 0.4099 Grad: 3.2280 LR: 0.000174 \n",
            "Epoch: [2][4200/8091] Elapsed 45m 11s (remain 41m 50s) Loss: 0.4083 Grad: 4.5242 LR: 0.000166 \n",
            "Epoch: [2][4300/8091] Elapsed 46m 15s (remain 40m 46s) Loss: 0.4094 Grad: 0.9052 LR: 0.000158 \n",
            "Epoch: [2][4400/8091] Elapsed 47m 20s (remain 39m 41s) Loss: 0.4077 Grad: 4.1646 LR: 0.000150 \n",
            "Epoch: [2][4500/8091] Elapsed 48m 24s (remain 38m 36s) Loss: 0.4045 Grad: 18.1641 LR: 0.000142 \n",
            "Epoch: [2][4600/8091] Elapsed 49m 29s (remain 37m 32s) Loss: 0.4051 Grad: 5.1257 LR: 0.000135 \n",
            "Epoch: [2][4700/8091] Elapsed 50m 33s (remain 36m 27s) Loss: 0.4039 Grad: 4.0571 LR: 0.000128 \n",
            "Epoch: [2][4800/8091] Elapsed 51m 38s (remain 35m 23s) Loss: 0.4028 Grad: 17.7448 LR: 0.000121 \n",
            "Epoch: [2][4900/8091] Elapsed 52m 42s (remain 34m 18s) Loss: 0.4043 Grad: 7.6388 LR: 0.000114 \n",
            "Epoch: [2][5000/8091] Elapsed 53m 47s (remain 33m 14s) Loss: 0.4038 Grad: 10.8376 LR: 0.000107 \n",
            "Epoch: [2][5100/8091] Elapsed 54m 51s (remain 32m 9s) Loss: 0.4042 Grad: 2.4098 LR: 0.000101 \n",
            "Epoch: [2][5200/8091] Elapsed 55m 56s (remain 31m 5s) Loss: 0.4027 Grad: 5.2068 LR: 0.000094 \n",
            "Epoch: [2][5300/8091] Elapsed 57m 1s (remain 30m 0s) Loss: 0.4019 Grad: 11.9791 LR: 0.000088 \n",
            "Epoch: [2][5400/8091] Elapsed 58m 5s (remain 28m 56s) Loss: 0.4006 Grad: 3.1204 LR: 0.000082 \n",
            "Epoch: [2][5500/8091] Elapsed 59m 10s (remain 27m 51s) Loss: 0.3994 Grad: 8.8252 LR: 0.000076 \n",
            "Epoch: [2][5600/8091] Elapsed 60m 14s (remain 26m 46s) Loss: 0.3969 Grad: 4.9761 LR: 0.000070 \n",
            "Epoch: [2][5700/8091] Elapsed 61m 19s (remain 25m 42s) Loss: 0.3965 Grad: 5.3403 LR: 0.000065 \n",
            "Epoch: [2][5800/8091] Elapsed 62m 23s (remain 24m 37s) Loss: 0.3946 Grad: 10.6438 LR: 0.000060 \n",
            "Epoch: [2][5900/8091] Elapsed 63m 28s (remain 23m 33s) Loss: 0.3945 Grad: 14.3145 LR: 0.000055 \n",
            "Epoch: [2][6000/8091] Elapsed 64m 32s (remain 22m 28s) Loss: 0.3941 Grad: 0.4117 LR: 0.000050 \n",
            "Epoch: [2][6100/8091] Elapsed 65m 37s (remain 21m 24s) Loss: 0.3938 Grad: 4.7210 LR: 0.000045 \n",
            "Epoch: [2][6200/8091] Elapsed 66m 41s (remain 20m 19s) Loss: 0.3948 Grad: 31.8952 LR: 0.000041 \n",
            "Epoch: [2][6300/8091] Elapsed 67m 46s (remain 19m 15s) Loss: 0.3940 Grad: 0.0760 LR: 0.000037 \n",
            "Epoch: [2][6400/8091] Elapsed 68m 51s (remain 18m 10s) Loss: 0.3922 Grad: 2.2733 LR: 0.000033 \n",
            "Epoch: [2][6500/8091] Elapsed 69m 55s (remain 17m 6s) Loss: 0.3920 Grad: 20.3849 LR: 0.000029 \n",
            "Epoch: [2][6600/8091] Elapsed 71m 0s (remain 16m 1s) Loss: 0.3930 Grad: 6.3272 LR: 0.000025 \n",
            "Epoch: [2][6700/8091] Elapsed 72m 4s (remain 14m 57s) Loss: 0.3942 Grad: 5.2738 LR: 0.000022 \n",
            "Epoch: [2][6800/8091] Elapsed 73m 8s (remain 13m 52s) Loss: 0.3954 Grad: 2.3643 LR: 0.000019 \n",
            "Epoch: [2][6900/8091] Elapsed 74m 13s (remain 12m 47s) Loss: 0.3954 Grad: 14.5381 LR: 0.000016 \n",
            "Epoch: [2][7000/8091] Elapsed 75m 18s (remain 11m 43s) Loss: 0.3966 Grad: 5.2013 LR: 0.000014 \n",
            "Epoch: [2][7100/8091] Elapsed 76m 22s (remain 10m 38s) Loss: 0.3948 Grad: 2.7220 LR: 0.000011 \n",
            "Epoch: [2][7200/8091] Elapsed 77m 26s (remain 9m 34s) Loss: 0.3946 Grad: 3.8983 LR: 0.000009 \n",
            "Epoch: [2][7300/8091] Elapsed 78m 31s (remain 8m 29s) Loss: 0.3956 Grad: 16.5273 LR: 0.000007 \n",
            "Epoch: [2][7400/8091] Elapsed 79m 35s (remain 7m 25s) Loss: 0.3949 Grad: 17.0026 LR: 0.000005 \n",
            "Epoch: [2][7500/8091] Elapsed 80m 40s (remain 6m 20s) Loss: 0.3947 Grad: 2.7693 LR: 0.000004 \n",
            "Epoch: [2][7600/8091] Elapsed 81m 44s (remain 5m 16s) Loss: 0.3936 Grad: 3.4386 LR: 0.000003 \n",
            "Epoch: [2][7700/8091] Elapsed 82m 49s (remain 4m 11s) Loss: 0.3940 Grad: 13.2868 LR: 0.000002 \n",
            "Epoch: [2][7800/8091] Elapsed 83m 53s (remain 3m 7s) Loss: 0.3933 Grad: 7.3299 LR: 0.000001 \n",
            "Epoch: [2][7900/8091] Elapsed 84m 58s (remain 2m 2s) Loss: 0.3930 Grad: 8.2684 LR: 0.000000 \n",
            "Epoch: [2][8000/8091] Elapsed 86m 3s (remain 0m 58s) Loss: 0.3938 Grad: 13.4109 LR: 0.000000 \n",
            "Epoch: [2][8090/8091] Elapsed 87m 1s (remain 0m 0s) Loss: 0.3935 Grad: 13.9180 LR: 0.000000 \n",
            "EVAL: [0/1024] Elapsed 0m 0s (remain 14m 10s) Loss: 0.4820 \n",
            "EVAL: [100/1024] Elapsed 0m 21s (remain 3m 17s) Loss: 0.3092 \n",
            "EVAL: [200/1024] Elapsed 0m 42s (remain 2m 53s) Loss: 0.3343 \n",
            "EVAL: [300/1024] Elapsed 1m 3s (remain 2m 31s) Loss: 0.2901 \n",
            "EVAL: [400/1024] Elapsed 1m 23s (remain 2m 10s) Loss: 0.2998 \n",
            "EVAL: [500/1024] Elapsed 1m 44s (remain 1m 49s) Loss: 0.2876 \n",
            "EVAL: [600/1024] Elapsed 2m 5s (remain 1m 28s) Loss: 0.2720 \n",
            "EVAL: [700/1024] Elapsed 2m 26s (remain 1m 7s) Loss: 0.2562 \n",
            "EVAL: [800/1024] Elapsed 2m 46s (remain 0m 46s) Loss: 0.2580 \n",
            "EVAL: [900/1024] Elapsed 3m 7s (remain 0m 25s) Loss: 0.2425 \n",
            "EVAL: [1000/1024] Elapsed 3m 28s (remain 0m 4s) Loss: 0.2413 \n",
            "EVAL: [1023/1024] Elapsed 3m 32s (remain 0m 0s) Loss: 0.2393 \n",
            "Post-processing 223 example predictions split into 3071 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.6676230595513107, Train Loss: 0.3935, Val Loss: 0.2393, Time: 5436s\n",
            "Epoch 2 - Save Best Model. score: 0.6676, loss: 0.2393\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.64819\n",
            "========== fold: 3 training ==========\n",
            "Some weights of the model checkpoint at google/rembert were not used when initializing RemBertModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RemBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RemBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/8058] Elapsed 0m 1s (remain 169m 26s) Loss: 5.4473 Grad: 4.7336 LR: 0.000000 \n",
            "Epoch: [1][100/8058] Elapsed 1m 5s (remain 86m 22s) Loss: 5.4488 Grad: 7.6034 LR: 0.000061 \n",
            "Epoch: [1][200/8058] Elapsed 2m 10s (remain 84m 53s) Loss: 4.4484 Grad: 8.4126 LR: 0.000122 \n",
            "Epoch: [1][300/8058] Elapsed 3m 14s (remain 83m 41s) Loss: 3.9839 Grad: 10.8030 LR: 0.000187 \n",
            "Epoch: [1][400/8058] Elapsed 4m 19s (remain 82m 31s) Loss: 3.7103 Grad: 15.8321 LR: 0.000248 \n",
            "Epoch: [1][500/8058] Elapsed 5m 23s (remain 81m 25s) Loss: 3.4841 Grad: 12.9699 LR: 0.000309 \n",
            "Epoch: [1][600/8058] Elapsed 6m 28s (remain 80m 19s) Loss: 3.2457 Grad: 28.1451 LR: 0.000370 \n",
            "Epoch: [1][700/8058] Elapsed 7m 32s (remain 79m 13s) Loss: 3.0386 Grad: 7.3548 LR: 0.000435 \n",
            "Epoch: [1][800/8058] Elapsed 8m 37s (remain 78m 8s) Loss: 2.8547 Grad: 9.2106 LR: 0.000496 \n",
            "Epoch: [1][900/8058] Elapsed 9m 41s (remain 77m 2s) Loss: 2.6845 Grad: 20.0878 LR: 0.000557 \n",
            "Epoch: [1][1000/8058] Elapsed 10m 46s (remain 75m 57s) Loss: 2.5254 Grad: 43.7072 LR: 0.000622 \n",
            "Epoch: [1][1100/8058] Elapsed 11m 50s (remain 74m 51s) Loss: 2.3856 Grad: 7.1599 LR: 0.000683 \n",
            "Epoch: [1][1200/8058] Elapsed 12m 55s (remain 73m 47s) Loss: 2.2894 Grad: 14.8574 LR: 0.000743 \n",
            "Epoch: [1][1300/8058] Elapsed 13m 59s (remain 72m 42s) Loss: 2.2047 Grad: 18.4719 LR: 0.000804 \n",
            "Epoch: [1][1400/8058] Elapsed 15m 4s (remain 71m 38s) Loss: 2.1240 Grad: 5.0041 LR: 0.000870 \n",
            "Epoch: [1][1500/8058] Elapsed 16m 9s (remain 70m 33s) Loss: 2.0493 Grad: 16.9295 LR: 0.000930 \n",
            "Epoch: [1][1600/8058] Elapsed 17m 13s (remain 69m 27s) Loss: 1.9827 Grad: 24.9058 LR: 0.000991 \n",
            "Epoch: [1][1700/8058] Elapsed 18m 18s (remain 68m 23s) Loss: 1.9362 Grad: 15.4272 LR: 0.001000 \n",
            "Epoch: [1][1800/8058] Elapsed 19m 22s (remain 67m 19s) Loss: 1.8861 Grad: 6.2775 LR: 0.001000 \n",
            "Epoch: [1][1900/8058] Elapsed 20m 27s (remain 66m 14s) Loss: 1.8471 Grad: 6.9707 LR: 0.000999 \n",
            "Epoch: [1][2000/8058] Elapsed 21m 31s (remain 65m 9s) Loss: 1.8069 Grad: 13.8468 LR: 0.000998 \n",
            "Epoch: [1][2100/8058] Elapsed 22m 36s (remain 64m 5s) Loss: 1.7643 Grad: 0.0435 LR: 0.000997 \n",
            "Epoch: [1][2200/8058] Elapsed 23m 40s (remain 63m 0s) Loss: 1.7359 Grad: 10.6056 LR: 0.000996 \n",
            "Epoch: [1][2300/8058] Elapsed 24m 45s (remain 61m 55s) Loss: 1.7019 Grad: 12.4559 LR: 0.000994 \n",
            "Epoch: [1][2400/8058] Elapsed 25m 49s (remain 60m 51s) Loss: 1.6654 Grad: 11.1466 LR: 0.000993 \n",
            "Epoch: [1][2500/8058] Elapsed 26m 54s (remain 59m 46s) Loss: 1.6371 Grad: 1.3207 LR: 0.000991 \n",
            "Epoch: [1][2600/8058] Elapsed 27m 58s (remain 58m 42s) Loss: 1.6176 Grad: 11.4349 LR: 0.000989 \n",
            "Epoch: [1][2700/8058] Elapsed 29m 3s (remain 57m 37s) Loss: 1.5918 Grad: 52.1984 LR: 0.000986 \n",
            "Epoch: [1][2800/8058] Elapsed 30m 7s (remain 56m 32s) Loss: 1.5713 Grad: 0.2256 LR: 0.000983 \n",
            "Epoch: [1][2900/8058] Elapsed 31m 12s (remain 55m 28s) Loss: 1.5552 Grad: 12.5427 LR: 0.000981 \n",
            "Epoch: [1][3000/8058] Elapsed 32m 16s (remain 54m 23s) Loss: 1.5278 Grad: 18.7717 LR: 0.000978 \n",
            "Epoch: [1][3100/8058] Elapsed 33m 21s (remain 53m 19s) Loss: 1.5048 Grad: 16.2177 LR: 0.000974 \n",
            "Epoch: [1][3200/8058] Elapsed 34m 25s (remain 52m 14s) Loss: 1.4845 Grad: 13.8937 LR: 0.000971 \n",
            "Epoch: [1][3300/8058] Elapsed 35m 30s (remain 51m 9s) Loss: 1.4654 Grad: 7.8158 LR: 0.000967 \n",
            "Epoch: [1][3400/8058] Elapsed 36m 34s (remain 50m 5s) Loss: 1.4540 Grad: 12.3608 LR: 0.000963 \n",
            "Epoch: [1][3500/8058] Elapsed 37m 39s (remain 49m 1s) Loss: 1.4358 Grad: 11.8599 LR: 0.000959 \n",
            "Epoch: [1][3600/8058] Elapsed 38m 44s (remain 47m 56s) Loss: 1.4207 Grad: 8.9391 LR: 0.000954 \n",
            "Epoch: [1][3700/8058] Elapsed 39m 48s (remain 46m 51s) Loss: 1.4107 Grad: 11.5869 LR: 0.000950 \n",
            "Epoch: [1][3800/8058] Elapsed 40m 53s (remain 45m 47s) Loss: 1.3995 Grad: 20.8846 LR: 0.000945 \n",
            "Epoch: [1][3900/8058] Elapsed 41m 57s (remain 44m 42s) Loss: 1.3918 Grad: 19.0089 LR: 0.000940 \n",
            "Epoch: [1][4000/8058] Elapsed 43m 2s (remain 43m 38s) Loss: 1.3817 Grad: 10.8345 LR: 0.000935 \n",
            "Epoch: [1][4100/8058] Elapsed 44m 6s (remain 42m 33s) Loss: 1.3661 Grad: 17.2845 LR: 0.000929 \n",
            "Epoch: [1][4200/8058] Elapsed 45m 11s (remain 41m 29s) Loss: 1.3546 Grad: 5.7750 LR: 0.000923 \n",
            "Epoch: [1][4300/8058] Elapsed 46m 16s (remain 40m 24s) Loss: 1.3462 Grad: 11.8929 LR: 0.000918 \n",
            "Epoch: [1][4400/8058] Elapsed 47m 20s (remain 39m 20s) Loss: 1.3323 Grad: 7.9886 LR: 0.000912 \n",
            "Epoch: [1][4500/8058] Elapsed 48m 25s (remain 38m 15s) Loss: 1.3210 Grad: 9.3521 LR: 0.000905 \n",
            "Epoch: [1][4600/8058] Elapsed 49m 29s (remain 37m 11s) Loss: 1.3131 Grad: 9.7299 LR: 0.000899 \n",
            "Epoch: [1][4700/8058] Elapsed 50m 34s (remain 36m 6s) Loss: 1.2976 Grad: 12.0924 LR: 0.000892 \n",
            "Epoch: [1][4800/8058] Elapsed 51m 38s (remain 35m 2s) Loss: 1.2880 Grad: 15.1440 LR: 0.000886 \n",
            "Epoch: [1][4900/8058] Elapsed 52m 43s (remain 33m 57s) Loss: 1.2811 Grad: 5.3632 LR: 0.000878 \n",
            "Epoch: [1][5000/8058] Elapsed 53m 48s (remain 32m 53s) Loss: 1.2721 Grad: 7.8849 LR: 0.000871 \n",
            "Epoch: [1][5100/8058] Elapsed 54m 52s (remain 31m 48s) Loss: 1.2591 Grad: 7.2490 LR: 0.000864 \n",
            "Epoch: [1][5200/8058] Elapsed 55m 57s (remain 30m 44s) Loss: 1.2521 Grad: 17.4940 LR: 0.000856 \n",
            "Epoch: [1][5300/8058] Elapsed 57m 1s (remain 29m 39s) Loss: 1.2451 Grad: 15.5023 LR: 0.000849 \n",
            "Epoch: [1][5400/8058] Elapsed 58m 6s (remain 28m 35s) Loss: 1.2399 Grad: 9.2485 LR: 0.000841 \n",
            "Epoch: [1][5500/8058] Elapsed 59m 10s (remain 27m 30s) Loss: 1.2340 Grad: 18.2674 LR: 0.000833 \n",
            "Epoch: [1][5600/8058] Elapsed 60m 15s (remain 26m 25s) Loss: 1.2275 Grad: 1.8177 LR: 0.000825 \n",
            "Epoch: [1][5700/8058] Elapsed 61m 19s (remain 25m 21s) Loss: 1.2220 Grad: 3.7607 LR: 0.000816 \n",
            "Epoch: [1][5800/8058] Elapsed 62m 24s (remain 24m 16s) Loss: 1.2125 Grad: 18.3582 LR: 0.000808 \n",
            "Epoch: [1][5900/8058] Elapsed 63m 29s (remain 23m 12s) Loss: 1.2060 Grad: 14.3679 LR: 0.000799 \n",
            "Epoch: [1][6000/8058] Elapsed 64m 33s (remain 22m 7s) Loss: 1.1993 Grad: 4.7464 LR: 0.000791 \n",
            "Epoch: [1][6100/8058] Elapsed 65m 38s (remain 21m 3s) Loss: 1.1933 Grad: 12.2795 LR: 0.000782 \n",
            "Epoch: [1][6200/8058] Elapsed 66m 42s (remain 19m 58s) Loss: 1.1870 Grad: 11.3897 LR: 0.000773 \n",
            "Epoch: [1][6300/8058] Elapsed 67m 47s (remain 18m 54s) Loss: 1.1800 Grad: 2.9808 LR: 0.000763 \n",
            "Epoch: [1][6400/8058] Elapsed 68m 51s (remain 17m 49s) Loss: 1.1755 Grad: 9.3165 LR: 0.000754 \n",
            "Epoch: [1][6500/8058] Elapsed 69m 56s (remain 16m 45s) Loss: 1.1695 Grad: 14.2810 LR: 0.000745 \n",
            "Epoch: [1][6600/8058] Elapsed 71m 0s (remain 15m 40s) Loss: 1.1647 Grad: 12.5508 LR: 0.000735 \n",
            "Epoch: [1][6700/8058] Elapsed 72m 5s (remain 14m 35s) Loss: 1.1597 Grad: 3.2920 LR: 0.000726 \n",
            "Epoch: [1][6800/8058] Elapsed 73m 9s (remain 13m 31s) Loss: 1.1531 Grad: 12.6930 LR: 0.000716 \n",
            "Epoch: [1][6900/8058] Elapsed 74m 14s (remain 12m 26s) Loss: 1.1482 Grad: 10.2862 LR: 0.000707 \n",
            "Epoch: [1][7000/8058] Elapsed 75m 19s (remain 11m 22s) Loss: 1.1444 Grad: 2.2264 LR: 0.000696 \n",
            "Epoch: [1][7100/8058] Elapsed 76m 23s (remain 10m 17s) Loss: 1.1410 Grad: 3.3158 LR: 0.000686 \n",
            "Epoch: [1][7200/8058] Elapsed 77m 28s (remain 9m 13s) Loss: 1.1383 Grad: 8.8115 LR: 0.000677 \n",
            "Epoch: [1][7300/8058] Elapsed 78m 32s (remain 8m 8s) Loss: 1.1349 Grad: 11.3910 LR: 0.000666 \n",
            "Epoch: [1][7400/8058] Elapsed 79m 37s (remain 7m 4s) Loss: 1.1311 Grad: 2.8250 LR: 0.000656 \n",
            "Epoch: [1][7500/8058] Elapsed 80m 41s (remain 5m 59s) Loss: 1.1275 Grad: 16.3902 LR: 0.000646 \n",
            "Epoch: [1][7600/8058] Elapsed 81m 46s (remain 4m 54s) Loss: 1.1233 Grad: 8.8486 LR: 0.000636 \n",
            "Epoch: [1][7700/8058] Elapsed 82m 50s (remain 3m 50s) Loss: 1.1177 Grad: 4.0341 LR: 0.000625 \n",
            "Epoch: [1][7800/8058] Elapsed 83m 55s (remain 2m 45s) Loss: 1.1138 Grad: 5.0210 LR: 0.000614 \n",
            "Epoch: [1][7900/8058] Elapsed 84m 59s (remain 1m 41s) Loss: 1.1081 Grad: 35.4569 LR: 0.000604 \n",
            "Epoch: [1][8000/8058] Elapsed 86m 4s (remain 0m 36s) Loss: 1.1036 Grad: 14.3130 LR: 0.000593 \n",
            "Epoch: [1][8057/8058] Elapsed 86m 39s (remain 0m 0s) Loss: 1.1026 Grad: 8.1268 LR: 0.000587 \n",
            "EVAL: [0/1058] Elapsed 0m 0s (remain 14m 37s) Loss: 0.0044 \n",
            "EVAL: [100/1058] Elapsed 0m 21s (remain 3m 24s) Loss: 0.2719 \n",
            "EVAL: [200/1058] Elapsed 0m 42s (remain 3m 0s) Loss: 0.2527 \n",
            "EVAL: [300/1058] Elapsed 1m 3s (remain 2m 38s) Loss: 0.2413 \n",
            "EVAL: [400/1058] Elapsed 1m 23s (remain 2m 17s) Loss: 0.2306 \n",
            "EVAL: [500/1058] Elapsed 1m 44s (remain 1m 56s) Loss: 0.2090 \n",
            "EVAL: [600/1058] Elapsed 2m 5s (remain 1m 35s) Loss: 0.2051 \n",
            "EVAL: [700/1058] Elapsed 2m 25s (remain 1m 14s) Loss: 0.1899 \n",
            "EVAL: [800/1058] Elapsed 2m 46s (remain 0m 53s) Loss: 0.1980 \n",
            "EVAL: [900/1058] Elapsed 3m 7s (remain 0m 32s) Loss: 0.1993 \n",
            "EVAL: [1000/1058] Elapsed 3m 28s (remain 0m 11s) Loss: 0.2027 \n",
            "EVAL: [1057/1058] Elapsed 3m 39s (remain 0m 0s) Loss: 0.1988 \n",
            "Post-processing 223 example predictions split into 3172 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Score: 0.6715406790518899, Train Loss: 1.1026, Val Loss: 0.1988, Time: 5423s\n",
            "Epoch 1 - Save Best Model. score: 0.6715, loss: 0.1988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2][0/8058] Elapsed 0m 1s (remain 171m 1s) Loss: 0.5170 Grad: 1.8005 LR: 0.000587 \n",
            "Epoch: [2][100/8058] Elapsed 1m 5s (remain 86m 20s) Loss: 0.4237 Grad: 9.6128 LR: 0.000576 \n",
            "Epoch: [2][200/8058] Elapsed 2m 10s (remain 84m 52s) Loss: 0.4527 Grad: 0.7074 LR: 0.000566 \n",
            "Epoch: [2][300/8058] Elapsed 3m 14s (remain 83m 39s) Loss: 0.4802 Grad: 8.2440 LR: 0.000554 \n",
            "Epoch: [2][400/8058] Elapsed 4m 19s (remain 82m 29s) Loss: 0.4758 Grad: 4.9831 LR: 0.000544 \n",
            "Epoch: [2][500/8058] Elapsed 5m 23s (remain 81m 23s) Loss: 0.4659 Grad: 3.6559 LR: 0.000533 \n",
            "Epoch: [2][600/8058] Elapsed 6m 28s (remain 80m 17s) Loss: 0.4630 Grad: 5.8474 LR: 0.000523 \n",
            "Epoch: [2][700/8058] Elapsed 7m 32s (remain 79m 12s) Loss: 0.4524 Grad: 1.5440 LR: 0.000511 \n",
            "Epoch: [2][800/8058] Elapsed 8m 37s (remain 78m 6s) Loss: 0.4436 Grad: 10.7668 LR: 0.000501 \n",
            "Epoch: [2][900/8058] Elapsed 9m 41s (remain 77m 1s) Loss: 0.4442 Grad: 8.2936 LR: 0.000490 \n",
            "Epoch: [2][1000/8058] Elapsed 10m 46s (remain 75m 56s) Loss: 0.4356 Grad: 20.2910 LR: 0.000479 \n",
            "Epoch: [2][1100/8058] Elapsed 11m 50s (remain 74m 51s) Loss: 0.4382 Grad: 9.4755 LR: 0.000468 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1pdUFYKYmOM"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fEjvHZUbgpH"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}