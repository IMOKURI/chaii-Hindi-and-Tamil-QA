{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chaii.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMPjEAO9dIzG8+Hcu7XSVpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IMOKURI/chaii-Hindi-and-Tamil-QA/blob/main/chaii.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8HABKC9jNl"
      },
      "source": [
        "# About this notebook ...\n",
        "\n",
        "[chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U4_rc_e9rOY"
      },
      "source": [
        "# Memo\n",
        "\n",
        "\n",
        "\n",
        "## ToDo\n",
        "\n",
        "- [ ] [ラベルノイズ補正](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264395)\n",
        "    - [ ] [これもかな](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266109)\n",
        "- [x] [Post process でスコアアップ](https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765)\n",
        "- モデル\n",
        "    - [ ] [monsoon-nlp/hindi-tpu-electra](https://huggingface.co/monsoon-nlp/hindi-tpu-electra) `AutoModelForQuestionAnswering` クラスが使えそう\n",
        "    - [ ] [RemBERT](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267827)\n",
        "    - [ ] [google/muril-base-cased](https://huggingface.co/google/muril-base-cased)\n",
        "    - その他 [multilingual & QA models](https://huggingface.co/models?filter=multilingual&pipeline_tag=question-answering)\n",
        "\n",
        "## Done\n",
        "\n",
        "\n",
        "## Works Well\n",
        "\n",
        "\n",
        "## Doesn't Work\n",
        "\n",
        "\n",
        "## Not To Do\n",
        "\n",
        "- [2つのモデルを作る](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267604) - [経緯](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264749)\n",
        "- [位置によるペナルティを課す Loss](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266832)\n",
        "\n",
        "\n",
        "## Additional Datasets\n",
        "\n",
        "Search from [here](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264581).\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) hindi のみ\n",
        "- [Squad_Translated_to_Tamil for Chaii](https://www.kaggle.com/msafi04/squad-translated-to-tamil-for-chaii) tamil のみ\n",
        "\n",
        "\n",
        "## Reference Notebooks\n",
        "\n",
        "- [ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBhwVS89vKZ"
      },
      "source": [
        "# Prepare for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEN6L1ol9d6d",
        "outputId": "f4286b65-0d6f-4184-a42c-02b2de40858d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Sep 14 02:41:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    38W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl14Cnli91Ol",
        "outputId": "87a2c90e-2ba0-4df9-cf01-a5e9bf7ac4b2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('init.txt'):\n",
        "    print(\"Already initialized.\")\n",
        "\n",
        "else:\n",
        "    if 'google.colab' in sys.modules:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        dataset_dir = \"/content/drive/MyDrive/Datasets\"\n",
        "\n",
        "        # ====================================================\n",
        "        # Competition datasets\n",
        "        # ====================================================\n",
        "        with zipfile.ZipFile(f\"{dataset_dir}/chaii-hindi-and-tamil-question-answering.zip\", \"r\") as zp:\n",
        "            zp.extractall(path=\"./\")\n",
        "        #with zipfile.ZipFile(f\"{dataset_dir}/chaii-external-data-mlqa-xquad-preprocessing.zip\", \"r\") as zp:\n",
        "        #    zp.extractall(path=\"./\")\n",
        "        #with zipfile.ZipFile(f\"{dataset_dir}/chaii-Squad_Translated_to_Tamil.zip\", \"r\") as zp:\n",
        "        #    zp.extractall(path=\"./\")\n",
        "\n",
        "    # for StratifiedGroupKFold\n",
        "    # !pip uninstall -y scikit-learn\n",
        "    # !pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n",
        "\n",
        "    # for MultilabelStratifiedKFold\n",
        "    # !pip install -q iterative-stratification\n",
        "\n",
        "    # for CosineAnnealingWarmupRestarts\n",
        "    # !pip install -qU 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
        "\n",
        "    !pip install -q wandb\n",
        "    # !pip install -q optuna\n",
        "\n",
        "    # ====================================================\n",
        "    # Competition specific libraries\n",
        "    # ====================================================\n",
        "    !pip install -q transformers\n",
        "    !pip install -q sentencepiece\n",
        "    # !pip install -q textstat\n",
        "    # !pip install -q nlpaug\n",
        "\n",
        "    !touch init.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXvMPm5_4h0"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB47hsio_5y6"
      },
      "source": [
        "# General libraries\n",
        "import collections\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import statistics\n",
        "import time\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.cuda.amp as amp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "# from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
        "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error, jaccard_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold  # , StratifiedGroupKFold\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyiVJefp4oOi"
      },
      "source": [
        "# Competition specific libraries\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import nlpaug.augmenter.sentence as nas\n",
        "# import nltk\n",
        "# import textstat\n",
        "import transformers as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-snxxwfCAO92"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxr17nzJAS2c"
      },
      "source": [
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxTu8mx-AXcw",
        "outputId": "0445950f-4ba3-4ed4-b2b4-f829c6696aad"
      },
      "source": [
        "netrc = \"/content/drive/MyDrive/.netrc\" if 'google.colab' in sys.modules else \"../input/wandbtoken/.netrc\"\n",
        "!cp -f {netrc} ~/\n",
        "!wandb login\n",
        "\n",
        "wandb_tags = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WavcpUepAQOs"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    wandb_tags.append(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_GYEnAnAqmJ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbzq6jTIAszZ"
      },
      "source": [
        "DATA_DIR = \"./\" if 'google.colab' in sys.modules else \"../input/chaii-hindi-and-tamil-question-answering/\"\n",
        "OUTPUT_DIR = \"./\"\n",
        "MODEL_DIR = \"./models/\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ey9Vh4CBMgo"
      },
      "source": [
        "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
        "sub = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n",
        "\n",
        "#external_squad_translated_tamil = pd.read_csv(DATA_DIR + \"squad_translated_tamil.csv\")\n",
        "#external_mlqa = pd.read_csv(DATA_DIR + \"mlqa_hindi.csv\")\n",
        "#external_xquad = pd.read_csv(DATA_DIR + \"xquad.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2Lqm0KBeb_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsNi7VmB9_M",
        "outputId": "12a323b6-5caf-4679-aa3c-5d582ceb75d8"
      },
      "source": [
        "# seed = random.randrange(10000)\n",
        "seed = 440\n",
        "print(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDIEcAvEBdqj"
      },
      "source": [
        "class Config:\n",
        "    wandb_entity = \"imokuri\"\n",
        "    wandb_project = \"chaii\"\n",
        "    print_freq = 100\n",
        "\n",
        "    preprocess = False\n",
        "    train = True\n",
        "    validate = False\n",
        "    inference = False\n",
        "\n",
        "    debug = False\n",
        "    num_debug_data = 50\n",
        "\n",
        "    amp = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI5SEjO0CS8k"
      },
      "source": [
        "config_defaults = {\n",
        "    \"seed\": seed,\n",
        "    # \"n_class\": 1,\n",
        "    \"n_fold\": 5,\n",
        "    \"epochs\": 2,\n",
        "    \"batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"criterion\": \"ChaiiCrossEntropyLoss\",\n",
        "    \"optimizer\": \"BertAdamW\",\n",
        "    \"scheduler\": \"get_cosine_schedule_with_warmup\",\n",
        "    \"max_lr\": 5e-5,\n",
        "    \"lr\": 2e-5,\n",
        "    \"min_lr\": 1e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"model_name\": \"deepset/xlm-roberta-large-squad2\",\n",
        "    # \"model_name\": \"deepset/xlm-roberta-base-squad2\",\n",
        "    \"max_len\": 384,\n",
        "    \"doc_stride\": 128,\n",
        "    \"dropout\": 0.0,\n",
        "    \"init_weights\": True,\n",
        "    \"init_layers\": 4,\n",
        "    # \"freeze_layers\": 0,\n",
        "    \"datasets\": [\n",
        "        \"mlqa:v1\",\n",
        "        \"xquad:v1\",\n",
        "        \"squad_translated_tamil:v1\",\n",
        "    ],\n",
        "    \"models\": [\n",
        "        \"base-models:v1\",\n",
        "    ]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Pk66rRDTGC"
      },
      "source": [
        "if Config.debug:\n",
        "    config_defaults[\"n_fold\"] = 3\n",
        "    config_defaults[\"epochs\"] = 1\n",
        "    Config.print_freq = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9WtvbYBtfN"
      },
      "source": [
        "if Config.train:\n",
        "    wandb_job_type = \"training\"\n",
        "\n",
        "elif Config.inference:\n",
        "    wandb_job_type = \"inference\"\n",
        "\n",
        "elif Config.validate:\n",
        "    wandb_job_type = \"validation\"\n",
        "\n",
        "elif Config.preprocess:\n",
        "    wandb_job_type = \"preprocess\"\n",
        "\n",
        "else:\n",
        "    wandb_job_type = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rVE3iK7Xaf"
      },
      "source": [
        "if Config.debug:\n",
        "    wandb_tags.append(\"debug\")\n",
        "    \n",
        "if Config.amp:\n",
        "    wandb_tags.append(\"amp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "9kskKEy9Dyqk",
        "outputId": "582a5709-fe60-4ffb-f215-b9a9c8b78aa2"
      },
      "source": [
        "if Config.debug:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        tags=wandb_tags,\n",
        "        mode=\"disabled\",\n",
        "    )\n",
        "else:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        job_type=wandb_job_type,\n",
        "        tags=wandb_tags,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">bumbling-bird-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/imokuri/chaii\" target=\"_blank\">https://wandb.ai/imokuri/chaii</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/imokuri/chaii/runs/2cj0dbgj\" target=\"_blank\">https://wandb.ai/imokuri/chaii/runs/2cj0dbgj</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210914_024111-2cj0dbgj</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev3bvwDvEMuS"
      },
      "source": [
        "config = wandb.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1pXRWoHy1_"
      },
      "source": [
        "# Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSYiJXBD4oy"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_data = []\n",
        "    for name_version in config.datasets:\n",
        "        name, version = name_version.split(\":\")\n",
        "        os.makedirs(name, exist_ok=True)\n",
        "\n",
        "        if Config.debug:\n",
        "            artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{name_version}\"\n",
        "            api = wandb.Api()\n",
        "            artifact = api.artifact(artifact_path)\n",
        "\n",
        "        else:\n",
        "            artifact_path = f\"{name_version}\"\n",
        "            artifact = run.use_artifact(artifact_path)\n",
        "\n",
        "        artifact.download(name)\n",
        "\n",
        "        df = pd.read_csv(f\"{name}/{name}.csv\")\n",
        "        external_data.append(df)\n",
        "\n",
        "    external_train = pd.concat(external_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWh7C_jN8EGC"
      },
      "source": [
        "if Config.inference:\n",
        "    api = wandb.Api()\n",
        "    for artifact_id in config.models:\n",
        "        name_version = artifact_id.replace(\":\", \"-\")\n",
        "        if not os.path.exists(name_version):\n",
        "            os.makedirs(name_version)\n",
        "\n",
        "        for fold in range(config.n_fold):\n",
        "            try:\n",
        "                artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{artifact_id}\"\n",
        "                artifact = api.artifact(artifact_path)\n",
        "                artifact.download(name_version)\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {artifact_path}, {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JepYxTDVFarm"
      },
      "source": [
        "# EDA-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGORKcNtFcAk",
        "outputId": "ce3e96a9-0220-49a9-e726-5dfecf328144"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "id              0\n",
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "id          0\n",
            "context     0\n",
            "question    0\n",
            "language    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvCjtVwvGacB",
        "outputId": "84cb7525-771c-46ff-a048-edb2fa499be1"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df[\"language\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "hindi    746\n",
            "tamil    368\n",
            "Name: language, dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    3\n",
            "tamil    2\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUXUouHDHa-i",
        "outputId": "7feaaa7f-68cc-4ed2-d0bf-bebac45ca3cf"
      },
      "source": [
        "if config.datasets != []:\n",
        "    print(external_train.isnull().sum())\n",
        "    print(f\"=\" * 120)\n",
        "    print(external_train[\"language\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    6615\n",
            "tamil    3567\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW1WEUKWEPrV"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0_3tcXS_AEO"
      },
      "source": [
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mGJYUQ0liem"
      },
      "source": [
        "def correct_labels(df):\n",
        "    df.loc[df['id'] == '', 'answer_text'] = ''\n",
        "    df.loc[df['id'] == '', 'answer_start'] = 0\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdjatcP_FJ9r"
      },
      "source": [
        "def get_train_data(train):\n",
        "    train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n",
        "\n",
        "    return train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-A8bRGjFVMw"
      },
      "source": [
        "def get_test_data(test):\n",
        "\n",
        "    return test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yae6ysRGvMH"
      },
      "source": [
        "train = get_train_data(train)\n",
        "\n",
        "if config.datasets != []:\n",
        "    external_train = get_train_data(external_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-okotQ7GwJT"
      },
      "source": [
        "test = get_test_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54joajJkBRbm"
      },
      "source": [
        "### External Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8RSTX1SBZef"
      },
      "source": [
        "# 前処理\n",
        "if False and Config.preprocess:\n",
        "    external_squad_translated_tamil[\"language\"] = \"tamil\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0bvUgZyzasg"
      },
      "source": [
        "# dataset 保存\n",
        "if False and Config.preprocess:\n",
        "    !mkdir -p squad_translated_tamil\n",
        "    external_squad_translated_tamil.to_csv(\"squad_translated_tamil/squad_translated_tamil.csv\", index=False)\n",
        "    artifact = wandb.Artifact('squad_translated_tamil', type='dataset')\n",
        "    artifact.add_dir(\"squad_translated_tamil/\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    !mkdir -p mlqa\n",
        "    external_mlqa.to_csv(\"mlqa/mlqa.csv\", index=False)\n",
        "    artifact = wandb.Artifact('mlqa', type='dataset')\n",
        "    artifact.add_dir(\"mlqa/\")\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    !mkdir -p xquad\n",
        "    external_xquad.to_csv(\"xquad/xquad.csv\", index=False)\n",
        "    artifact = wandb.Artifact('xquad', type='dataset')\n",
        "    artifact.add_dir(\"xquad/\")\n",
        "    run.log_artifact(artifact)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEkueCnpHpGW"
      },
      "source": [
        "# EDA-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CEM8_FTTHqrM",
        "outputId": "a5110aa1-f3b9-432d-ec43-431473aacf0c"
      },
      "source": [
        "for df in [train, test, sub]:\n",
        "    print(f\"=\" * 120)\n",
        "    df.info()\n",
        "    display(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1114 entries, 0 to 1113\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id            1114 non-null   object\n",
            " 1   context       1114 non-null   object\n",
            " 2   question      1114 non-null   object\n",
            " 3   answer_text   1114 non-null   object\n",
            " 4   answer_start  1114 non-null   int64 \n",
            " 5   language      1114 non-null   object\n",
            " 6   answers       1114 non-null   object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 61.0+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>903deec17</td>\n",
              "      <td>ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...</td>\n",
              "      <td>மனித உடலில் எத்தனை எலும்புகள் உள்ளன?</td>\n",
              "      <td>206</td>\n",
              "      <td>53</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [53], 'text': ['206']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d9841668c</td>\n",
              "      <td>காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...</td>\n",
              "      <td>காளிதாசன் எங்கு பிறந்தார்?</td>\n",
              "      <td>காசுமீரில்</td>\n",
              "      <td>2358</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [2358], 'text': ['காசுமீரில்']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29d154b56</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...</td>\n",
              "      <td>பென்சிலின் கண்டுபிடித்தவர் யார்?</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங்</td>\n",
              "      <td>0</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41660850a</td>\n",
              "      <td>குழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...</td>\n",
              "      <td>தமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...</td>\n",
              "      <td>தாலாட்டு</td>\n",
              "      <td>68</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [68], 'text': ['தாலாட்டு']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b29c82c22</td>\n",
              "      <td>சூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...</td>\n",
              "      <td>பூமியின் அருகில் உள்ள விண்மீன் எது?</td>\n",
              "      <td>சூரியனும்</td>\n",
              "      <td>585</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [585], 'text': ['சூரியனும்']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            answers\n",
              "0  903deec17  ...            {'answer_start': [53], 'text': ['206']}\n",
              "1  d9841668c  ...   {'answer_start': [2358], 'text': ['காசுமீரில்']}\n",
              "2  29d154b56  ...  {'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...\n",
              "3  41660850a  ...       {'answer_start': [68], 'text': ['தாலாட்டு']}\n",
              "4  b29c82c22  ...     {'answer_start': [585], 'text': ['சூரியனும்']}\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        5 non-null      object\n",
            " 1   context   5 non-null      object\n",
            " 2   question  5 non-null      object\n",
            " 3   language  5 non-null      object\n",
            "dtypes: object(4)\n",
            "memory usage: 288.0+ bytes\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>ज्वाला गुट्टा (जन्म: 7 सितंबर 1983; वर्धा, महा...</td>\n",
              "      <td>ज्वाला गुट्टा की माँ का नाम क्या है</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>गूगल मानचित्र (Google Maps) (पूर्व में गूगल लो...</td>\n",
              "      <td>गूगल मैप्स कब लॉन्च किया गया था?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>गुस्ताव रॉबर्ट किरचॉफ़ (१२ मार्च १८२४ - १७ अक्...</td>\n",
              "      <td>गुस्ताव किरचॉफ का जन्म कब हुआ था?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>அலுமினியம் (ஆங்கிலம்: அலுமினியம்; வட அமெரிக்க ...</td>\n",
              "      <td>அலுமினியத்தின் அணு எண் என்ன?</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>கூட்டுறவு இயக்க வரலாறு, இங்கிலாந்து  நாட்டில் ...</td>\n",
              "      <td>இந்தியாவில் பசுமை புரட்சியின் தந்தை என்று கருத...</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... language\n",
              "0  22bff3dec  ...    hindi\n",
              "1  282758170  ...    hindi\n",
              "2  d60987e0e  ...    hindi\n",
              "3  f99c770dc  ...    tamil\n",
              "4  40dec1964  ...    tamil\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   id                5 non-null      object \n",
            " 1   PredictionString  0 non-null      float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 208.0+ bytes\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  PredictionString\n",
              "0  22bff3dec               NaN\n",
              "1  282758170               NaN\n",
              "2  d60987e0e               NaN\n",
              "3  f99c770dc               NaN\n",
              "4  40dec1964               NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODUeiq5P_O8s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyZ9-uMH2gK"
      },
      "source": [
        "# CV Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6xvuGssH-4"
      },
      "source": [
        "if Config.debug:\n",
        "    train = train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if config.datasets != []:\n",
        "        external_train = external_train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if len(sub) > Config.num_debug_data:\n",
        "        test = test.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "        sub = sub.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XnW0e1AH4Cn",
        "outputId": "e5327c94-b03d-491f-abba-414ae419394e"
      },
      "source": [
        "Fold = StratifiedKFold(n_splits=config.n_fold, shuffle=True, random_state=seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"language\"])):\n",
        "    train.loc[val_index, \"fold\"] = int(n)\n",
        "train[\"fold\"] = train[\"fold\"].astype(np.int8)\n",
        "print(train.groupby([\"fold\", \"language\"]).size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold  language\n",
            "0     hindi       149\n",
            "      tamil        74\n",
            "1     hindi       149\n",
            "      tamil        74\n",
            "2     hindi       149\n",
            "      tamil        74\n",
            "3     hindi       150\n",
            "      tamil        73\n",
            "4     hindi       149\n",
            "      tamil        73\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W902aP490hEk"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_train[\"fold\"] = -1\n",
        "    external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "    train = pd.concat([train, external_train]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0991CRIMH-"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veHPCKXQIJRv"
      },
      "source": [
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    LOGGER.info(f\"[{name}] start\")\n",
        "    yield\n",
        "    LOGGER.info(f\"[{name}] done in {time.time() - t0:.0f} s.\")\n",
        "\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_torch(seed=config.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTjVqALnIXKQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqzv3qSpITRU"
      },
      "source": [
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, df, model_name, include_labels=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = T.AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.features = []\n",
        "        if include_labels:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_train_features(row)\n",
        "        else:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_test_features(row)\n",
        "\n",
        "        self.include_labels = include_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        feature = self.features[item]\n",
        "\n",
        "        if self.include_labels:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                # 'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }\n",
        "\n",
        "    def prepare_train_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "        features = []\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "\n",
        "            input_ids = tokenized_example[\"input_ids\"][i]\n",
        "            attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "            feature['input_ids'] = input_ids\n",
        "            feature['attention_mask'] = attention_mask\n",
        "            feature['offset_mapping'] = offsets\n",
        "\n",
        "            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "            sequence_ids = tokenized_example.sequence_ids(i)\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in sequence_ids]\n",
        "\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = example[\"answers\"]\n",
        "\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != 1:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != 1:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    feature[\"start_position\"] = cls_index\n",
        "                    feature[\"end_position\"] = cls_index\n",
        "                else:\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    feature[\"start_position\"] = token_start_index - 1\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "            features.append(feature)\n",
        "        return features\n",
        "\n",
        "    def prepare_test_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        features = []\n",
        "        for i in range(len(tokenized_example[\"input_ids\"])):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "            feature['input_ids'] = tokenized_example['input_ids'][i]\n",
        "            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n",
        "            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n",
        "            features.append(feature)\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rjufTiUtJb",
        "outputId": "cf91330d-2ac8-49a4-b446-cb2d49936c90"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    train_ds = BaseDataset(train, config.model_name)\n",
        "    print(train_ds[0])\n",
        "    print(len(train_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([     0,  69535,  81049,  37368, 153264,  12095,  52989,  21883,   1629,\n",
            "        145615,     32,      2,      2,   3219, 224013, 124335,   5966,  69535,\n",
            "          4930,  74149,  12095,  52989,  21883, 182394,   3686,  51833,  57210,\n",
            "        101912,     15,   6161,   2912,  70597,  52989,  21883, 102080,  54512,\n",
            "         91585,   1962, 212933,  18599,  16242,  94236,     16, 198236,  29160,\n",
            "         12095,  52989,  21883, 173139,  23618,  72817,      5,   5894, 198236,\n",
            "         81049,  37334, 144257,   7827,  82890,  84853,  80517, 114452, 232094,\n",
            "          3686,  17984,  11830,  62001, 182394,   4167,      5, 203312,  10753,\n",
            "         50667,   2650,      4,  45303,   1962, 163062, 198236,  29160, 176030,\n",
            "         15453,      4,   3219, 171093,   5944,   2650,   8120,  10175,  12095,\n",
            "         52989,  21883,     15,   2650,  24183,   5638,  14861,     16,  56735,\n",
            "          3219, 171093,   5944,   2650,  12009, 145578,  10832,   2802,   2650,\n",
            "         26873,  52989,  21883,  53336,  26415,  38640,  31067,     74, 105457,\n",
            "          5966,  22050,  12095,  52989,  21883, 202342,  59386,  12095,  52989,\n",
            "         13070,   2650,   1962,  39311,   9654,  37964,  18806,      4, 196586,\n",
            "        105457,   5966,  22467,  78876,  52989,  21883,     74, 102080,   6896,\n",
            "            20,  21162,  14233,  94097,   4864,  12152,  12095,  52989,  21883,\n",
            "          1629, 210828,   1381, 198236,   2782, 191297,  10832,   2802,   2650,\n",
            "         26873,  52989,  21883,   1629,   3912,  59987,   1962, 212933,  26415,\n",
            "         20567,      5,  69535,   9696, 184936,  28258,  89933,   1039,  12095,\n",
            "         52989,  21883,   1629,     15,  10753,   2802,   6149,  11674,  16043,\n",
            "         26873,   2913,  21883, 202342, 137010,     16, 145615,     74, 164359,\n",
            "         12095,  11993, 181576,  87783,  35186,     15,  15182,  53208,     16,\n",
            "         12095,  52989,  21883,  91585,  11772,    616,  71987,  12095,  52989,\n",
            "         21883,  91585,  11772,     15,   1021,  26136,  32881,      7,     16,\n",
            "         73417,  73949, 163493,      5,     15,   4875,   5427,   3770, 136259,\n",
            "          1629,  88115,   2650, 230988, 112578,  53336,   4167, 136259, 173139,\n",
            "             6,  79464,   2798, 143825,      5,     16, 181576,  87783,  35186,\n",
            "         12095,  52989,  21883,   1629,  10021,    106, 190707,   4875, 128236,\n",
            "         52989,  21883,     15,  20549,    289,  32881,     16,    116,  14184,\n",
            "          3937, 145181,  52989,  21883,     15,  24980,     13,   1803,  32881,\n",
            "            16,   1737,    138, 116180,  17056,   3686,   4875, 128236,  52989,\n",
            "         21883,     15,  99736,    289,  32881,     16,   1737,    201,  22262,\n",
            "         95344,  12095,  52989,  21883,     15,   6652,  88354,    289,  32881,\n",
            "            16,   6001,   6343,   8850,  12095,  52989,  21883,     15,      7,\n",
            "         88322,  48899,  32881,     16,  60070,  12784,   2782,  35424,  26873,\n",
            "         52989,  21883,     15,  12421,    432,    532,  32881,     16,  71987,\n",
            "         12095,  52989,  21883,   1629,  31203,    361, 145578,  51153,  18805,\n",
            "         12095,  52989,  21883,     15,  12018,  28236,     16,    305, 177292,\n",
            "         95424,  18805,  12095,  52989,  21883,     15,  24084,   2298,     16,\n",
            "          1737,   2690,  42353,  78876,  52989,  21883,     15,  16917,  10325,\n",
            "         32881,     16,   1737,    190,   6390,  62481,  12095,  52989,  21883,\n",
            "            15,   3285,    519,  47148,  32881,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start_position': tensor(27), 'end_position': tensor(27)}\n",
            "26881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBEpGYP-kjH",
        "outputId": "fea9d66c-92f3-4e4f-ea6a-b8589ce04abc"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    test_ds = BaseDataset(test, config.model_name, include_labels=False)\n",
        "    print(test_ds[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([     0,      6,  38033,  91262,  20546,  85149,    471,  58380,    641,\n",
            "          8062,   6004,    460,      2,      2,      6,  38033,  91262,  20546,\n",
            "         85149,     15, 206327,     12,    361, 182198,  26819,     74,      6,\n",
            "        196859,   1026,      4,  15297,     16,    967,   9261,  41162, 156793,\n",
            "         64382,  46297, 103766,   1404,   7294,   1293,    125, 222600,   5725,\n",
            "         11515,      6,  38033,  91262,  20546,  85149,    641,  22274,    361,\n",
            "        182198,  26819,    629,      6, 196859,   1026,      4,  15297,    421,\n",
            "         11645,   3813,    125,  24939,  62573,  39477,      5, 233097,   4429,\n",
            "         45951,  25594,    871,  56980,   4149,   1471,    998,  23263,    646,\n",
            "          1293,    125,  35056,  56980,   4149,   1471,    998,  20546,  85149,\n",
            "         47211,  12797,  27193,    421,   5564, 220307,   9331,    287,   3765,\n",
            "          3946,  13430, 120116,    125,      6,  38033,  91262,  20546,  85149,\n",
            "           471, 222600,   5725,  53812,   9729, 204778,    646,  17035,    871,\n",
            "         33171,    838,    646,  21191,  41162, 156793,  64382,  46297,  12189,\n",
            "          1748,   1780,  26252,   4029,    125,  82645, 209393,    209,   9512,\n",
            "           471,  78087,    646,   2093,      6,  38033,  91262,  20546,  85149,\n",
            "          1142,  31160,      5,  31598,      5,  33142, 160674,    646,  77022,\n",
            "         10067, 166922,  26252,   1896,   8785,   3813,    125,  31160,      5,\n",
            "         31598,      5,  33142, 160674,   3946,    287,  13325,   4592,   1187,\n",
            "         12189, 201742,   1293, 207411,   8771,  74163,   4846, 157426, 138314,\n",
            "           646, 191604,   4029,   5349,    460,    125,  47211,  12797,    702,\n",
            "          9512,    471,  78087,    421,  21191,  22009,   4415, 188408,  41162,\n",
            "        156793,  64382,  46297, 115739,  77666,  51476, 151576,  41657,    659,\n",
            "          9917,    125,   9512,   3576,    421,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,    729,   9512,    471,  78087,    421,  70159,  85134,\n",
            "        188408,  41162, 156793,  64382,  46297, 115739,  77666,  51476, 151576,\n",
            "         41657,    659,    125,  64021,   9512,  21191,      6, 170555,   3558,\n",
            "        159967,  51476,    287,   3765, 227649,   6473,    421,  91298,    659,\n",
            "         17837,   2617,   9179,  73457,    287, 227649,   6473,  70159,  85134,\n",
            "        188408,  41162, 156793,  64382,  46297, 115739,  77666,  51476, 151576,\n",
            "           871,  13371,    998,  85134, 188408,  41162, 156793,  64382,  46297,\n",
            "        115739,  77666,  51476, 151576,    421,  41657,  76613,    471,    125,\n",
            "             6, 170555,   3558, 159967,  51476,    287,   3765,  35056,  91298,\n",
            "           659,  52170, 195730,   6927,   7231, 192872,    125,   5726,    646,\n",
            "          2021,   7231,  73451,  32534,  12797,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,  73457,    287, 188408, 115474,   1471,  73254,    421,\n",
            "         41657,  76613,    471,    125,  39556,   9236, 227649,   6473,    287,\n",
            "          3765,      9, 105456,      6,  38033,  91262,  20546,  85149,   1142,\n",
            "        119253,   3282, 227649,   6473,    421,   1780,  67963,  76613,    471,\n",
            "           871,   3946,    471, 227649,   6473,    421,  13353, 203104, 160161,\n",
            "        118689,    838,    125,  54968,   1532,  59308,  26609,   8683,  13056,\n",
            "          8531, 156180,   6473,    421,   1780,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,   5564, 221876,   2139,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'offset_mapping': [(0, 0), (0, 1), (0, 2), (2, 6), (6, 9), (9, 13), (13, 16), (16, 20), (20, 23), (23, 27), (27, 32), (32, 35), (0, 0), (0, 0), (0, 1), (0, 2), (2, 6), (6, 9), (9, 13), (13, 15), (15, 19), (19, 20), (20, 22), (22, 29), (29, 34), (34, 35), (35, 36), (36, 40), (40, 41), (41, 42), (42, 53), (53, 54), (54, 57), (57, 64), (64, 67), (67, 69), (69, 71), (71, 73), (73, 76), (76, 78), (78, 80), (80, 84), (84, 85), (87, 95), (95, 97), (97, 102), (103, 104), (104, 106), (106, 110), (110, 113), (113, 117), (117, 120), (120, 125), (125, 127), (127, 134), (134, 139), (139, 142), (142, 143), (143, 147), (147, 148), (148, 149), (149, 160), (160, 164), (164, 168), (168, 171), (171, 172), (172, 177), (177, 182), (182, 185), (185, 186), (186, 194), (194, 197), (197, 199), (199, 201), (201, 204), (204, 208), (208, 211), (211, 212), (212, 213), (213, 217), (217, 220), (220, 224), (224, 225), (225, 230), (230, 234), (234, 237), (237, 238), (238, 239), (239, 242), (242, 246), (246, 251), (251, 255), (255, 260), (260, 264), (264, 269), (269, 274), (274, 277), (277, 280), (280, 284), (284, 289), (289, 292), (292, 296), (296, 297), (297, 298), (298, 300), (300, 304), (304, 307), (307, 311), (311, 314), (314, 322), (322, 324), (324, 328), (328, 330), (330, 339), (339, 342), (342, 346), (346, 349), (349, 353), (353, 354), (354, 357), (357, 366), (366, 369), (369, 371), (371, 373), (373, 375), (375, 379), (379, 381), (381, 384), (384, 389), (389, 394), (394, 395), (397, 400), (400, 404), (405, 408), (408, 412), (412, 415), (415, 420), (420, 423), (423, 426), (426, 427), (427, 429), (429, 433), (433, 436), (436, 440), (440, 443), (443, 446), (446, 447), (447, 449), (449, 450), (450, 453), (453, 455), (455, 458), (458, 464), (464, 467), (467, 472), (472, 477), (477, 480), (480, 485), (485, 488), (488, 489), (489, 492), (492, 493), (493, 495), (495, 496), (496, 499), (499, 501), (501, 506), (506, 509), (509, 514), (514, 517), (517, 519), (519, 523), (523, 533), (533, 537), (537, 545), (545, 547), (547, 550), (550, 551), (551, 557), (557, 564), (564, 567), (567, 576), (576, 581), (581, 585), (585, 588), (588, 589), (589, 594), (594, 598), (598, 601), (601, 605), (605, 608), (608, 613), (613, 617), (617, 626), (626, 629), (629, 631), (631, 637), (637, 640), (640, 642), (642, 644), (644, 646), (646, 649), (649, 651), (651, 654), (654, 657), (657, 661), (661, 662), (662, 665), (665, 666), (666, 670), (670, 675), (675, 679), (679, 680), (680, 682), (682, 686), (686, 689), (689, 693), (693, 696), (696, 699), (699, 703), (703, 706), (706, 711), (711, 715), (715, 719), (719, 722), (722, 728), (728, 731), (731, 733), (733, 735), (735, 737), (737, 740), (740, 742), (742, 745), (745, 748), (748, 752), (752, 753), (753, 754), (754, 758), (758, 762), (762, 771), (771, 772), (772, 777), (777, 778), (778, 782), (782, 785), (785, 788), (788, 792), (792, 796), (796, 798), (798, 802), (802, 807), (807, 808), (808, 812), (812, 814), (814, 818), (818, 826), (826, 829), (829, 833), (833, 835), (835, 839), (839, 842), (842, 848), (848, 851), (851, 853), (853, 855), (855, 857), (857, 860), (860, 862), (862, 865), (865, 868), (868, 871), (871, 874), (874, 875), (875, 878), (878, 884), (884, 887), (887, 889), (889, 891), (891, 893), (893, 896), (896, 898), (898, 901), (901, 904), (904, 908), (908, 912), (912, 918), (918, 921), (921, 922), (922, 923), (923, 928), (928, 929), (929, 933), (933, 936), (936, 939), (939, 943), (943, 948), (948, 953), (953, 954), (954, 959), (959, 964), (964, 968), (968, 971), (971, 975), (975, 976), (976, 981), (981, 984), (984, 989), (989, 992), (992, 999), (999, 1003), (1003, 1007), (1007, 1008), (1008, 1010), (1010, 1014), (1014, 1017), (1017, 1021), (1021, 1024), (1024, 1032), (1032, 1035), (1035, 1041), (1041, 1045), (1045, 1046), (1046, 1058), (1058, 1062), (1062, 1066), (1066, 1072), (1072, 1075), (1075, 1076), (1076, 1079), (1079, 1085), (1085, 1089), (1089, 1091), (1091, 1094), (1094, 1098), (1098, 1099), (1099, 1102), (1102, 1103), (1103, 1105), (1105, 1109), (1109, 1112), (1112, 1116), (1116, 1119), (1119, 1125), (1125, 1127), (1127, 1131), (1131, 1133), (1133, 1137), (1137, 1140), (1140, 1146), (1146, 1152), (1152, 1155), (1155, 1158), (1158, 1163), (1163, 1166), (1166, 1170), (1170, 1172), (1172, 1176), (1176, 1181), (1181, 1189), (1189, 1197), (1197, 1201), (1201, 1202), (1202, 1203), (1203, 1206), (1206, 1211), (1211, 1214), (1214, 1216), (1216, 1218), (1218, 1220), (1220, 1221), (1221, 1225), (1225, 1227), (1227, 1231), (1231, 1234), (1234, 1235), (1235, 1237), (1237, 1241), (1241, 1244), (1244, 1248), (1248, 1251), (1251, 1256), (1256, 1264), (1264, 1266), (0, 0)], 'sequence_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'id': '22bff3dec', 'context': 'ज्वाला गुट्टा (जन्म: 7 सितंबर 1983; वर्धा, महाराष्ट्र) एक भारतीय बैडमिंटन खिलाडी हैं। \\n प्रारंभिक जीवन \\nज्वाला गुट्टा का जन्म 7 सितंबर 1983 को वर्धा, महाराष्ट्र में हुआ था। उनके पिता एम. क्रांति तेलुगु और मां येलन चीन से हैं। उनकी मां येलन गुट्टा पहली बार 1977 में अपने दादा जी के साथ भारत आई थीं। ज्वाला गुट्टा की प्रारंभिक पढ़ाई हैदराबाद से हुई और यहीं से उन्होंने बैडमिंटन खेलना भी शुरू किया। \\n कॅरियर \\n10 साल की उम्र से ही ज्वाला गुट्टा ने एस.एम. आरिफ से ट्रेनिंग लेना शुरू कर दिया था। एस.एम. आरिफ भारत के जाने माने खेल प्रशिक्षक हैं जिन्हें द्रोणाचार्य अवार्ड से सम्मानित किया गया है। पहली बार 13 साल की उम्र में उन्होंने मिनी नेशनल बैडमिंटन चैंपियनशिप जीती थी। साल 2000 में ज्वाला गुट्टा ने 17 साल की उम्र में जूनियर नेशनल बैडमिंटन चैंपियनशिप जीती। इसी साल उन्होंने श्रुति कुरियन के साथ डबल्स में जोड़ी बनाते हुए महिलाओं के डबल्स जूनियर नेशनल बैडमिंटन चैंपियनशिप और सीनियर नेशनल बैडमिंटन चैंपियनशिप में जीत हासिल की। श्रुति कुरियन के साथ उनकी जोड़ी काफी लंबे समय तक चली। 2002 से 2008 तक लगातार सात बार ज्वाला गुट्टा ने महिलाओं के नेशनल युगल प्रतियोगिता में जीत हासिल की।[2]\\nमहिला डबल्स के साथ-साथ ज्वाला गुट्टा ने मिश्रित डबल्स में भी सफलता हासिल की और भारत की डबल्स में सबसे बेहतरीन खिलाड़ी बनीं।[3] 2010 कॉमनवेल्थ गेम्स में भी ज्वाला गुट्टा ने अपने पार्टनर अश्विनी पोनप्पा के साथ भारत के लिए स्वर्ण पदक जीता। कॉमनवेल्थ गेम्स के बाद से एक बार फिर ज्वाला गुट्टा भारतीय बैडमिंटन में चर्चा का विषय बन गई हैं।[4][5]\\nग्लासगो में आयोजित कॉमनवेल्थ गेम्स, 2014 में ज्वाला गुट्टा ने स्वर्ण पदक हासिल किया।\\n व्यक्तिगत जीवन \\nमैदान पर बाएं हाथ से तेज-तर्रार शॉट लगाने वाली ज्वाला निजी जिंदगी में भी काफी तेज और चर्चाओं में छाई रहती हैं। ज्वाला ने 2005 में बैडमिंटन खिलाड़ी चेतन आनंद से शादी की थी, 29 जून 2011 को उन्होंने अपने पति पूर्व बैडमिंटन खिलाड़ी चेतन आनंद से तलाक लिया है। चेतन आनंद भी एक बेहतरीन भारतीय बैडमिंटन खिलाड़ी हैं।\\n फिल्मोग्राफी \\nGunde Jaari Gallanthayyinde[6] \\nफुगली (2014)\\n उपलब्धियां \\nरिकॉर्ड 13 बार नेशनल बैडमिंटन चैंपियनशिप की विजेता। \\nभारत की सबसे बेहतरीन डबल्स प्लेयर। \\nसाल 2011 में उन्हें “अर्जुन पुरस्कार” से सम्मानित किया गया। \\nराष्ट्रमंडल खेल, 2014 (ग्लासगो) में स्वर्ण पदक जीता। \\n चित्र दीर्घा \\n\\n\\nवी दीजू और ज्वाला गुट्टा\\nकेबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\\nकेबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\\n\\n सन्दर्भ \\n\\n बाहरी कड़ियाँ \\n\\n\\n\\n\\n\\nश्रेणी:हिन्द की बेटियाँ\\nश्रेणी:विकिपरियोजना हिन्द की बेटियाँ\\nश्रेणी:भारत के खिलाड़ी\\nश्रेणी:1983 में जन्मे लोग\\nश्रेणी:जीवित लोग\\nश्रेणी:भारतीय महिला बैडमिंटन खिलाड़ी\\nश्रेणी:राष्ट्रमंडल खेलों के पदक प्राप्तकर्ता\\nश्रेणी:महाराष्ट्र के लोग\\nश्रेणी:बैडमिंटन खिलाड़ी', 'question': 'ज्वाला गुट्टा की माँ का नाम क्या है'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4zuBuCCAv-8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tJieX2AAueP",
        "outputId": "d5621131-fb47-421c-ec47-a3b1d707bcfc"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    print(T.AutoConfig.from_pretrained(config.model_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"language\": \"english\",\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"name\": \"XLMRoberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZtghUknA1b8"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.auto_config = T.AutoConfig.from_pretrained(model_name)\n",
        "        self.auto_config.update({\n",
        "            \"hidden_dropout_prob\": config.dropout,\n",
        "            \"layer_norm_eps\": 1e-7,\n",
        "        })\n",
        "\n",
        "        self.auto_model = T.AutoModel.from_pretrained(model_name, config=self.auto_config)\n",
        "\n",
        "        self.qa_outputs = nn.Linear(self.auto_config.hidden_size, 2)\n",
        "\n",
        "        if config.init_weights:\n",
        "            self._init_weights(self.qa_outputs)\n",
        "\n",
        "        if config.init_layers > 0:\n",
        "            self._init_layers()\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.auto_config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def _init_layers(self):\n",
        "        # re-init pooler\n",
        "        # self.auto_model.pooler.dense.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "        # self.auto_model.pooler.dense.bias.data.zero_()\n",
        "        # for p in self.auto_model.pooler.parameters():\n",
        "        #     p.requires_grad = True\n",
        "\n",
        "        # re-init encoder\n",
        "        layers = self.auto_model.encoder.layer[-config.init_layers:]\n",
        "        for layer in layers:\n",
        "            for module in layer.modules():\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "                    # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "                    module.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "                    if module.bias is not None:\n",
        "                        module.bias.data.zero_()\n",
        "                elif isinstance(module, nn.Embedding):\n",
        "                    module.weight.data.normal_(mean=0.0, std=self.auto_model.config.initializer_range)\n",
        "                    if module.padding_idx is not None:\n",
        "                        module.weight.data[module.padding_idx].zero_()\n",
        "                elif isinstance(module, nn.LayerNorm):\n",
        "                    module.bias.data.zero_()\n",
        "                    module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.auto_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        \n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        \n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M92xPyOyCSQZ",
        "outputId": "52036118-f2a2-4edb-95e5-d285e80ca400"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    model = BaseModel(config.model_name)\n",
        "    print(model)\n",
        "\n",
        "    train_dataset = BaseDataset(train, config.model_name)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    for features in train_loader:\n",
        "        output = model(features[\"input_ids\"], features[\"attention_mask\"])\n",
        "        print(output)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseModel(\n",
            "  (auto_model): XLMRobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
            ")\n",
            "(tensor([[-0.0449, -0.1836,  0.0572,  ..., -0.2571, -0.1786,  0.4821],\n",
            "        [-0.0183,  0.3002,  0.2683,  ...,  0.5663,  0.5663,  0.5663],\n",
            "        [-0.0947,  0.3354, -0.0305,  ...,  0.4237,  0.4237,  0.4237],\n",
            "        [ 0.0760,  0.4362,  0.8359,  ..., -0.0822,  0.2064,  0.5684]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[-0.5815, -0.2798, -0.5579,  ..., -0.5710,  0.0297, -0.9712],\n",
            "        [-0.5924, -0.5869, -0.5174,  ..., -0.8569, -0.8569, -0.8569],\n",
            "        [-0.5595, -0.1239,  0.2446,  ..., -0.9003, -0.9003, -0.9003],\n",
            "        [-0.6841, -0.6954, -0.9542,  ...,  0.5764,  0.2951, -0.9454]],\n",
            "       grad_fn=<SqueezeBackward1>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wEQxmYCwUr",
        "outputId": "e710f169-e66e-4ba7-a8ee-d19a96cb5e51"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    for n, (name, tensor) in enumerate(list(model.named_parameters())):\n",
        "        print(f\"{n:>4}: {tensor.requires_grad}, {name}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0: True, auto_model.embeddings.word_embeddings.weight\n",
            "   1: True, auto_model.embeddings.position_embeddings.weight\n",
            "   2: True, auto_model.embeddings.token_type_embeddings.weight\n",
            "   3: True, auto_model.embeddings.LayerNorm.weight\n",
            "   4: True, auto_model.embeddings.LayerNorm.bias\n",
            "   5: True, auto_model.encoder.layer.0.attention.self.query.weight\n",
            "   6: True, auto_model.encoder.layer.0.attention.self.query.bias\n",
            "   7: True, auto_model.encoder.layer.0.attention.self.key.weight\n",
            "   8: True, auto_model.encoder.layer.0.attention.self.key.bias\n",
            "   9: True, auto_model.encoder.layer.0.attention.self.value.weight\n",
            "  10: True, auto_model.encoder.layer.0.attention.self.value.bias\n",
            "  11: True, auto_model.encoder.layer.0.attention.output.dense.weight\n",
            "  12: True, auto_model.encoder.layer.0.attention.output.dense.bias\n",
            "  13: True, auto_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "  14: True, auto_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "  15: True, auto_model.encoder.layer.0.intermediate.dense.weight\n",
            "  16: True, auto_model.encoder.layer.0.intermediate.dense.bias\n",
            "  17: True, auto_model.encoder.layer.0.output.dense.weight\n",
            "  18: True, auto_model.encoder.layer.0.output.dense.bias\n",
            "  19: True, auto_model.encoder.layer.0.output.LayerNorm.weight\n",
            "  20: True, auto_model.encoder.layer.0.output.LayerNorm.bias\n",
            "  21: True, auto_model.encoder.layer.1.attention.self.query.weight\n",
            "  22: True, auto_model.encoder.layer.1.attention.self.query.bias\n",
            "  23: True, auto_model.encoder.layer.1.attention.self.key.weight\n",
            "  24: True, auto_model.encoder.layer.1.attention.self.key.bias\n",
            "  25: True, auto_model.encoder.layer.1.attention.self.value.weight\n",
            "  26: True, auto_model.encoder.layer.1.attention.self.value.bias\n",
            "  27: True, auto_model.encoder.layer.1.attention.output.dense.weight\n",
            "  28: True, auto_model.encoder.layer.1.attention.output.dense.bias\n",
            "  29: True, auto_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "  30: True, auto_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "  31: True, auto_model.encoder.layer.1.intermediate.dense.weight\n",
            "  32: True, auto_model.encoder.layer.1.intermediate.dense.bias\n",
            "  33: True, auto_model.encoder.layer.1.output.dense.weight\n",
            "  34: True, auto_model.encoder.layer.1.output.dense.bias\n",
            "  35: True, auto_model.encoder.layer.1.output.LayerNorm.weight\n",
            "  36: True, auto_model.encoder.layer.1.output.LayerNorm.bias\n",
            "  37: True, auto_model.encoder.layer.2.attention.self.query.weight\n",
            "  38: True, auto_model.encoder.layer.2.attention.self.query.bias\n",
            "  39: True, auto_model.encoder.layer.2.attention.self.key.weight\n",
            "  40: True, auto_model.encoder.layer.2.attention.self.key.bias\n",
            "  41: True, auto_model.encoder.layer.2.attention.self.value.weight\n",
            "  42: True, auto_model.encoder.layer.2.attention.self.value.bias\n",
            "  43: True, auto_model.encoder.layer.2.attention.output.dense.weight\n",
            "  44: True, auto_model.encoder.layer.2.attention.output.dense.bias\n",
            "  45: True, auto_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "  46: True, auto_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "  47: True, auto_model.encoder.layer.2.intermediate.dense.weight\n",
            "  48: True, auto_model.encoder.layer.2.intermediate.dense.bias\n",
            "  49: True, auto_model.encoder.layer.2.output.dense.weight\n",
            "  50: True, auto_model.encoder.layer.2.output.dense.bias\n",
            "  51: True, auto_model.encoder.layer.2.output.LayerNorm.weight\n",
            "  52: True, auto_model.encoder.layer.2.output.LayerNorm.bias\n",
            "  53: True, auto_model.encoder.layer.3.attention.self.query.weight\n",
            "  54: True, auto_model.encoder.layer.3.attention.self.query.bias\n",
            "  55: True, auto_model.encoder.layer.3.attention.self.key.weight\n",
            "  56: True, auto_model.encoder.layer.3.attention.self.key.bias\n",
            "  57: True, auto_model.encoder.layer.3.attention.self.value.weight\n",
            "  58: True, auto_model.encoder.layer.3.attention.self.value.bias\n",
            "  59: True, auto_model.encoder.layer.3.attention.output.dense.weight\n",
            "  60: True, auto_model.encoder.layer.3.attention.output.dense.bias\n",
            "  61: True, auto_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "  62: True, auto_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "  63: True, auto_model.encoder.layer.3.intermediate.dense.weight\n",
            "  64: True, auto_model.encoder.layer.3.intermediate.dense.bias\n",
            "  65: True, auto_model.encoder.layer.3.output.dense.weight\n",
            "  66: True, auto_model.encoder.layer.3.output.dense.bias\n",
            "  67: True, auto_model.encoder.layer.3.output.LayerNorm.weight\n",
            "  68: True, auto_model.encoder.layer.3.output.LayerNorm.bias\n",
            "  69: True, auto_model.encoder.layer.4.attention.self.query.weight\n",
            "  70: True, auto_model.encoder.layer.4.attention.self.query.bias\n",
            "  71: True, auto_model.encoder.layer.4.attention.self.key.weight\n",
            "  72: True, auto_model.encoder.layer.4.attention.self.key.bias\n",
            "  73: True, auto_model.encoder.layer.4.attention.self.value.weight\n",
            "  74: True, auto_model.encoder.layer.4.attention.self.value.bias\n",
            "  75: True, auto_model.encoder.layer.4.attention.output.dense.weight\n",
            "  76: True, auto_model.encoder.layer.4.attention.output.dense.bias\n",
            "  77: True, auto_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "  78: True, auto_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "  79: True, auto_model.encoder.layer.4.intermediate.dense.weight\n",
            "  80: True, auto_model.encoder.layer.4.intermediate.dense.bias\n",
            "  81: True, auto_model.encoder.layer.4.output.dense.weight\n",
            "  82: True, auto_model.encoder.layer.4.output.dense.bias\n",
            "  83: True, auto_model.encoder.layer.4.output.LayerNorm.weight\n",
            "  84: True, auto_model.encoder.layer.4.output.LayerNorm.bias\n",
            "  85: True, auto_model.encoder.layer.5.attention.self.query.weight\n",
            "  86: True, auto_model.encoder.layer.5.attention.self.query.bias\n",
            "  87: True, auto_model.encoder.layer.5.attention.self.key.weight\n",
            "  88: True, auto_model.encoder.layer.5.attention.self.key.bias\n",
            "  89: True, auto_model.encoder.layer.5.attention.self.value.weight\n",
            "  90: True, auto_model.encoder.layer.5.attention.self.value.bias\n",
            "  91: True, auto_model.encoder.layer.5.attention.output.dense.weight\n",
            "  92: True, auto_model.encoder.layer.5.attention.output.dense.bias\n",
            "  93: True, auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "  94: True, auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "  95: True, auto_model.encoder.layer.5.intermediate.dense.weight\n",
            "  96: True, auto_model.encoder.layer.5.intermediate.dense.bias\n",
            "  97: True, auto_model.encoder.layer.5.output.dense.weight\n",
            "  98: True, auto_model.encoder.layer.5.output.dense.bias\n",
            "  99: True, auto_model.encoder.layer.5.output.LayerNorm.weight\n",
            " 100: True, auto_model.encoder.layer.5.output.LayerNorm.bias\n",
            " 101: True, auto_model.encoder.layer.6.attention.self.query.weight\n",
            " 102: True, auto_model.encoder.layer.6.attention.self.query.bias\n",
            " 103: True, auto_model.encoder.layer.6.attention.self.key.weight\n",
            " 104: True, auto_model.encoder.layer.6.attention.self.key.bias\n",
            " 105: True, auto_model.encoder.layer.6.attention.self.value.weight\n",
            " 106: True, auto_model.encoder.layer.6.attention.self.value.bias\n",
            " 107: True, auto_model.encoder.layer.6.attention.output.dense.weight\n",
            " 108: True, auto_model.encoder.layer.6.attention.output.dense.bias\n",
            " 109: True, auto_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            " 110: True, auto_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            " 111: True, auto_model.encoder.layer.6.intermediate.dense.weight\n",
            " 112: True, auto_model.encoder.layer.6.intermediate.dense.bias\n",
            " 113: True, auto_model.encoder.layer.6.output.dense.weight\n",
            " 114: True, auto_model.encoder.layer.6.output.dense.bias\n",
            " 115: True, auto_model.encoder.layer.6.output.LayerNorm.weight\n",
            " 116: True, auto_model.encoder.layer.6.output.LayerNorm.bias\n",
            " 117: True, auto_model.encoder.layer.7.attention.self.query.weight\n",
            " 118: True, auto_model.encoder.layer.7.attention.self.query.bias\n",
            " 119: True, auto_model.encoder.layer.7.attention.self.key.weight\n",
            " 120: True, auto_model.encoder.layer.7.attention.self.key.bias\n",
            " 121: True, auto_model.encoder.layer.7.attention.self.value.weight\n",
            " 122: True, auto_model.encoder.layer.7.attention.self.value.bias\n",
            " 123: True, auto_model.encoder.layer.7.attention.output.dense.weight\n",
            " 124: True, auto_model.encoder.layer.7.attention.output.dense.bias\n",
            " 125: True, auto_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            " 126: True, auto_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            " 127: True, auto_model.encoder.layer.7.intermediate.dense.weight\n",
            " 128: True, auto_model.encoder.layer.7.intermediate.dense.bias\n",
            " 129: True, auto_model.encoder.layer.7.output.dense.weight\n",
            " 130: True, auto_model.encoder.layer.7.output.dense.bias\n",
            " 131: True, auto_model.encoder.layer.7.output.LayerNorm.weight\n",
            " 132: True, auto_model.encoder.layer.7.output.LayerNorm.bias\n",
            " 133: True, auto_model.encoder.layer.8.attention.self.query.weight\n",
            " 134: True, auto_model.encoder.layer.8.attention.self.query.bias\n",
            " 135: True, auto_model.encoder.layer.8.attention.self.key.weight\n",
            " 136: True, auto_model.encoder.layer.8.attention.self.key.bias\n",
            " 137: True, auto_model.encoder.layer.8.attention.self.value.weight\n",
            " 138: True, auto_model.encoder.layer.8.attention.self.value.bias\n",
            " 139: True, auto_model.encoder.layer.8.attention.output.dense.weight\n",
            " 140: True, auto_model.encoder.layer.8.attention.output.dense.bias\n",
            " 141: True, auto_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            " 142: True, auto_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            " 143: True, auto_model.encoder.layer.8.intermediate.dense.weight\n",
            " 144: True, auto_model.encoder.layer.8.intermediate.dense.bias\n",
            " 145: True, auto_model.encoder.layer.8.output.dense.weight\n",
            " 146: True, auto_model.encoder.layer.8.output.dense.bias\n",
            " 147: True, auto_model.encoder.layer.8.output.LayerNorm.weight\n",
            " 148: True, auto_model.encoder.layer.8.output.LayerNorm.bias\n",
            " 149: True, auto_model.encoder.layer.9.attention.self.query.weight\n",
            " 150: True, auto_model.encoder.layer.9.attention.self.query.bias\n",
            " 151: True, auto_model.encoder.layer.9.attention.self.key.weight\n",
            " 152: True, auto_model.encoder.layer.9.attention.self.key.bias\n",
            " 153: True, auto_model.encoder.layer.9.attention.self.value.weight\n",
            " 154: True, auto_model.encoder.layer.9.attention.self.value.bias\n",
            " 155: True, auto_model.encoder.layer.9.attention.output.dense.weight\n",
            " 156: True, auto_model.encoder.layer.9.attention.output.dense.bias\n",
            " 157: True, auto_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            " 158: True, auto_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            " 159: True, auto_model.encoder.layer.9.intermediate.dense.weight\n",
            " 160: True, auto_model.encoder.layer.9.intermediate.dense.bias\n",
            " 161: True, auto_model.encoder.layer.9.output.dense.weight\n",
            " 162: True, auto_model.encoder.layer.9.output.dense.bias\n",
            " 163: True, auto_model.encoder.layer.9.output.LayerNorm.weight\n",
            " 164: True, auto_model.encoder.layer.9.output.LayerNorm.bias\n",
            " 165: True, auto_model.encoder.layer.10.attention.self.query.weight\n",
            " 166: True, auto_model.encoder.layer.10.attention.self.query.bias\n",
            " 167: True, auto_model.encoder.layer.10.attention.self.key.weight\n",
            " 168: True, auto_model.encoder.layer.10.attention.self.key.bias\n",
            " 169: True, auto_model.encoder.layer.10.attention.self.value.weight\n",
            " 170: True, auto_model.encoder.layer.10.attention.self.value.bias\n",
            " 171: True, auto_model.encoder.layer.10.attention.output.dense.weight\n",
            " 172: True, auto_model.encoder.layer.10.attention.output.dense.bias\n",
            " 173: True, auto_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            " 174: True, auto_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            " 175: True, auto_model.encoder.layer.10.intermediate.dense.weight\n",
            " 176: True, auto_model.encoder.layer.10.intermediate.dense.bias\n",
            " 177: True, auto_model.encoder.layer.10.output.dense.weight\n",
            " 178: True, auto_model.encoder.layer.10.output.dense.bias\n",
            " 179: True, auto_model.encoder.layer.10.output.LayerNorm.weight\n",
            " 180: True, auto_model.encoder.layer.10.output.LayerNorm.bias\n",
            " 181: True, auto_model.encoder.layer.11.attention.self.query.weight\n",
            " 182: True, auto_model.encoder.layer.11.attention.self.query.bias\n",
            " 183: True, auto_model.encoder.layer.11.attention.self.key.weight\n",
            " 184: True, auto_model.encoder.layer.11.attention.self.key.bias\n",
            " 185: True, auto_model.encoder.layer.11.attention.self.value.weight\n",
            " 186: True, auto_model.encoder.layer.11.attention.self.value.bias\n",
            " 187: True, auto_model.encoder.layer.11.attention.output.dense.weight\n",
            " 188: True, auto_model.encoder.layer.11.attention.output.dense.bias\n",
            " 189: True, auto_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            " 190: True, auto_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            " 191: True, auto_model.encoder.layer.11.intermediate.dense.weight\n",
            " 192: True, auto_model.encoder.layer.11.intermediate.dense.bias\n",
            " 193: True, auto_model.encoder.layer.11.output.dense.weight\n",
            " 194: True, auto_model.encoder.layer.11.output.dense.bias\n",
            " 195: True, auto_model.encoder.layer.11.output.LayerNorm.weight\n",
            " 196: True, auto_model.encoder.layer.11.output.LayerNorm.bias\n",
            " 197: True, auto_model.encoder.layer.12.attention.self.query.weight\n",
            " 198: True, auto_model.encoder.layer.12.attention.self.query.bias\n",
            " 199: True, auto_model.encoder.layer.12.attention.self.key.weight\n",
            " 200: True, auto_model.encoder.layer.12.attention.self.key.bias\n",
            " 201: True, auto_model.encoder.layer.12.attention.self.value.weight\n",
            " 202: True, auto_model.encoder.layer.12.attention.self.value.bias\n",
            " 203: True, auto_model.encoder.layer.12.attention.output.dense.weight\n",
            " 204: True, auto_model.encoder.layer.12.attention.output.dense.bias\n",
            " 205: True, auto_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
            " 206: True, auto_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
            " 207: True, auto_model.encoder.layer.12.intermediate.dense.weight\n",
            " 208: True, auto_model.encoder.layer.12.intermediate.dense.bias\n",
            " 209: True, auto_model.encoder.layer.12.output.dense.weight\n",
            " 210: True, auto_model.encoder.layer.12.output.dense.bias\n",
            " 211: True, auto_model.encoder.layer.12.output.LayerNorm.weight\n",
            " 212: True, auto_model.encoder.layer.12.output.LayerNorm.bias\n",
            " 213: True, auto_model.encoder.layer.13.attention.self.query.weight\n",
            " 214: True, auto_model.encoder.layer.13.attention.self.query.bias\n",
            " 215: True, auto_model.encoder.layer.13.attention.self.key.weight\n",
            " 216: True, auto_model.encoder.layer.13.attention.self.key.bias\n",
            " 217: True, auto_model.encoder.layer.13.attention.self.value.weight\n",
            " 218: True, auto_model.encoder.layer.13.attention.self.value.bias\n",
            " 219: True, auto_model.encoder.layer.13.attention.output.dense.weight\n",
            " 220: True, auto_model.encoder.layer.13.attention.output.dense.bias\n",
            " 221: True, auto_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
            " 222: True, auto_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
            " 223: True, auto_model.encoder.layer.13.intermediate.dense.weight\n",
            " 224: True, auto_model.encoder.layer.13.intermediate.dense.bias\n",
            " 225: True, auto_model.encoder.layer.13.output.dense.weight\n",
            " 226: True, auto_model.encoder.layer.13.output.dense.bias\n",
            " 227: True, auto_model.encoder.layer.13.output.LayerNorm.weight\n",
            " 228: True, auto_model.encoder.layer.13.output.LayerNorm.bias\n",
            " 229: True, auto_model.encoder.layer.14.attention.self.query.weight\n",
            " 230: True, auto_model.encoder.layer.14.attention.self.query.bias\n",
            " 231: True, auto_model.encoder.layer.14.attention.self.key.weight\n",
            " 232: True, auto_model.encoder.layer.14.attention.self.key.bias\n",
            " 233: True, auto_model.encoder.layer.14.attention.self.value.weight\n",
            " 234: True, auto_model.encoder.layer.14.attention.self.value.bias\n",
            " 235: True, auto_model.encoder.layer.14.attention.output.dense.weight\n",
            " 236: True, auto_model.encoder.layer.14.attention.output.dense.bias\n",
            " 237: True, auto_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
            " 238: True, auto_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
            " 239: True, auto_model.encoder.layer.14.intermediate.dense.weight\n",
            " 240: True, auto_model.encoder.layer.14.intermediate.dense.bias\n",
            " 241: True, auto_model.encoder.layer.14.output.dense.weight\n",
            " 242: True, auto_model.encoder.layer.14.output.dense.bias\n",
            " 243: True, auto_model.encoder.layer.14.output.LayerNorm.weight\n",
            " 244: True, auto_model.encoder.layer.14.output.LayerNorm.bias\n",
            " 245: True, auto_model.encoder.layer.15.attention.self.query.weight\n",
            " 246: True, auto_model.encoder.layer.15.attention.self.query.bias\n",
            " 247: True, auto_model.encoder.layer.15.attention.self.key.weight\n",
            " 248: True, auto_model.encoder.layer.15.attention.self.key.bias\n",
            " 249: True, auto_model.encoder.layer.15.attention.self.value.weight\n",
            " 250: True, auto_model.encoder.layer.15.attention.self.value.bias\n",
            " 251: True, auto_model.encoder.layer.15.attention.output.dense.weight\n",
            " 252: True, auto_model.encoder.layer.15.attention.output.dense.bias\n",
            " 253: True, auto_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
            " 254: True, auto_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
            " 255: True, auto_model.encoder.layer.15.intermediate.dense.weight\n",
            " 256: True, auto_model.encoder.layer.15.intermediate.dense.bias\n",
            " 257: True, auto_model.encoder.layer.15.output.dense.weight\n",
            " 258: True, auto_model.encoder.layer.15.output.dense.bias\n",
            " 259: True, auto_model.encoder.layer.15.output.LayerNorm.weight\n",
            " 260: True, auto_model.encoder.layer.15.output.LayerNorm.bias\n",
            " 261: True, auto_model.encoder.layer.16.attention.self.query.weight\n",
            " 262: True, auto_model.encoder.layer.16.attention.self.query.bias\n",
            " 263: True, auto_model.encoder.layer.16.attention.self.key.weight\n",
            " 264: True, auto_model.encoder.layer.16.attention.self.key.bias\n",
            " 265: True, auto_model.encoder.layer.16.attention.self.value.weight\n",
            " 266: True, auto_model.encoder.layer.16.attention.self.value.bias\n",
            " 267: True, auto_model.encoder.layer.16.attention.output.dense.weight\n",
            " 268: True, auto_model.encoder.layer.16.attention.output.dense.bias\n",
            " 269: True, auto_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
            " 270: True, auto_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
            " 271: True, auto_model.encoder.layer.16.intermediate.dense.weight\n",
            " 272: True, auto_model.encoder.layer.16.intermediate.dense.bias\n",
            " 273: True, auto_model.encoder.layer.16.output.dense.weight\n",
            " 274: True, auto_model.encoder.layer.16.output.dense.bias\n",
            " 275: True, auto_model.encoder.layer.16.output.LayerNorm.weight\n",
            " 276: True, auto_model.encoder.layer.16.output.LayerNorm.bias\n",
            " 277: True, auto_model.encoder.layer.17.attention.self.query.weight\n",
            " 278: True, auto_model.encoder.layer.17.attention.self.query.bias\n",
            " 279: True, auto_model.encoder.layer.17.attention.self.key.weight\n",
            " 280: True, auto_model.encoder.layer.17.attention.self.key.bias\n",
            " 281: True, auto_model.encoder.layer.17.attention.self.value.weight\n",
            " 282: True, auto_model.encoder.layer.17.attention.self.value.bias\n",
            " 283: True, auto_model.encoder.layer.17.attention.output.dense.weight\n",
            " 284: True, auto_model.encoder.layer.17.attention.output.dense.bias\n",
            " 285: True, auto_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
            " 286: True, auto_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
            " 287: True, auto_model.encoder.layer.17.intermediate.dense.weight\n",
            " 288: True, auto_model.encoder.layer.17.intermediate.dense.bias\n",
            " 289: True, auto_model.encoder.layer.17.output.dense.weight\n",
            " 290: True, auto_model.encoder.layer.17.output.dense.bias\n",
            " 291: True, auto_model.encoder.layer.17.output.LayerNorm.weight\n",
            " 292: True, auto_model.encoder.layer.17.output.LayerNorm.bias\n",
            " 293: True, auto_model.encoder.layer.18.attention.self.query.weight\n",
            " 294: True, auto_model.encoder.layer.18.attention.self.query.bias\n",
            " 295: True, auto_model.encoder.layer.18.attention.self.key.weight\n",
            " 296: True, auto_model.encoder.layer.18.attention.self.key.bias\n",
            " 297: True, auto_model.encoder.layer.18.attention.self.value.weight\n",
            " 298: True, auto_model.encoder.layer.18.attention.self.value.bias\n",
            " 299: True, auto_model.encoder.layer.18.attention.output.dense.weight\n",
            " 300: True, auto_model.encoder.layer.18.attention.output.dense.bias\n",
            " 301: True, auto_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
            " 302: True, auto_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
            " 303: True, auto_model.encoder.layer.18.intermediate.dense.weight\n",
            " 304: True, auto_model.encoder.layer.18.intermediate.dense.bias\n",
            " 305: True, auto_model.encoder.layer.18.output.dense.weight\n",
            " 306: True, auto_model.encoder.layer.18.output.dense.bias\n",
            " 307: True, auto_model.encoder.layer.18.output.LayerNorm.weight\n",
            " 308: True, auto_model.encoder.layer.18.output.LayerNorm.bias\n",
            " 309: True, auto_model.encoder.layer.19.attention.self.query.weight\n",
            " 310: True, auto_model.encoder.layer.19.attention.self.query.bias\n",
            " 311: True, auto_model.encoder.layer.19.attention.self.key.weight\n",
            " 312: True, auto_model.encoder.layer.19.attention.self.key.bias\n",
            " 313: True, auto_model.encoder.layer.19.attention.self.value.weight\n",
            " 314: True, auto_model.encoder.layer.19.attention.self.value.bias\n",
            " 315: True, auto_model.encoder.layer.19.attention.output.dense.weight\n",
            " 316: True, auto_model.encoder.layer.19.attention.output.dense.bias\n",
            " 317: True, auto_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
            " 318: True, auto_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
            " 319: True, auto_model.encoder.layer.19.intermediate.dense.weight\n",
            " 320: True, auto_model.encoder.layer.19.intermediate.dense.bias\n",
            " 321: True, auto_model.encoder.layer.19.output.dense.weight\n",
            " 322: True, auto_model.encoder.layer.19.output.dense.bias\n",
            " 323: True, auto_model.encoder.layer.19.output.LayerNorm.weight\n",
            " 324: True, auto_model.encoder.layer.19.output.LayerNorm.bias\n",
            " 325: True, auto_model.encoder.layer.20.attention.self.query.weight\n",
            " 326: True, auto_model.encoder.layer.20.attention.self.query.bias\n",
            " 327: True, auto_model.encoder.layer.20.attention.self.key.weight\n",
            " 328: True, auto_model.encoder.layer.20.attention.self.key.bias\n",
            " 329: True, auto_model.encoder.layer.20.attention.self.value.weight\n",
            " 330: True, auto_model.encoder.layer.20.attention.self.value.bias\n",
            " 331: True, auto_model.encoder.layer.20.attention.output.dense.weight\n",
            " 332: True, auto_model.encoder.layer.20.attention.output.dense.bias\n",
            " 333: True, auto_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
            " 334: True, auto_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
            " 335: True, auto_model.encoder.layer.20.intermediate.dense.weight\n",
            " 336: True, auto_model.encoder.layer.20.intermediate.dense.bias\n",
            " 337: True, auto_model.encoder.layer.20.output.dense.weight\n",
            " 338: True, auto_model.encoder.layer.20.output.dense.bias\n",
            " 339: True, auto_model.encoder.layer.20.output.LayerNorm.weight\n",
            " 340: True, auto_model.encoder.layer.20.output.LayerNorm.bias\n",
            " 341: True, auto_model.encoder.layer.21.attention.self.query.weight\n",
            " 342: True, auto_model.encoder.layer.21.attention.self.query.bias\n",
            " 343: True, auto_model.encoder.layer.21.attention.self.key.weight\n",
            " 344: True, auto_model.encoder.layer.21.attention.self.key.bias\n",
            " 345: True, auto_model.encoder.layer.21.attention.self.value.weight\n",
            " 346: True, auto_model.encoder.layer.21.attention.self.value.bias\n",
            " 347: True, auto_model.encoder.layer.21.attention.output.dense.weight\n",
            " 348: True, auto_model.encoder.layer.21.attention.output.dense.bias\n",
            " 349: True, auto_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
            " 350: True, auto_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
            " 351: True, auto_model.encoder.layer.21.intermediate.dense.weight\n",
            " 352: True, auto_model.encoder.layer.21.intermediate.dense.bias\n",
            " 353: True, auto_model.encoder.layer.21.output.dense.weight\n",
            " 354: True, auto_model.encoder.layer.21.output.dense.bias\n",
            " 355: True, auto_model.encoder.layer.21.output.LayerNorm.weight\n",
            " 356: True, auto_model.encoder.layer.21.output.LayerNorm.bias\n",
            " 357: True, auto_model.encoder.layer.22.attention.self.query.weight\n",
            " 358: True, auto_model.encoder.layer.22.attention.self.query.bias\n",
            " 359: True, auto_model.encoder.layer.22.attention.self.key.weight\n",
            " 360: True, auto_model.encoder.layer.22.attention.self.key.bias\n",
            " 361: True, auto_model.encoder.layer.22.attention.self.value.weight\n",
            " 362: True, auto_model.encoder.layer.22.attention.self.value.bias\n",
            " 363: True, auto_model.encoder.layer.22.attention.output.dense.weight\n",
            " 364: True, auto_model.encoder.layer.22.attention.output.dense.bias\n",
            " 365: True, auto_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
            " 366: True, auto_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
            " 367: True, auto_model.encoder.layer.22.intermediate.dense.weight\n",
            " 368: True, auto_model.encoder.layer.22.intermediate.dense.bias\n",
            " 369: True, auto_model.encoder.layer.22.output.dense.weight\n",
            " 370: True, auto_model.encoder.layer.22.output.dense.bias\n",
            " 371: True, auto_model.encoder.layer.22.output.LayerNorm.weight\n",
            " 372: True, auto_model.encoder.layer.22.output.LayerNorm.bias\n",
            " 373: True, auto_model.encoder.layer.23.attention.self.query.weight\n",
            " 374: True, auto_model.encoder.layer.23.attention.self.query.bias\n",
            " 375: True, auto_model.encoder.layer.23.attention.self.key.weight\n",
            " 376: True, auto_model.encoder.layer.23.attention.self.key.bias\n",
            " 377: True, auto_model.encoder.layer.23.attention.self.value.weight\n",
            " 378: True, auto_model.encoder.layer.23.attention.self.value.bias\n",
            " 379: True, auto_model.encoder.layer.23.attention.output.dense.weight\n",
            " 380: True, auto_model.encoder.layer.23.attention.output.dense.bias\n",
            " 381: True, auto_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
            " 382: True, auto_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
            " 383: True, auto_model.encoder.layer.23.intermediate.dense.weight\n",
            " 384: True, auto_model.encoder.layer.23.intermediate.dense.bias\n",
            " 385: True, auto_model.encoder.layer.23.output.dense.weight\n",
            " 386: True, auto_model.encoder.layer.23.output.dense.bias\n",
            " 387: True, auto_model.encoder.layer.23.output.LayerNorm.weight\n",
            " 388: True, auto_model.encoder.layer.23.output.LayerNorm.bias\n",
            " 389: True, auto_model.pooler.dense.weight\n",
            " 390: True, auto_model.pooler.dense.bias\n",
            " 391: True, qa_outputs.weight\n",
            " 392: True, qa_outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6NSxPjSDAp0"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_stCiHpfE4Cj"
      },
      "source": [
        "def bert_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    if \"base\" in config.model_name or \"L-12\" in config.model_name:\n",
        "        bert_parameters = named_parameters[:197]    \n",
        "        regressor_parameters = named_parameters[199:]\n",
        "        second_block = 69\n",
        "        third_block = 133\n",
        "\n",
        "    elif \"large\" in config.model_name or \"L-24\" in config.model_name:\n",
        "        bert_parameters = named_parameters[:388]    \n",
        "        regressor_parameters = named_parameters[391:]\n",
        "        second_block = 133\n",
        "        third_block = 261\n",
        "        \n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(bert_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else config.weight_decay\n",
        "\n",
        "        if layer_num >= third_block:\n",
        "            lr = config.max_lr\n",
        "        elif layer_num >= second_block:\n",
        "            lr = config.lr\n",
        "        else:\n",
        "            lr = config.min_lr\n",
        "\n",
        "        parameters.append({\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr})\n",
        "\n",
        "    return T.AdamW(parameters, eps=1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENcidivxDVIj"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAGe7WFDWG4"
      },
      "source": [
        "def chaii_cross_entropy(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJr7FSz5KvLt"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXFVz_OVKu2O"
      },
      "source": [
        "def jaccard(row): \n",
        "    str1 = row[0]\n",
        "    str2 = row[1]\n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfqKjGpLaMMX"
      },
      "source": [
        "def get_result(result_df, fold=config.n_fold):\n",
        "    score = result_df[\"jaccard\"].mean()\n",
        "    LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "    if fold == config.n_fold:\n",
        "        wandb.log({\"CV\": score})\n",
        "    else:\n",
        "        wandb.log({f\"CV_fold{fold}\": score})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7aZ38xCMG__"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD2wDdHMMMSc"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrVjLs3H8DXV"
      },
      "source": [
        "def compute_grad_norm(parameters, norm_type=2.0):\n",
        "    \"\"\"Refer to torch.nn.utils.clip_grad_norm_\"\"\"\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    norm_type = float(norm_type)\n",
        "    total_norm = 0\n",
        "    for p in parameters:\n",
        "        param_norm = p.grad.data.norm(norm_type)\n",
        "        total_norm += param_norm.item() ** norm_type\n",
        "    total_norm = total_norm ** (1. / norm_type)\n",
        "    return total_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laoX2YvHMW40"
      },
      "source": [
        "def train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, features in enumerate(train_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        with amp.autocast(enabled=Config.amp):\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "            loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss = loss / config.gradient_accumulation_steps\n",
        "            \n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        else:\n",
        "            grad_norm = compute_grad_norm(model.parameters())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(train_loader) - 1):\n",
        "            print(\n",
        "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "                f\"Grad: {grad_norm:.4f} \"\n",
        "                f\"LR: {scheduler.get_lr()[0]:.6f} \"\n",
        "            )\n",
        "            # wandb.log({\n",
        "            #     \"step\": (epoch) * len(train_loader) + step,\n",
        "            #     f\"loss/fold{fold}\": losses.avg,\n",
        "            #     f\"grad/fold{fold}\": grad_norm,\n",
        "            #     f\"lr/fold{fold}\": scheduler.get_lr()[0],\n",
        "            # })\n",
        "\n",
        "    return losses.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4GZ8PcPpLt"
      },
      "source": [
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds_start = []\n",
        "    preds_end = []\n",
        "    start = time.time()\n",
        "\n",
        "    for step, features in enumerate(valid_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        preds_start.append(out_start.to(\"cpu\").numpy())\n",
        "        preds_end.append(out_end.to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(valid_loader) - 1):\n",
        "            print(\n",
        "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "            )\n",
        "\n",
        "    predictions_start = np.concatenate(preds_start)\n",
        "    predictions_end = np.concatenate(preds_end)\n",
        "    return losses.avg, predictions_start, predictions_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aS_0cqjWy5P"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5pkRxmW0z_"
      },
      "source": [
        "def postprocess_qa_predictions(examples, features, tokenizer, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    \n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    for example_index, example in examples.iterrows():\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "\n",
        "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
        "            context_index = 1\n",
        "\n",
        "            features[feature_index][\"offset_mapping\"] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
        "            ]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUqyC9I8xyd9"
      },
      "source": [
        "# https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765\n",
        "def postpurocess_by_nbroad(preds_df):\n",
        "    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n",
        "    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n",
        "\n",
        "    cleaned_preds = []\n",
        "    for pred, context in preds_df[[\"prediction\", \"context\"]].to_numpy():\n",
        "        if pred == \"\":\n",
        "            cleaned_preds.append(pred)\n",
        "            continue\n",
        "        while any([pred.startswith(y) for y in bad_starts]):\n",
        "            pred = pred[1:]\n",
        "        while any([pred.endswith(y) for y in bad_endings]):\n",
        "            if pred.endswith(\"...\"):\n",
        "                pred = pred[:-3]\n",
        "            else:\n",
        "                pred = pred[:-1]\n",
        "\n",
        "        cleaned_preds.append(pred)\n",
        "\n",
        "    preds_df[\"prediction\"] = cleaned_preds\n",
        "\n",
        "    return preds_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vwcRHThRbcm"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmu1ZdXRdA7"
      },
      "source": [
        "def train_loop(df, fold):\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # Data Loader\n",
        "    # ====================================================\n",
        "    trn_idx = df[df[\"fold\"] != fold].index\n",
        "    val_idx = df[df[\"fold\"] == fold].index\n",
        "\n",
        "    train_folds = df.loc[trn_idx].reset_index(drop=True)\n",
        "    valid_folds = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = BaseDataset(train_folds, config.model_name)\n",
        "    valid_dataset = BaseDataset(valid_folds, config.model_name)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # ====================================================\n",
        "    # Optimizer\n",
        "    # ====================================================\n",
        "    def get_optimizer(model):\n",
        "        if config.optimizer == \"Adam\":\n",
        "            optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"AdamW\":\n",
        "            optimizer = T.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"BertAdamW\":\n",
        "            optimizer = bert_optimizer(model)\n",
        "        return optimizer\n",
        "\n",
        "    # ====================================================\n",
        "    # Scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(optimizer):\n",
        "        # num_data = len(train_folds)\n",
        "        num_data = len(train_dataset)\n",
        "        num_steps = num_data // (config.batch_size * config.gradient_accumulation_steps) * config.epochs\n",
        "\n",
        "        if config.scheduler == \"CosineAnnealingWarmRestarts\":\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=num_steps, T_mult=1, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingLR\":\n",
        "            scheduler = CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingWarmupRestarts\":\n",
        "            scheduler = CosineAnnealingWarmupRestarts(\n",
        "                optimizer, first_cycle_steps=num_steps, max_lr=config.lr, min_lr=config.min_lr, warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        elif config.scheduler == \"get_cosine_schedule_with_warmup\":\n",
        "            scheduler = T.get_cosine_schedule_with_warmup(\n",
        "                optimizer, num_training_steps=num_steps, num_warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        return scheduler\n",
        "\n",
        "    # ====================================================\n",
        "    # Model\n",
        "    # ====================================================\n",
        "    model = BaseModel(config.model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = get_optimizer(model)\n",
        "    scaler = amp.GradScaler(enabled=Config.amp)\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "\n",
        "    # ====================================================\n",
        "    # Criterion\n",
        "    # ====================================================\n",
        "    def get_criterion():\n",
        "        if config.criterion == \"CrossEntropyLoss\":\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "        elif config.criterion == \"BCEWithLogitsLoss\":\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "        elif config.criterion == \"MSELoss\":\n",
        "            criterion = nn.MSELoss()\n",
        "        elif config.criterion == \"ChaiiCrossEntropyLoss\":\n",
        "            criterion = chaii_cross_entropy\n",
        "        return criterion\n",
        "\n",
        "    criterion = get_criterion()\n",
        "\n",
        "    # ====================================================\n",
        "    # Loop\n",
        "    # ====================================================\n",
        "    best_score = -1\n",
        "    best_loss = np.inf\n",
        "    best_preds = None\n",
        "\n",
        "    wandb.watch(model, log_freq=Config.print_freq)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, preds_start, preds_end = valid_fn(valid_loader, model, criterion, device)\n",
        "\n",
        "        # postprocess 1\n",
        "        predictions = postprocess_qa_predictions(\n",
        "            valid_folds, valid_dataset.features, valid_dataset.tokenizer, (preds_start, preds_end)\n",
        "        )\n",
        "\n",
        "        oof_df = valid_folds[[\"id\", \"answer_text\"]]\n",
        "        oof_df[\"prediction\"] = oof_df['id'].apply(lambda r: predictions[r])\n",
        "\n",
        "        # postprocess 2\n",
        "        oof_df = postpurocess_by_nbroad(oof_df)\n",
        "\n",
        "        # scoring\n",
        "        oof_df['jaccard'] = oof_df[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
        "        score = oof_df[\"jaccard\"].mean()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {elapsed:.0f}s\")\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            f\"val_loss/fold{fold}\": avg_val_loss,\n",
        "            f\"score/fold{fold}\": score,\n",
        "        })\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_score = score\n",
        "            best_loss = avg_val_loss\n",
        "            best_preds = predictions\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Model. score: {best_score:.4f}, loss: {best_loss:.4f}\")\n",
        "\n",
        "            model_subdir = MODEL_DIR + f\"fold{fold}/\"\n",
        "            os.makedirs(model_subdir, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{model_subdir}/pytorch_model.bin\")\n",
        "            with open(f'{model_subdir}/preds.json', 'w') as f:\n",
        "                f.write(json.dumps(predictions, sort_keys=True, indent=4, ensure_ascii=False))\n",
        "            model.auto_config.save_pretrained(model_subdir)\n",
        "            train_dataset.tokenizer.save_pretrained(model_subdir)\n",
        "\n",
        "    valid_folds[\"prediction\"] = valid_folds['id'].apply(lambda r: best_preds[r])\n",
        "    valid_folds['jaccard'] = valid_folds[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
        "\n",
        "    return valid_folds, best_score, best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znc9U4s9YPqs"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPgK02eYRCX"
      },
      "source": [
        "def main():\n",
        "    # ====================================================\n",
        "    # Training\n",
        "    # ====================================================\n",
        "    if Config.train:\n",
        "        oof_df = pd.DataFrame()\n",
        "        oof_result = []\n",
        "        for fold in range(config.n_fold):\n",
        "            seed_torch(seed + fold)\n",
        "\n",
        "            _oof_df, score, loss = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            oof_result.append([fold, score, loss])\n",
        "\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df, fold)\n",
        "\n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        loss = statistics.mean([d[2] for d in oof_result])\n",
        "        wandb.log({\"loss\": loss})\n",
        "\n",
        "        table = wandb.Table(data=oof_result, columns = [\"fold\", \"score\", \"loss\"])\n",
        "        run.log({\"Fold Result\": table})\n",
        "        \n",
        "        # save result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "        wandb.save(OUTPUT_DIR + \"oof_df.csv\")\n",
        "\n",
        "        artifact = wandb.Artifact('base-models', type='model')\n",
        "        artifact.add_dir(MODEL_DIR)\n",
        "        run.log_artifact(artifact)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Q3YuoeYiLS",
        "outputId": "d1c160f8-00b3-416e-96fc-41c51e19f592"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "========== fold: 0 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/5982] Elapsed 0m 1s (remain 122m 12s) Loss: 6.3085 Grad: 7.0538 LR: 0.000000 \n",
            "Epoch: [1][100/5982] Elapsed 0m 58s (remain 56m 31s) Loss: 5.2013 Grad: 9.9451 LR: 0.000084 \n",
            "Epoch: [1][200/5982] Elapsed 1m 55s (remain 55m 10s) Loss: 3.4426 Grad: 77.9482 LR: 0.000167 \n",
            "Epoch: [1][300/5982] Elapsed 2m 51s (remain 54m 4s) Loss: 2.7173 Grad: 42.7391 LR: 0.000251 \n",
            "Epoch: [1][400/5982] Elapsed 3m 48s (remain 53m 3s) Loss: 2.2595 Grad: 10.9319 LR: 0.000334 \n",
            "Epoch: [1][500/5982] Elapsed 4m 45s (remain 52m 2s) Loss: 2.0066 Grad: 25.1315 LR: 0.000418 \n",
            "Epoch: [1][600/5982] Elapsed 5m 42s (remain 51m 4s) Loss: 1.8266 Grad: 0.7310 LR: 0.000502 \n",
            "Epoch: [1][700/5982] Elapsed 6m 38s (remain 50m 5s) Loss: 1.7045 Grad: 12.7254 LR: 0.000585 \n",
            "Epoch: [1][800/5982] Elapsed 7m 35s (remain 49m 7s) Loss: 1.6093 Grad: 18.4767 LR: 0.000669 \n",
            "Epoch: [1][900/5982] Elapsed 8m 32s (remain 48m 10s) Loss: 1.5377 Grad: 13.5373 LR: 0.000753 \n",
            "Epoch: [1][1000/5982] Elapsed 9m 29s (remain 47m 13s) Loss: 1.4853 Grad: 43.8679 LR: 0.000836 \n",
            "Epoch: [1][1100/5982] Elapsed 10m 26s (remain 46m 15s) Loss: 1.4381 Grad: 4.1300 LR: 0.000920 \n",
            "Epoch: [1][1200/5982] Elapsed 11m 22s (remain 45m 18s) Loss: 1.4117 Grad: 12.0335 LR: 0.001000 \n",
            "Epoch: [1][1300/5982] Elapsed 12m 19s (remain 44m 21s) Loss: 1.3824 Grad: 2.3322 LR: 0.001000 \n",
            "Epoch: [1][1400/5982] Elapsed 13m 16s (remain 43m 24s) Loss: 1.3652 Grad: 2.1372 LR: 0.000999 \n",
            "Epoch: [1][1500/5982] Elapsed 14m 13s (remain 42m 27s) Loss: 1.3500 Grad: 19.7360 LR: 0.000998 \n",
            "Epoch: [1][1600/5982] Elapsed 15m 10s (remain 41m 30s) Loss: 1.3346 Grad: 16.5818 LR: 0.000997 \n",
            "Epoch: [1][1700/5982] Elapsed 16m 6s (remain 40m 33s) Loss: 1.3105 Grad: 14.7102 LR: 0.000995 \n",
            "Epoch: [1][1800/5982] Elapsed 17m 3s (remain 39m 36s) Loss: 1.2871 Grad: 8.0279 LR: 0.000992 \n",
            "Epoch: [1][1900/5982] Elapsed 18m 0s (remain 38m 39s) Loss: 1.2685 Grad: 26.1092 LR: 0.000989 \n",
            "Epoch: [1][2000/5982] Elapsed 18m 57s (remain 37m 42s) Loss: 1.2517 Grad: 13.0269 LR: 0.000986 \n",
            "Epoch: [1][2100/5982] Elapsed 19m 53s (remain 36m 45s) Loss: 1.2370 Grad: 5.0123 LR: 0.000983 \n",
            "Epoch: [1][2200/5982] Elapsed 20m 50s (remain 35m 48s) Loss: 1.2266 Grad: 6.4401 LR: 0.000979 \n",
            "Epoch: [1][2300/5982] Elapsed 21m 47s (remain 34m 51s) Loss: 1.2070 Grad: 9.5895 LR: 0.000974 \n",
            "Epoch: [1][2400/5982] Elapsed 22m 44s (remain 33m 54s) Loss: 1.1968 Grad: 5.8976 LR: 0.000969 \n",
            "Epoch: [1][2500/5982] Elapsed 23m 41s (remain 32m 57s) Loss: 1.1897 Grad: 8.3207 LR: 0.000964 \n",
            "Epoch: [1][2600/5982] Elapsed 24m 37s (remain 32m 1s) Loss: 1.1770 Grad: 10.2514 LR: 0.000959 \n",
            "Epoch: [1][2700/5982] Elapsed 25m 34s (remain 31m 4s) Loss: 1.1662 Grad: 8.0950 LR: 0.000953 \n",
            "Epoch: [1][2800/5982] Elapsed 26m 31s (remain 30m 7s) Loss: 1.1554 Grad: 5.0446 LR: 0.000946 \n",
            "Epoch: [1][2900/5982] Elapsed 27m 28s (remain 29m 10s) Loss: 1.1466 Grad: 16.5069 LR: 0.000939 \n",
            "Epoch: [1][3000/5982] Elapsed 28m 25s (remain 28m 13s) Loss: 1.1371 Grad: 5.3356 LR: 0.000932 \n",
            "Epoch: [1][3100/5982] Elapsed 29m 21s (remain 27m 16s) Loss: 1.1318 Grad: 13.8175 LR: 0.000925 \n",
            "Epoch: [1][3200/5982] Elapsed 30m 18s (remain 26m 20s) Loss: 1.1250 Grad: 0.8968 LR: 0.000917 \n",
            "Epoch: [1][3300/5982] Elapsed 31m 15s (remain 25m 23s) Loss: 1.1183 Grad: 1.6857 LR: 0.000909 \n",
            "Epoch: [1][3400/5982] Elapsed 32m 12s (remain 24m 26s) Loss: 1.1119 Grad: 0.1462 LR: 0.000900 \n",
            "Epoch: [1][3500/5982] Elapsed 33m 9s (remain 23m 29s) Loss: 1.1074 Grad: 12.6894 LR: 0.000891 \n",
            "Epoch: [1][3600/5982] Elapsed 34m 6s (remain 22m 32s) Loss: 1.1020 Grad: 6.1812 LR: 0.000882 \n",
            "Epoch: [1][3700/5982] Elapsed 35m 2s (remain 21m 36s) Loss: 1.0937 Grad: 14.7386 LR: 0.000872 \n",
            "Epoch: [1][3800/5982] Elapsed 35m 59s (remain 20m 39s) Loss: 1.0884 Grad: 18.5887 LR: 0.000862 \n",
            "Epoch: [1][3900/5982] Elapsed 36m 56s (remain 19m 42s) Loss: 1.0806 Grad: 4.8369 LR: 0.000852 \n",
            "Epoch: [1][4000/5982] Elapsed 37m 53s (remain 18m 45s) Loss: 1.0736 Grad: 21.2770 LR: 0.000842 \n",
            "Epoch: [1][4100/5982] Elapsed 38m 50s (remain 17m 48s) Loss: 1.0648 Grad: 3.8672 LR: 0.000831 \n",
            "Epoch: [1][4200/5982] Elapsed 39m 46s (remain 16m 51s) Loss: 1.0595 Grad: 5.3544 LR: 0.000820 \n",
            "Epoch: [1][4300/5982] Elapsed 40m 43s (remain 15m 55s) Loss: 1.0569 Grad: 14.8832 LR: 0.000808 \n",
            "Epoch: [1][4400/5982] Elapsed 41m 40s (remain 14m 58s) Loss: 1.0538 Grad: 2.5597 LR: 0.000797 \n",
            "Epoch: [1][4500/5982] Elapsed 42m 37s (remain 14m 1s) Loss: 1.0491 Grad: 14.4122 LR: 0.000785 \n",
            "Epoch: [1][4600/5982] Elapsed 43m 34s (remain 13m 4s) Loss: 1.0432 Grad: 14.4542 LR: 0.000773 \n",
            "Epoch: [1][4700/5982] Elapsed 44m 30s (remain 12m 7s) Loss: 1.0398 Grad: 15.5477 LR: 0.000761 \n",
            "Epoch: [1][4800/5982] Elapsed 45m 27s (remain 11m 10s) Loss: 1.0364 Grad: 8.7642 LR: 0.000748 \n",
            "Epoch: [1][4900/5982] Elapsed 46m 24s (remain 10m 14s) Loss: 1.0326 Grad: 0.0119 LR: 0.000735 \n",
            "Epoch: [1][5000/5982] Elapsed 47m 21s (remain 9m 17s) Loss: 1.0289 Grad: 19.3933 LR: 0.000722 \n",
            "Epoch: [1][5100/5982] Elapsed 48m 17s (remain 8m 20s) Loss: 1.0252 Grad: 4.0785 LR: 0.000709 \n",
            "Epoch: [1][5200/5982] Elapsed 49m 14s (remain 7m 23s) Loss: 1.0225 Grad: 0.2144 LR: 0.000696 \n",
            "Epoch: [1][5300/5982] Elapsed 50m 11s (remain 6m 26s) Loss: 1.0213 Grad: 4.3122 LR: 0.000682 \n",
            "Epoch: [1][5400/5982] Elapsed 51m 8s (remain 5m 30s) Loss: 1.0183 Grad: 3.9903 LR: 0.000669 \n",
            "Epoch: [1][5500/5982] Elapsed 52m 5s (remain 4m 33s) Loss: 1.0136 Grad: 29.7575 LR: 0.000655 \n",
            "Epoch: [1][5600/5982] Elapsed 53m 2s (remain 3m 36s) Loss: 1.0092 Grad: 23.7369 LR: 0.000641 \n",
            "Epoch: [1][5700/5982] Elapsed 53m 58s (remain 2m 39s) Loss: 1.0059 Grad: 12.0134 LR: 0.000627 \n",
            "Epoch: [1][5800/5982] Elapsed 54m 55s (remain 1m 42s) Loss: 1.0048 Grad: 15.0554 LR: 0.000613 \n",
            "Epoch: [1][5900/5982] Elapsed 55m 52s (remain 0m 46s) Loss: 1.0035 Grad: 0.5855 LR: 0.000598 \n",
            "Epoch: [1][5981/5982] Elapsed 56m 36s (remain 0m 0s) Loss: 1.0009 Grad: 6.8784 LR: 0.000587 \n",
            "EVAL: [0/738] Elapsed 0m 0s (remain 9m 3s) Loss: 0.3492 \n",
            "EVAL: [100/738] Elapsed 0m 17s (remain 1m 53s) Loss: 0.2952 \n",
            "EVAL: [200/738] Elapsed 0m 35s (remain 1m 33s) Loss: 0.2862 \n",
            "EVAL: [300/738] Elapsed 0m 52s (remain 1m 15s) Loss: 0.2614 \n",
            "EVAL: [400/738] Elapsed 1m 9s (remain 0m 58s) Loss: 0.2756 \n",
            "EVAL: [500/738] Elapsed 1m 26s (remain 0m 41s) Loss: 0.2648 \n",
            "EVAL: [600/738] Elapsed 1m 43s (remain 0m 23s) Loss: 0.2704 \n",
            "EVAL: [700/738] Elapsed 2m 1s (remain 0m 6s) Loss: 0.2711 \n",
            "EVAL: [737/738] Elapsed 2m 7s (remain 0m 0s) Loss: 0.2693 \n",
            "Post-processing 223 example predictions split into 2952 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6473467862481316, Train Loss: 1.0009, Val Loss: 0.2693, Time: 3526s\n",
            "Epoch 1 - Save Best Model. score: 0.6473, loss: 0.2693\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/5982] Elapsed 0m 1s (remain 111m 39s) Loss: 0.2959 Grad: 5.6314 LR: 0.000587 \n",
            "Epoch: [2][100/5982] Elapsed 0m 57s (remain 56m 12s) Loss: 0.5333 Grad: 13.6227 LR: 0.000572 \n",
            "Epoch: [2][200/5982] Elapsed 1m 56s (remain 55m 53s) Loss: 0.5271 Grad: 31.5247 LR: 0.000558 \n",
            "Epoch: [2][300/5982] Elapsed 2m 53s (remain 54m 32s) Loss: 0.5174 Grad: 1.1121 LR: 0.000543 \n",
            "Epoch: [2][400/5982] Elapsed 3m 50s (remain 53m 23s) Loss: 0.4947 Grad: 11.2409 LR: 0.000529 \n",
            "Epoch: [2][500/5982] Elapsed 4m 46s (remain 52m 19s) Loss: 0.4763 Grad: 1.0905 LR: 0.000514 \n",
            "Epoch: [2][600/5982] Elapsed 5m 43s (remain 51m 17s) Loss: 0.4774 Grad: 11.0586 LR: 0.000500 \n",
            "Epoch: [2][700/5982] Elapsed 6m 40s (remain 50m 17s) Loss: 0.4578 Grad: 1.5215 LR: 0.000485 \n",
            "Epoch: [2][800/5982] Elapsed 7m 37s (remain 49m 18s) Loss: 0.4618 Grad: 0.1316 LR: 0.000471 \n",
            "Epoch: [2][900/5982] Elapsed 8m 34s (remain 48m 19s) Loss: 0.4580 Grad: 31.7452 LR: 0.000456 \n",
            "Epoch: [2][1000/5982] Elapsed 9m 30s (remain 47m 20s) Loss: 0.4543 Grad: 1.7621 LR: 0.000441 \n",
            "Epoch: [2][1100/5982] Elapsed 10m 27s (remain 46m 22s) Loss: 0.4569 Grad: 0.4262 LR: 0.000427 \n",
            "Epoch: [2][1200/5982] Elapsed 11m 24s (remain 45m 24s) Loss: 0.4453 Grad: 1.1155 LR: 0.000413 \n",
            "Epoch: [2][1300/5982] Elapsed 12m 21s (remain 44m 27s) Loss: 0.4405 Grad: 10.1253 LR: 0.000398 \n",
            "Epoch: [2][1400/5982] Elapsed 13m 18s (remain 43m 29s) Loss: 0.4384 Grad: 26.2791 LR: 0.000384 \n",
            "Epoch: [2][1500/5982] Elapsed 14m 14s (remain 42m 32s) Loss: 0.4383 Grad: 0.3495 LR: 0.000370 \n",
            "Epoch: [2][1600/5982] Elapsed 15m 11s (remain 41m 34s) Loss: 0.4418 Grad: 5.6017 LR: 0.000356 \n",
            "Epoch: [2][1700/5982] Elapsed 16m 8s (remain 40m 37s) Loss: 0.4393 Grad: 0.1026 LR: 0.000342 \n",
            "Epoch: [2][1800/5982] Elapsed 17m 5s (remain 39m 40s) Loss: 0.4349 Grad: 8.8325 LR: 0.000328 \n",
            "Epoch: [2][1900/5982] Elapsed 18m 2s (remain 38m 42s) Loss: 0.4306 Grad: 0.9700 LR: 0.000315 \n",
            "Epoch: [2][2000/5982] Elapsed 18m 58s (remain 37m 45s) Loss: 0.4332 Grad: 2.0007 LR: 0.000301 \n",
            "Epoch: [2][2100/5982] Elapsed 19m 55s (remain 36m 48s) Loss: 0.4340 Grad: 0.8051 LR: 0.000288 \n",
            "Epoch: [2][2200/5982] Elapsed 20m 52s (remain 35m 51s) Loss: 0.4320 Grad: 8.1454 LR: 0.000275 \n",
            "Epoch: [2][2300/5982] Elapsed 21m 49s (remain 34m 54s) Loss: 0.4335 Grad: 0.1370 LR: 0.000262 \n",
            "Epoch: [2][2400/5982] Elapsed 22m 46s (remain 33m 57s) Loss: 0.4295 Grad: 33.7990 LR: 0.000249 \n",
            "Epoch: [2][2500/5982] Elapsed 23m 42s (remain 33m 0s) Loss: 0.4247 Grad: 4.3847 LR: 0.000236 \n",
            "Epoch: [2][2600/5982] Elapsed 24m 39s (remain 32m 3s) Loss: 0.4220 Grad: 3.7608 LR: 0.000224 \n",
            "Epoch: [2][2700/5982] Elapsed 25m 36s (remain 31m 6s) Loss: 0.4190 Grad: 21.6257 LR: 0.000212 \n",
            "Epoch: [2][2800/5982] Elapsed 26m 33s (remain 30m 9s) Loss: 0.4160 Grad: 9.2194 LR: 0.000200 \n",
            "Epoch: [2][2900/5982] Elapsed 27m 30s (remain 29m 12s) Loss: 0.4152 Grad: 2.0008 LR: 0.000189 \n",
            "Epoch: [2][3000/5982] Elapsed 28m 26s (remain 28m 15s) Loss: 0.4141 Grad: 45.9692 LR: 0.000177 \n",
            "Epoch: [2][3100/5982] Elapsed 29m 23s (remain 27m 18s) Loss: 0.4144 Grad: 0.3793 LR: 0.000166 \n",
            "Epoch: [2][3200/5982] Elapsed 30m 20s (remain 26m 21s) Loss: 0.4129 Grad: 7.7379 LR: 0.000156 \n",
            "Epoch: [2][3300/5982] Elapsed 31m 17s (remain 25m 24s) Loss: 0.4137 Grad: 16.4970 LR: 0.000145 \n",
            "Epoch: [2][3400/5982] Elapsed 32m 14s (remain 24m 27s) Loss: 0.4113 Grad: 3.7552 LR: 0.000135 \n",
            "Epoch: [2][3500/5982] Elapsed 33m 10s (remain 23m 30s) Loss: 0.4115 Grad: 22.7805 LR: 0.000125 \n",
            "Epoch: [2][3600/5982] Elapsed 34m 7s (remain 22m 33s) Loss: 0.4090 Grad: 23.4113 LR: 0.000116 \n",
            "Epoch: [2][3700/5982] Elapsed 35m 4s (remain 21m 37s) Loss: 0.4074 Grad: 36.2476 LR: 0.000107 \n",
            "Epoch: [2][3800/5982] Elapsed 36m 1s (remain 20m 40s) Loss: 0.4080 Grad: 23.4241 LR: 0.000098 \n",
            "Epoch: [2][3900/5982] Elapsed 36m 57s (remain 19m 43s) Loss: 0.4077 Grad: 3.2426 LR: 0.000089 \n",
            "Epoch: [2][4000/5982] Elapsed 37m 54s (remain 18m 46s) Loss: 0.4079 Grad: 1.5957 LR: 0.000081 \n",
            "Epoch: [2][4100/5982] Elapsed 38m 51s (remain 17m 49s) Loss: 0.4082 Grad: 1.1322 LR: 0.000073 \n",
            "Epoch: [2][4200/5982] Elapsed 39m 48s (remain 16m 52s) Loss: 0.4069 Grad: 2.3879 LR: 0.000066 \n",
            "Epoch: [2][4300/5982] Elapsed 40m 45s (remain 15m 55s) Loss: 0.4055 Grad: 13.8576 LR: 0.000059 \n",
            "Epoch: [2][4400/5982] Elapsed 41m 41s (remain 14m 58s) Loss: 0.4069 Grad: 14.7790 LR: 0.000052 \n",
            "Epoch: [2][4500/5982] Elapsed 42m 38s (remain 14m 1s) Loss: 0.4063 Grad: 3.7884 LR: 0.000046 \n",
            "Epoch: [2][4600/5982] Elapsed 43m 35s (remain 13m 5s) Loss: 0.4040 Grad: 0.7065 LR: 0.000040 \n",
            "Epoch: [2][4700/5982] Elapsed 44m 32s (remain 12m 8s) Loss: 0.4033 Grad: 44.4912 LR: 0.000034 \n",
            "Epoch: [2][4800/5982] Elapsed 45m 28s (remain 11m 11s) Loss: 0.4036 Grad: 7.5894 LR: 0.000029 \n",
            "Epoch: [2][4900/5982] Elapsed 46m 25s (remain 10m 14s) Loss: 0.4018 Grad: 2.1991 LR: 0.000025 \n",
            "Epoch: [2][5000/5982] Elapsed 47m 22s (remain 9m 17s) Loss: 0.4009 Grad: 30.9040 LR: 0.000020 \n",
            "Epoch: [2][5100/5982] Elapsed 48m 19s (remain 8m 20s) Loss: 0.3986 Grad: 4.3422 LR: 0.000016 \n",
            "Epoch: [2][5200/5982] Elapsed 49m 16s (remain 7m 23s) Loss: 0.3974 Grad: 8.7893 LR: 0.000013 \n",
            "Epoch: [2][5300/5982] Elapsed 50m 12s (remain 6m 27s) Loss: 0.3974 Grad: 6.1644 LR: 0.000010 \n",
            "Epoch: [2][5400/5982] Elapsed 51m 9s (remain 5m 30s) Loss: 0.3995 Grad: 0.2643 LR: 0.000007 \n",
            "Epoch: [2][5500/5982] Elapsed 52m 6s (remain 4m 33s) Loss: 0.3996 Grad: 0.2398 LR: 0.000005 \n",
            "Epoch: [2][5600/5982] Elapsed 53m 3s (remain 3m 36s) Loss: 0.4004 Grad: 4.8276 LR: 0.000003 \n",
            "Epoch: [2][5700/5982] Elapsed 54m 0s (remain 2m 39s) Loss: 0.4002 Grad: 20.7617 LR: 0.000002 \n",
            "Epoch: [2][5800/5982] Elapsed 54m 56s (remain 1m 42s) Loss: 0.3996 Grad: 0.2668 LR: 0.000001 \n",
            "Epoch: [2][5900/5982] Elapsed 55m 53s (remain 0m 46s) Loss: 0.3990 Grad: 0.8896 LR: 0.000000 \n",
            "Epoch: [2][5981/5982] Elapsed 56m 40s (remain 0m 0s) Loss: 0.4002 Grad: 30.7633 LR: 0.000000 \n",
            "EVAL: [0/738] Elapsed 0m 0s (remain 9m 0s) Loss: 0.0175 \n",
            "EVAL: [100/738] Elapsed 0m 17s (remain 1m 53s) Loss: 0.3222 \n",
            "EVAL: [200/738] Elapsed 0m 35s (remain 1m 33s) Loss: 0.2837 \n",
            "EVAL: [300/738] Elapsed 0m 52s (remain 1m 15s) Loss: 0.2571 \n",
            "EVAL: [400/738] Elapsed 1m 9s (remain 0m 58s) Loss: 0.2666 \n",
            "EVAL: [500/738] Elapsed 1m 26s (remain 0m 41s) Loss: 0.2651 \n",
            "EVAL: [600/738] Elapsed 1m 43s (remain 0m 23s) Loss: 0.2728 \n",
            "EVAL: [700/738] Elapsed 2m 1s (remain 0m 6s) Loss: 0.2791 \n",
            "EVAL: [737/738] Elapsed 2m 7s (remain 0m 0s) Loss: 0.2812 \n",
            "Post-processing 223 example predictions split into 2952 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.6558420528151468, Train Loss: 0.4002, Val Loss: 0.2812, Time: 3529s\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.64735\n",
            "========== fold: 1 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/5994] Elapsed 0m 1s (remain 113m 38s) Loss: 6.4718 Grad: 8.0335 LR: 0.000000 \n",
            "Epoch: [1][100/5994] Elapsed 0m 58s (remain 56m 35s) Loss: 5.1531 Grad: 10.5120 LR: 0.000084 \n",
            "Epoch: [1][200/5994] Elapsed 1m 54s (remain 55m 12s) Loss: 3.4795 Grad: 75.2828 LR: 0.000167 \n",
            "Epoch: [1][300/5994] Elapsed 2m 51s (remain 54m 7s) Loss: 2.7288 Grad: 7.8279 LR: 0.000251 \n",
            "Epoch: [1][400/5994] Elapsed 3m 48s (remain 53m 5s) Loss: 2.2909 Grad: 20.6639 LR: 0.000334 \n",
            "Epoch: [1][500/5994] Elapsed 4m 45s (remain 52m 6s) Loss: 2.0390 Grad: 20.2688 LR: 0.000418 \n",
            "Epoch: [1][600/5994] Elapsed 5m 41s (remain 51m 8s) Loss: 1.8413 Grad: 20.8684 LR: 0.000502 \n",
            "Epoch: [1][700/5994] Elapsed 6m 38s (remain 50m 10s) Loss: 1.7185 Grad: 10.9610 LR: 0.000585 \n",
            "Epoch: [1][800/5994] Elapsed 7m 35s (remain 49m 12s) Loss: 1.6352 Grad: 25.9792 LR: 0.000669 \n",
            "Epoch: [1][900/5994] Elapsed 8m 32s (remain 48m 14s) Loss: 1.5408 Grad: 9.2528 LR: 0.000753 \n",
            "Epoch: [1][1000/5994] Elapsed 9m 28s (remain 47m 17s) Loss: 1.4852 Grad: 0.8724 LR: 0.000836 \n",
            "Epoch: [1][1100/5994] Elapsed 10m 25s (remain 46m 20s) Loss: 1.4436 Grad: 11.4843 LR: 0.000920 \n",
            "Epoch: [1][1200/5994] Elapsed 11m 22s (remain 45m 23s) Loss: 1.3982 Grad: 14.8196 LR: 0.001000 \n",
            "Epoch: [1][1300/5994] Elapsed 12m 19s (remain 44m 26s) Loss: 1.3715 Grad: 4.8155 LR: 0.001000 \n",
            "Epoch: [1][1400/5994] Elapsed 13m 15s (remain 43m 29s) Loss: 1.3509 Grad: 17.0817 LR: 0.000999 \n",
            "Epoch: [1][1500/5994] Elapsed 14m 12s (remain 42m 32s) Loss: 1.3166 Grad: 43.5538 LR: 0.000998 \n",
            "Epoch: [1][1600/5994] Elapsed 15m 9s (remain 41m 35s) Loss: 1.3034 Grad: 24.5082 LR: 0.000997 \n",
            "Epoch: [1][1700/5994] Elapsed 16m 6s (remain 40m 38s) Loss: 1.2807 Grad: 14.3048 LR: 0.000995 \n",
            "Epoch: [1][1800/5994] Elapsed 17m 2s (remain 39m 41s) Loss: 1.2725 Grad: 13.0727 LR: 0.000992 \n",
            "Epoch: [1][1900/5994] Elapsed 17m 59s (remain 38m 44s) Loss: 1.2522 Grad: 8.2802 LR: 0.000990 \n",
            "Epoch: [1][2000/5994] Elapsed 18m 56s (remain 37m 47s) Loss: 1.2392 Grad: 18.9944 LR: 0.000986 \n",
            "Epoch: [1][2100/5994] Elapsed 19m 53s (remain 36m 50s) Loss: 1.2212 Grad: 5.5207 LR: 0.000983 \n",
            "Epoch: [1][2200/5994] Elapsed 20m 49s (remain 35m 54s) Loss: 1.2017 Grad: 8.0607 LR: 0.000979 \n",
            "Epoch: [1][2300/5994] Elapsed 21m 46s (remain 34m 57s) Loss: 1.1950 Grad: 22.2997 LR: 0.000974 \n",
            "Epoch: [1][2400/5994] Elapsed 22m 43s (remain 34m 0s) Loss: 1.1864 Grad: 13.0737 LR: 0.000970 \n",
            "Epoch: [1][2500/5994] Elapsed 23m 40s (remain 33m 3s) Loss: 1.1786 Grad: 9.9294 LR: 0.000964 \n",
            "Epoch: [1][2600/5994] Elapsed 24m 37s (remain 32m 6s) Loss: 1.1700 Grad: 3.2868 LR: 0.000959 \n",
            "Epoch: [1][2700/5994] Elapsed 25m 33s (remain 31m 10s) Loss: 1.1565 Grad: 13.9456 LR: 0.000953 \n",
            "Epoch: [1][2800/5994] Elapsed 26m 30s (remain 30m 13s) Loss: 1.1455 Grad: 8.1268 LR: 0.000946 \n",
            "Epoch: [1][2900/5994] Elapsed 27m 27s (remain 29m 16s) Loss: 1.1391 Grad: 1.2910 LR: 0.000940 \n",
            "Epoch: [1][3000/5994] Elapsed 28m 24s (remain 28m 19s) Loss: 1.1345 Grad: 8.2042 LR: 0.000933 \n",
            "Epoch: [1][3100/5994] Elapsed 29m 20s (remain 27m 22s) Loss: 1.1249 Grad: 3.3477 LR: 0.000925 \n",
            "Epoch: [1][3200/5994] Elapsed 30m 17s (remain 26m 26s) Loss: 1.1179 Grad: 9.1385 LR: 0.000917 \n",
            "Epoch: [1][3300/5994] Elapsed 31m 14s (remain 25m 29s) Loss: 1.1116 Grad: 21.1325 LR: 0.000909 \n",
            "Epoch: [1][3400/5994] Elapsed 32m 11s (remain 24m 32s) Loss: 1.1052 Grad: 9.3392 LR: 0.000901 \n",
            "Epoch: [1][3500/5994] Elapsed 33m 8s (remain 23m 35s) Loss: 1.0996 Grad: 12.4115 LR: 0.000892 \n",
            "Epoch: [1][3600/5994] Elapsed 34m 4s (remain 22m 38s) Loss: 1.0918 Grad: 22.3806 LR: 0.000882 \n",
            "Epoch: [1][3700/5994] Elapsed 35m 1s (remain 21m 42s) Loss: 1.0869 Grad: 23.0010 LR: 0.000873 \n",
            "Epoch: [1][3800/5994] Elapsed 35m 58s (remain 20m 45s) Loss: 1.0787 Grad: 22.4982 LR: 0.000863 \n",
            "Epoch: [1][3900/5994] Elapsed 36m 55s (remain 19m 48s) Loss: 1.0713 Grad: 0.1229 LR: 0.000853 \n",
            "Epoch: [1][4000/5994] Elapsed 37m 51s (remain 18m 51s) Loss: 1.0671 Grad: 15.2646 LR: 0.000842 \n",
            "Epoch: [1][4100/5994] Elapsed 38m 48s (remain 17m 54s) Loss: 1.0629 Grad: 1.7707 LR: 0.000832 \n",
            "Epoch: [1][4200/5994] Elapsed 39m 45s (remain 16m 58s) Loss: 1.0550 Grad: 10.7779 LR: 0.000821 \n",
            "Epoch: [1][4300/5994] Elapsed 40m 42s (remain 16m 1s) Loss: 1.0511 Grad: 0.8292 LR: 0.000809 \n",
            "Epoch: [1][4400/5994] Elapsed 41m 39s (remain 15m 4s) Loss: 1.0467 Grad: 9.9277 LR: 0.000798 \n",
            "Epoch: [1][4500/5994] Elapsed 42m 35s (remain 14m 7s) Loss: 1.0434 Grad: 18.3672 LR: 0.000786 \n",
            "Epoch: [1][4600/5994] Elapsed 43m 32s (remain 13m 11s) Loss: 1.0417 Grad: 30.6459 LR: 0.000774 \n",
            "Epoch: [1][4700/5994] Elapsed 44m 29s (remain 12m 14s) Loss: 1.0371 Grad: 10.0708 LR: 0.000762 \n",
            "Epoch: [1][4800/5994] Elapsed 45m 26s (remain 11m 17s) Loss: 1.0327 Grad: 20.4590 LR: 0.000749 \n",
            "Epoch: [1][4900/5994] Elapsed 46m 22s (remain 10m 20s) Loss: 1.0298 Grad: 3.3492 LR: 0.000736 \n",
            "Epoch: [1][5000/5994] Elapsed 47m 19s (remain 9m 23s) Loss: 1.0247 Grad: 14.4389 LR: 0.000723 \n",
            "Epoch: [1][5100/5994] Elapsed 48m 16s (remain 8m 27s) Loss: 1.0192 Grad: 0.3485 LR: 0.000710 \n",
            "Epoch: [1][5200/5994] Elapsed 49m 13s (remain 7m 30s) Loss: 1.0144 Grad: 4.8855 LR: 0.000697 \n",
            "Epoch: [1][5300/5994] Elapsed 50m 10s (remain 6m 33s) Loss: 1.0128 Grad: 8.4407 LR: 0.000683 \n",
            "Epoch: [1][5400/5994] Elapsed 51m 6s (remain 5m 36s) Loss: 1.0097 Grad: 24.2760 LR: 0.000670 \n",
            "Epoch: [1][5500/5994] Elapsed 52m 3s (remain 4m 39s) Loss: 1.0049 Grad: 15.2609 LR: 0.000656 \n",
            "Epoch: [1][5600/5994] Elapsed 53m 0s (remain 3m 43s) Loss: 1.0020 Grad: 14.2856 LR: 0.000642 \n",
            "Epoch: [1][5700/5994] Elapsed 53m 57s (remain 2m 46s) Loss: 0.9980 Grad: 6.2608 LR: 0.000628 \n",
            "Epoch: [1][5800/5994] Elapsed 54m 53s (remain 1m 49s) Loss: 0.9956 Grad: 15.0771 LR: 0.000614 \n",
            "Epoch: [1][5900/5994] Elapsed 55m 50s (remain 0m 52s) Loss: 0.9907 Grad: 16.2680 LR: 0.000600 \n",
            "Epoch: [1][5993/5994] Elapsed 56m 41s (remain 0m 0s) Loss: 0.9867 Grad: 28.2320 LR: 0.000587 \n",
            "EVAL: [0/726] Elapsed 0m 0s (remain 9m 3s) Loss: 1.2471 \n",
            "EVAL: [100/726] Elapsed 0m 17s (remain 1m 50s) Loss: 0.2737 \n",
            "EVAL: [200/726] Elapsed 0m 35s (remain 1m 31s) Loss: 0.2941 \n",
            "EVAL: [300/726] Elapsed 0m 52s (remain 1m 13s) Loss: 0.2721 \n",
            "EVAL: [400/726] Elapsed 1m 9s (remain 0m 56s) Loss: 0.2688 \n",
            "EVAL: [500/726] Elapsed 1m 26s (remain 0m 38s) Loss: 0.2481 \n",
            "EVAL: [600/726] Elapsed 1m 43s (remain 0m 21s) Loss: 0.2429 \n",
            "EVAL: [700/726] Elapsed 2m 0s (remain 0m 4s) Loss: 0.2422 \n",
            "EVAL: [725/726] Elapsed 2m 5s (remain 0m 0s) Loss: 0.2373 \n",
            "Post-processing 223 example predictions split into 2902 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6532084134102072, Train Loss: 0.9867, Val Loss: 0.2373, Time: 3528s\n",
            "Epoch 1 - Save Best Model. score: 0.6532, loss: 0.2373\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/5994] Elapsed 0m 1s (remain 114m 48s) Loss: 0.1743 Grad: 4.1461 LR: 0.000587 \n",
            "Epoch: [2][100/5994] Elapsed 0m 57s (remain 56m 23s) Loss: 0.5405 Grad: 17.7346 LR: 0.000572 \n",
            "Epoch: [2][200/5994] Elapsed 1m 54s (remain 55m 7s) Loss: 0.4876 Grad: 13.5165 LR: 0.000558 \n",
            "Epoch: [2][300/5994] Elapsed 2m 51s (remain 54m 4s) Loss: 0.4528 Grad: 3.4967 LR: 0.000543 \n",
            "Epoch: [2][400/5994] Elapsed 3m 48s (remain 53m 4s) Loss: 0.4344 Grad: 1.7407 LR: 0.000529 \n",
            "Epoch: [2][500/5994] Elapsed 4m 45s (remain 52m 5s) Loss: 0.4274 Grad: 3.9580 LR: 0.000514 \n",
            "Epoch: [2][600/5994] Elapsed 5m 41s (remain 51m 7s) Loss: 0.4386 Grad: 6.2164 LR: 0.000500 \n",
            "Epoch: [2][700/5994] Elapsed 6m 38s (remain 50m 9s) Loss: 0.4262 Grad: 1.5878 LR: 0.000485 \n",
            "Epoch: [2][800/5994] Elapsed 7m 35s (remain 49m 12s) Loss: 0.4294 Grad: 9.6828 LR: 0.000471 \n",
            "Epoch: [2][900/5994] Elapsed 8m 32s (remain 48m 15s) Loss: 0.4239 Grad: 17.8739 LR: 0.000456 \n",
            "Epoch: [2][1000/5994] Elapsed 9m 28s (remain 47m 18s) Loss: 0.4339 Grad: 16.6861 LR: 0.000442 \n",
            "Epoch: [2][1100/5994] Elapsed 10m 25s (remain 46m 20s) Loss: 0.4365 Grad: 25.2168 LR: 0.000427 \n",
            "Epoch: [2][1200/5994] Elapsed 11m 22s (remain 45m 23s) Loss: 0.4461 Grad: 11.9705 LR: 0.000413 \n",
            "Epoch: [2][1300/5994] Elapsed 12m 19s (remain 44m 26s) Loss: 0.4388 Grad: 0.3341 LR: 0.000398 \n",
            "Epoch: [2][1400/5994] Elapsed 13m 16s (remain 43m 29s) Loss: 0.4405 Grad: 1.6157 LR: 0.000384 \n",
            "Epoch: [2][1500/5994] Elapsed 14m 12s (remain 42m 32s) Loss: 0.4413 Grad: 38.8765 LR: 0.000370 \n",
            "Epoch: [2][1600/5994] Elapsed 15m 9s (remain 41m 35s) Loss: 0.4410 Grad: 14.6304 LR: 0.000356 \n",
            "Epoch: [2][1700/5994] Elapsed 16m 6s (remain 40m 38s) Loss: 0.4448 Grad: 1.4599 LR: 0.000342 \n",
            "Epoch: [2][1800/5994] Elapsed 17m 3s (remain 39m 42s) Loss: 0.4435 Grad: 0.2778 LR: 0.000329 \n",
            "Epoch: [2][1900/5994] Elapsed 17m 59s (remain 38m 45s) Loss: 0.4412 Grad: 7.9837 LR: 0.000315 \n",
            "Epoch: [2][2000/5994] Elapsed 18m 56s (remain 37m 48s) Loss: 0.4424 Grad: 37.2386 LR: 0.000301 \n",
            "Epoch: [2][2100/5994] Elapsed 19m 53s (remain 36m 51s) Loss: 0.4353 Grad: 1.3140 LR: 0.000288 \n",
            "Epoch: [2][2200/5994] Elapsed 20m 50s (remain 35m 54s) Loss: 0.4299 Grad: 46.7038 LR: 0.000275 \n",
            "Epoch: [2][2300/5994] Elapsed 21m 47s (remain 34m 57s) Loss: 0.4322 Grad: 18.8870 LR: 0.000262 \n",
            "Epoch: [2][2400/5994] Elapsed 22m 43s (remain 34m 1s) Loss: 0.4322 Grad: 3.5856 LR: 0.000249 \n",
            "Epoch: [2][2500/5994] Elapsed 23m 40s (remain 33m 4s) Loss: 0.4313 Grad: 2.4475 LR: 0.000237 \n",
            "Epoch: [2][2600/5994] Elapsed 24m 37s (remain 32m 7s) Loss: 0.4300 Grad: 2.3873 LR: 0.000225 \n",
            "Epoch: [2][2700/5994] Elapsed 25m 34s (remain 31m 10s) Loss: 0.4271 Grad: 0.1634 LR: 0.000213 \n",
            "Epoch: [2][2800/5994] Elapsed 26m 31s (remain 30m 13s) Loss: 0.4232 Grad: 1.9686 LR: 0.000201 \n",
            "Epoch: [2][2900/5994] Elapsed 27m 27s (remain 29m 16s) Loss: 0.4288 Grad: 1.8019 LR: 0.000189 \n",
            "Epoch: [2][3000/5994] Elapsed 28m 24s (remain 28m 20s) Loss: 0.4271 Grad: 16.4744 LR: 0.000178 \n",
            "Epoch: [2][3100/5994] Elapsed 29m 21s (remain 27m 23s) Loss: 0.4257 Grad: 7.0370 LR: 0.000167 \n",
            "Epoch: [2][3200/5994] Elapsed 30m 18s (remain 26m 26s) Loss: 0.4241 Grad: 33.0960 LR: 0.000156 \n",
            "Epoch: [2][3300/5994] Elapsed 31m 14s (remain 25m 29s) Loss: 0.4241 Grad: 16.4496 LR: 0.000146 \n",
            "Epoch: [2][3400/5994] Elapsed 32m 11s (remain 24m 32s) Loss: 0.4229 Grad: 8.2038 LR: 0.000136 \n",
            "Epoch: [2][3500/5994] Elapsed 33m 8s (remain 23m 35s) Loss: 0.4216 Grad: 26.6947 LR: 0.000126 \n",
            "Epoch: [2][3600/5994] Elapsed 34m 5s (remain 22m 39s) Loss: 0.4223 Grad: 1.2359 LR: 0.000116 \n",
            "Epoch: [2][3700/5994] Elapsed 35m 2s (remain 21m 42s) Loss: 0.4198 Grad: 11.7499 LR: 0.000107 \n",
            "Epoch: [2][3800/5994] Elapsed 35m 58s (remain 20m 45s) Loss: 0.4188 Grad: 11.1249 LR: 0.000098 \n",
            "Epoch: [2][3900/5994] Elapsed 36m 55s (remain 19m 48s) Loss: 0.4163 Grad: 0.1748 LR: 0.000090 \n",
            "Epoch: [2][4000/5994] Elapsed 37m 52s (remain 18m 51s) Loss: 0.4140 Grad: 8.3373 LR: 0.000082 \n",
            "Epoch: [2][4100/5994] Elapsed 38m 49s (remain 17m 55s) Loss: 0.4117 Grad: 18.5621 LR: 0.000074 \n",
            "Epoch: [2][4200/5994] Elapsed 39m 45s (remain 16m 58s) Loss: 0.4103 Grad: 9.8117 LR: 0.000067 \n",
            "Epoch: [2][4300/5994] Elapsed 40m 42s (remain 16m 1s) Loss: 0.4098 Grad: 23.4852 LR: 0.000059 \n",
            "Epoch: [2][4400/5994] Elapsed 41m 39s (remain 15m 4s) Loss: 0.4101 Grad: 0.8219 LR: 0.000053 \n",
            "Epoch: [2][4500/5994] Elapsed 42m 36s (remain 14m 7s) Loss: 0.4104 Grad: 17.9257 LR: 0.000046 \n",
            "Epoch: [2][4600/5994] Elapsed 43m 33s (remain 13m 11s) Loss: 0.4091 Grad: 0.0102 LR: 0.000041 \n",
            "Epoch: [2][4700/5994] Elapsed 44m 29s (remain 12m 14s) Loss: 0.4091 Grad: 0.8884 LR: 0.000035 \n",
            "Epoch: [2][4800/5994] Elapsed 45m 26s (remain 11m 17s) Loss: 0.4065 Grad: 3.5937 LR: 0.000030 \n",
            "Epoch: [2][4900/5994] Elapsed 46m 23s (remain 10m 20s) Loss: 0.4060 Grad: 3.4469 LR: 0.000025 \n",
            "Epoch: [2][5000/5994] Elapsed 47m 20s (remain 9m 23s) Loss: 0.4068 Grad: 0.0085 LR: 0.000021 \n",
            "Epoch: [2][5100/5994] Elapsed 48m 17s (remain 8m 27s) Loss: 0.4056 Grad: 12.1203 LR: 0.000017 \n",
            "Epoch: [2][5200/5994] Elapsed 49m 13s (remain 7m 30s) Loss: 0.4033 Grad: 3.4316 LR: 0.000013 \n",
            "Epoch: [2][5300/5994] Elapsed 50m 10s (remain 6m 33s) Loss: 0.4035 Grad: 20.6528 LR: 0.000010 \n",
            "Epoch: [2][5400/5994] Elapsed 51m 7s (remain 5m 36s) Loss: 0.4029 Grad: 2.8045 LR: 0.000007 \n",
            "Epoch: [2][5500/5994] Elapsed 52m 4s (remain 4m 39s) Loss: 0.4007 Grad: 7.3926 LR: 0.000005 \n",
            "Epoch: [2][5600/5994] Elapsed 53m 0s (remain 3m 43s) Loss: 0.4011 Grad: 38.0910 LR: 0.000003 \n",
            "Epoch: [2][5700/5994] Elapsed 53m 57s (remain 2m 46s) Loss: 0.3999 Grad: 10.0824 LR: 0.000002 \n",
            "Epoch: [2][5800/5994] Elapsed 54m 54s (remain 1m 49s) Loss: 0.4000 Grad: 25.8224 LR: 0.000001 \n",
            "Epoch: [2][5900/5994] Elapsed 55m 51s (remain 0m 52s) Loss: 0.3987 Grad: 0.0982 LR: 0.000000 \n",
            "Epoch: [2][5993/5994] Elapsed 56m 44s (remain 0m 0s) Loss: 0.3989 Grad: 42.3886 LR: 0.000000 \n",
            "EVAL: [0/726] Elapsed 0m 0s (remain 9m 9s) Loss: 1.3249 \n",
            "EVAL: [100/726] Elapsed 0m 17s (remain 1m 50s) Loss: 0.2856 \n",
            "EVAL: [200/726] Elapsed 0m 35s (remain 1m 31s) Loss: 0.3243 \n",
            "EVAL: [300/726] Elapsed 0m 52s (remain 1m 13s) Loss: 0.2925 \n",
            "EVAL: [400/726] Elapsed 1m 9s (remain 0m 56s) Loss: 0.2897 \n",
            "EVAL: [500/726] Elapsed 1m 26s (remain 0m 38s) Loss: 0.2622 \n",
            "EVAL: [600/726] Elapsed 1m 43s (remain 0m 21s) Loss: 0.2628 \n",
            "EVAL: [700/726] Elapsed 2m 0s (remain 0m 4s) Loss: 0.2581 \n",
            "EVAL: [725/726] Elapsed 2m 5s (remain 0m 0s) Loss: 0.2516 \n",
            "Post-processing 223 example predictions split into 2902 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.695382233610933, Train Loss: 0.3989, Val Loss: 0.2516, Time: 3531s\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.65321\n",
            "========== fold: 2 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/5972] Elapsed 0m 1s (remain 111m 33s) Loss: 6.1972 Grad: 7.0892 LR: 0.000000 \n",
            "Epoch: [1][100/5972] Elapsed 0m 57s (remain 56m 5s) Loss: 5.2088 Grad: 8.6436 LR: 0.000084 \n",
            "Epoch: [1][200/5972] Elapsed 1m 54s (remain 54m 52s) Loss: 3.5610 Grad: 25.9987 LR: 0.000168 \n",
            "Epoch: [1][300/5972] Elapsed 2m 51s (remain 53m 49s) Loss: 2.7585 Grad: 21.4420 LR: 0.000252 \n",
            "Epoch: [1][400/5972] Elapsed 3m 48s (remain 52m 49s) Loss: 2.3225 Grad: 36.3407 LR: 0.000336 \n",
            "Epoch: [1][500/5972] Elapsed 4m 44s (remain 51m 51s) Loss: 2.0301 Grad: 19.0755 LR: 0.000419 \n",
            "Epoch: [1][600/5972] Elapsed 5m 41s (remain 50m 53s) Loss: 1.8525 Grad: 1.2706 LR: 0.000503 \n",
            "Epoch: [1][700/5972] Elapsed 6m 38s (remain 49m 55s) Loss: 1.7180 Grad: 8.3318 LR: 0.000587 \n",
            "Epoch: [1][800/5972] Elapsed 7m 35s (remain 48m 58s) Loss: 1.6176 Grad: 20.1324 LR: 0.000671 \n",
            "Epoch: [1][900/5972] Elapsed 8m 31s (remain 48m 1s) Loss: 1.5455 Grad: 6.9270 LR: 0.000755 \n",
            "Epoch: [1][1000/5972] Elapsed 9m 28s (remain 47m 4s) Loss: 1.4976 Grad: 10.1401 LR: 0.000839 \n",
            "Epoch: [1][1100/5972] Elapsed 10m 25s (remain 46m 7s) Loss: 1.4564 Grad: 15.6238 LR: 0.000923 \n",
            "Epoch: [1][1200/5972] Elapsed 11m 22s (remain 45m 10s) Loss: 1.4249 Grad: 30.9928 LR: 0.001000 \n",
            "Epoch: [1][1300/5972] Elapsed 12m 18s (remain 44m 13s) Loss: 1.3890 Grad: 0.3405 LR: 0.001000 \n",
            "Epoch: [1][1400/5972] Elapsed 13m 15s (remain 43m 16s) Loss: 1.3607 Grad: 13.0881 LR: 0.000999 \n",
            "Epoch: [1][1500/5972] Elapsed 14m 12s (remain 42m 19s) Loss: 1.3345 Grad: 27.7157 LR: 0.000998 \n",
            "Epoch: [1][1600/5972] Elapsed 15m 9s (remain 41m 22s) Loss: 1.3056 Grad: 2.6866 LR: 0.000996 \n",
            "Epoch: [1][1700/5972] Elapsed 16m 6s (remain 40m 25s) Loss: 1.2928 Grad: 14.8500 LR: 0.000995 \n",
            "Epoch: [1][1800/5972] Elapsed 17m 2s (remain 39m 28s) Loss: 1.2675 Grad: 3.1939 LR: 0.000992 \n",
            "Epoch: [1][1900/5972] Elapsed 17m 59s (remain 38m 31s) Loss: 1.2479 Grad: 5.3917 LR: 0.000989 \n",
            "Epoch: [1][2000/5972] Elapsed 18m 56s (remain 37m 35s) Loss: 1.2380 Grad: 29.9988 LR: 0.000986 \n",
            "Epoch: [1][2100/5972] Elapsed 19m 53s (remain 36m 38s) Loss: 1.2276 Grad: 26.7972 LR: 0.000983 \n",
            "Epoch: [1][2200/5972] Elapsed 20m 49s (remain 35m 41s) Loss: 1.2133 Grad: 0.0054 LR: 0.000978 \n",
            "Epoch: [1][2300/5972] Elapsed 21m 46s (remain 34m 44s) Loss: 1.2017 Grad: 6.2304 LR: 0.000974 \n",
            "Epoch: [1][2400/5972] Elapsed 22m 43s (remain 33m 47s) Loss: 1.1845 Grad: 8.9845 LR: 0.000969 \n",
            "Epoch: [1][2500/5972] Elapsed 23m 40s (remain 32m 51s) Loss: 1.1770 Grad: 9.7714 LR: 0.000964 \n",
            "Epoch: [1][2600/5972] Elapsed 24m 37s (remain 31m 54s) Loss: 1.1659 Grad: 12.5201 LR: 0.000958 \n",
            "Epoch: [1][2700/5972] Elapsed 25m 33s (remain 30m 57s) Loss: 1.1599 Grad: 9.8684 LR: 0.000952 \n",
            "Epoch: [1][2800/5972] Elapsed 26m 30s (remain 30m 0s) Loss: 1.1535 Grad: 21.1271 LR: 0.000946 \n",
            "Epoch: [1][2900/5972] Elapsed 27m 27s (remain 29m 3s) Loss: 1.1458 Grad: 10.4409 LR: 0.000939 \n",
            "Epoch: [1][3000/5972] Elapsed 28m 24s (remain 28m 7s) Loss: 1.1424 Grad: 14.1383 LR: 0.000932 \n",
            "Epoch: [1][3100/5972] Elapsed 29m 21s (remain 27m 10s) Loss: 1.1334 Grad: 10.7112 LR: 0.000924 \n",
            "Epoch: [1][3200/5972] Elapsed 30m 17s (remain 26m 13s) Loss: 1.1261 Grad: 5.8646 LR: 0.000916 \n",
            "Epoch: [1][3300/5972] Elapsed 31m 14s (remain 25m 16s) Loss: 1.1203 Grad: 5.8493 LR: 0.000908 \n",
            "Epoch: [1][3400/5972] Elapsed 32m 11s (remain 24m 19s) Loss: 1.1105 Grad: 6.9493 LR: 0.000900 \n",
            "Epoch: [1][3500/5972] Elapsed 33m 8s (remain 23m 23s) Loss: 1.1076 Grad: 11.3686 LR: 0.000891 \n",
            "Epoch: [1][3600/5972] Elapsed 34m 4s (remain 22m 26s) Loss: 1.0996 Grad: 8.3702 LR: 0.000881 \n",
            "Epoch: [1][3700/5972] Elapsed 35m 1s (remain 21m 29s) Loss: 1.0933 Grad: 4.6607 LR: 0.000872 \n",
            "Epoch: [1][3800/5972] Elapsed 35m 58s (remain 20m 32s) Loss: 1.0937 Grad: 8.0038 LR: 0.000862 \n",
            "Epoch: [1][3900/5972] Elapsed 36m 55s (remain 19m 35s) Loss: 1.0879 Grad: 1.0202 LR: 0.000851 \n",
            "Epoch: [1][4000/5972] Elapsed 37m 51s (remain 18m 39s) Loss: 1.0807 Grad: 4.3488 LR: 0.000841 \n",
            "Epoch: [1][4100/5972] Elapsed 38m 48s (remain 17m 42s) Loss: 1.0756 Grad: 18.5954 LR: 0.000830 \n",
            "Epoch: [1][4200/5972] Elapsed 39m 45s (remain 16m 45s) Loss: 1.0676 Grad: 10.5000 LR: 0.000819 \n",
            "Epoch: [1][4300/5972] Elapsed 40m 42s (remain 15m 48s) Loss: 1.0612 Grad: 6.3966 LR: 0.000808 \n",
            "Epoch: [1][4400/5972] Elapsed 41m 39s (remain 14m 52s) Loss: 1.0544 Grad: 2.3983 LR: 0.000796 \n",
            "Epoch: [1][4500/5972] Elapsed 42m 35s (remain 13m 55s) Loss: 1.0504 Grad: 23.7743 LR: 0.000784 \n",
            "Epoch: [1][4600/5972] Elapsed 43m 32s (remain 12m 58s) Loss: 1.0461 Grad: 11.6305 LR: 0.000772 \n",
            "Epoch: [1][4700/5972] Elapsed 44m 29s (remain 12m 1s) Loss: 1.0418 Grad: 17.9073 LR: 0.000760 \n",
            "Epoch: [1][4800/5972] Elapsed 45m 26s (remain 11m 4s) Loss: 1.0354 Grad: 16.0380 LR: 0.000747 \n",
            "Epoch: [1][4900/5972] Elapsed 46m 23s (remain 10m 8s) Loss: 1.0348 Grad: 28.1293 LR: 0.000734 \n",
            "Epoch: [1][5000/5972] Elapsed 47m 19s (remain 9m 11s) Loss: 1.0308 Grad: 12.5438 LR: 0.000721 \n",
            "Epoch: [1][5100/5972] Elapsed 48m 16s (remain 8m 14s) Loss: 1.0270 Grad: 11.1427 LR: 0.000708 \n",
            "Epoch: [1][5200/5972] Elapsed 49m 13s (remain 7m 17s) Loss: 1.0230 Grad: 16.3970 LR: 0.000695 \n",
            "Epoch: [1][5300/5972] Elapsed 50m 10s (remain 6m 21s) Loss: 1.0188 Grad: 7.0718 LR: 0.000681 \n",
            "Epoch: [1][5400/5972] Elapsed 51m 7s (remain 5m 24s) Loss: 1.0149 Grad: 14.6597 LR: 0.000667 \n",
            "Epoch: [1][5500/5972] Elapsed 52m 3s (remain 4m 27s) Loss: 1.0093 Grad: 22.0976 LR: 0.000654 \n",
            "Epoch: [1][5600/5972] Elapsed 53m 0s (remain 3m 30s) Loss: 1.0032 Grad: 9.6525 LR: 0.000640 \n",
            "Epoch: [1][5700/5972] Elapsed 53m 57s (remain 2m 33s) Loss: 1.0007 Grad: 24.4498 LR: 0.000625 \n",
            "Epoch: [1][5800/5972] Elapsed 54m 54s (remain 1m 37s) Loss: 0.9964 Grad: 28.5479 LR: 0.000611 \n",
            "Epoch: [1][5900/5972] Elapsed 55m 51s (remain 0m 40s) Loss: 0.9918 Grad: 12.8512 LR: 0.000597 \n",
            "Epoch: [1][5971/5972] Elapsed 56m 30s (remain 0m 0s) Loss: 0.9880 Grad: 34.3019 LR: 0.000587 \n",
            "EVAL: [0/748] Elapsed 0m 0s (remain 9m 39s) Loss: 0.2782 \n",
            "EVAL: [100/748] Elapsed 0m 17s (remain 1m 55s) Loss: 0.3944 \n",
            "EVAL: [200/748] Elapsed 0m 35s (remain 1m 35s) Loss: 0.3588 \n",
            "EVAL: [300/748] Elapsed 0m 52s (remain 1m 17s) Loss: 0.3557 \n",
            "EVAL: [400/748] Elapsed 1m 9s (remain 1m 0s) Loss: 0.3176 \n",
            "EVAL: [500/748] Elapsed 1m 26s (remain 0m 42s) Loss: 0.2934 \n",
            "EVAL: [600/748] Elapsed 1m 43s (remain 0m 25s) Loss: 0.3001 \n",
            "EVAL: [700/748] Elapsed 2m 0s (remain 0m 8s) Loss: 0.2892 \n",
            "EVAL: [747/748] Elapsed 2m 8s (remain 0m 0s) Loss: 0.2773 \n",
            "Post-processing 223 example predictions split into 2990 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.617012695824355, Train Loss: 0.9880, Val Loss: 0.2773, Time: 3520s\n",
            "Epoch 1 - Save Best Model. score: 0.6170, loss: 0.2773\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/5972] Elapsed 0m 1s (remain 115m 42s) Loss: 0.1941 Grad: 4.3892 LR: 0.000587 \n",
            "Epoch: [2][100/5972] Elapsed 0m 57s (remain 56m 9s) Loss: 0.5013 Grad: 2.7812 LR: 0.000572 \n",
            "Epoch: [2][200/5972] Elapsed 1m 54s (remain 54m 55s) Loss: 0.4705 Grad: 0.0086 LR: 0.000558 \n",
            "Epoch: [2][300/5972] Elapsed 2m 51s (remain 53m 52s) Loss: 0.4457 Grad: 6.1731 LR: 0.000543 \n",
            "Epoch: [2][400/5972] Elapsed 3m 48s (remain 52m 52s) Loss: 0.4407 Grad: 1.5018 LR: 0.000529 \n",
            "Epoch: [2][500/5972] Elapsed 4m 45s (remain 51m 53s) Loss: 0.4369 Grad: 2.0417 LR: 0.000514 \n",
            "Epoch: [2][600/5972] Elapsed 5m 41s (remain 50m 55s) Loss: 0.4372 Grad: 0.0178 LR: 0.000499 \n",
            "Epoch: [2][700/5972] Elapsed 6m 38s (remain 49m 57s) Loss: 0.4344 Grad: 21.2925 LR: 0.000485 \n",
            "Epoch: [2][800/5972] Elapsed 7m 35s (remain 49m 0s) Loss: 0.4309 Grad: 11.4428 LR: 0.000470 \n",
            "Epoch: [2][900/5972] Elapsed 8m 32s (remain 48m 2s) Loss: 0.4317 Grad: 0.0810 LR: 0.000456 \n",
            "Epoch: [2][1000/5972] Elapsed 9m 28s (remain 47m 5s) Loss: 0.4363 Grad: 9.8886 LR: 0.000441 \n",
            "Epoch: [2][1100/5972] Elapsed 10m 25s (remain 46m 8s) Loss: 0.4324 Grad: 0.0558 LR: 0.000427 \n",
            "Epoch: [2][1200/5972] Elapsed 11m 22s (remain 45m 11s) Loss: 0.4352 Grad: 17.9578 LR: 0.000412 \n",
            "Epoch: [2][1300/5972] Elapsed 12m 19s (remain 44m 14s) Loss: 0.4387 Grad: 10.2412 LR: 0.000398 \n",
            "Epoch: [2][1400/5972] Elapsed 13m 16s (remain 43m 17s) Loss: 0.4407 Grad: 2.7525 LR: 0.000384 \n",
            "Epoch: [2][1500/5972] Elapsed 14m 12s (remain 42m 20s) Loss: 0.4389 Grad: 0.4099 LR: 0.000369 \n",
            "Epoch: [2][1600/5972] Elapsed 15m 9s (remain 41m 23s) Loss: 0.4392 Grad: 3.3482 LR: 0.000355 \n",
            "Epoch: [2][1700/5972] Elapsed 16m 6s (remain 40m 26s) Loss: 0.4368 Grad: 4.3605 LR: 0.000341 \n",
            "Epoch: [2][1800/5972] Elapsed 17m 3s (remain 39m 29s) Loss: 0.4340 Grad: 10.9482 LR: 0.000328 \n",
            "Epoch: [2][1900/5972] Elapsed 18m 0s (remain 38m 33s) Loss: 0.4288 Grad: 22.2223 LR: 0.000314 \n",
            "Epoch: [2][2000/5972] Elapsed 18m 56s (remain 37m 36s) Loss: 0.4268 Grad: 1.1852 LR: 0.000301 \n",
            "Epoch: [2][2100/5972] Elapsed 19m 53s (remain 36m 39s) Loss: 0.4303 Grad: 9.1562 LR: 0.000287 \n",
            "Epoch: [2][2200/5972] Elapsed 20m 50s (remain 35m 42s) Loss: 0.4282 Grad: 1.0000 LR: 0.000274 \n",
            "Epoch: [2][2300/5972] Elapsed 21m 47s (remain 34m 45s) Loss: 0.4243 Grad: 20.7492 LR: 0.000261 \n",
            "Epoch: [2][2400/5972] Elapsed 22m 44s (remain 33m 48s) Loss: 0.4230 Grad: 2.3729 LR: 0.000248 \n",
            "Epoch: [2][2500/5972] Elapsed 23m 40s (remain 32m 51s) Loss: 0.4212 Grad: 3.6693 LR: 0.000236 \n",
            "Epoch: [2][2600/5972] Elapsed 24m 37s (remain 31m 55s) Loss: 0.4202 Grad: 8.2322 LR: 0.000224 \n",
            "Epoch: [2][2700/5972] Elapsed 25m 34s (remain 30m 58s) Loss: 0.4174 Grad: 33.0428 LR: 0.000212 \n",
            "Epoch: [2][2800/5972] Elapsed 26m 31s (remain 30m 1s) Loss: 0.4185 Grad: 5.4076 LR: 0.000200 \n",
            "Epoch: [2][2900/5972] Elapsed 27m 28s (remain 29m 4s) Loss: 0.4153 Grad: 1.7788 LR: 0.000188 \n",
            "Epoch: [2][3000/5972] Elapsed 28m 24s (remain 28m 7s) Loss: 0.4153 Grad: 19.3829 LR: 0.000177 \n",
            "Epoch: [2][3100/5972] Elapsed 29m 21s (remain 27m 10s) Loss: 0.4121 Grad: 4.5092 LR: 0.000166 \n",
            "Epoch: [2][3200/5972] Elapsed 30m 18s (remain 26m 14s) Loss: 0.4136 Grad: 10.7614 LR: 0.000155 \n",
            "Epoch: [2][3300/5972] Elapsed 31m 15s (remain 25m 17s) Loss: 0.4117 Grad: 4.5956 LR: 0.000145 \n",
            "Epoch: [2][3400/5972] Elapsed 32m 11s (remain 24m 20s) Loss: 0.4096 Grad: 6.5446 LR: 0.000135 \n",
            "Epoch: [2][3500/5972] Elapsed 33m 8s (remain 23m 23s) Loss: 0.4090 Grad: 16.1896 LR: 0.000125 \n",
            "Epoch: [2][3600/5972] Elapsed 34m 5s (remain 22m 26s) Loss: 0.4072 Grad: 7.2734 LR: 0.000115 \n",
            "Epoch: [2][3700/5972] Elapsed 35m 2s (remain 21m 29s) Loss: 0.4052 Grad: 17.9834 LR: 0.000106 \n",
            "Epoch: [2][3800/5972] Elapsed 35m 58s (remain 20m 33s) Loss: 0.4059 Grad: 12.4192 LR: 0.000097 \n",
            "Epoch: [2][3900/5972] Elapsed 36m 55s (remain 19m 36s) Loss: 0.4043 Grad: 0.3754 LR: 0.000089 \n",
            "Epoch: [2][4000/5972] Elapsed 37m 52s (remain 18m 39s) Loss: 0.4038 Grad: 5.5842 LR: 0.000081 \n",
            "Epoch: [2][4100/5972] Elapsed 38m 49s (remain 17m 42s) Loss: 0.4045 Grad: 8.7291 LR: 0.000073 \n",
            "Epoch: [2][4200/5972] Elapsed 39m 46s (remain 16m 45s) Loss: 0.4035 Grad: 21.5728 LR: 0.000066 \n",
            "Epoch: [2][4300/5972] Elapsed 40m 42s (remain 15m 49s) Loss: 0.4033 Grad: 5.7775 LR: 0.000058 \n",
            "Epoch: [2][4400/5972] Elapsed 41m 39s (remain 14m 52s) Loss: 0.4020 Grad: 25.2244 LR: 0.000052 \n",
            "Epoch: [2][4500/5972] Elapsed 42m 36s (remain 13m 55s) Loss: 0.4011 Grad: 23.4652 LR: 0.000046 \n",
            "Epoch: [2][4600/5972] Elapsed 43m 33s (remain 12m 58s) Loss: 0.3996 Grad: 2.4502 LR: 0.000040 \n",
            "Epoch: [2][4700/5972] Elapsed 44m 30s (remain 12m 1s) Loss: 0.3998 Grad: 1.0274 LR: 0.000034 \n",
            "Epoch: [2][4800/5972] Elapsed 45m 26s (remain 11m 5s) Loss: 0.3993 Grad: 0.0538 LR: 0.000029 \n",
            "Epoch: [2][4900/5972] Elapsed 46m 23s (remain 10m 8s) Loss: 0.3978 Grad: 6.7731 LR: 0.000024 \n",
            "Epoch: [2][5000/5972] Elapsed 47m 20s (remain 9m 11s) Loss: 0.3975 Grad: 0.9577 LR: 0.000020 \n",
            "Epoch: [2][5100/5972] Elapsed 48m 17s (remain 8m 14s) Loss: 0.3966 Grad: 0.0070 LR: 0.000016 \n",
            "Epoch: [2][5200/5972] Elapsed 49m 14s (remain 7m 17s) Loss: 0.3955 Grad: 43.6661 LR: 0.000013 \n",
            "Epoch: [2][5300/5972] Elapsed 50m 10s (remain 6m 21s) Loss: 0.3952 Grad: 2.3427 LR: 0.000010 \n",
            "Epoch: [2][5400/5972] Elapsed 51m 7s (remain 5m 24s) Loss: 0.3949 Grad: 1.3864 LR: 0.000007 \n",
            "Epoch: [2][5500/5972] Elapsed 52m 4s (remain 4m 27s) Loss: 0.3943 Grad: 3.1101 LR: 0.000005 \n",
            "Epoch: [2][5600/5972] Elapsed 53m 1s (remain 3m 30s) Loss: 0.3940 Grad: 23.2942 LR: 0.000003 \n",
            "Epoch: [2][5700/5972] Elapsed 53m 58s (remain 2m 33s) Loss: 0.3929 Grad: 10.2850 LR: 0.000002 \n",
            "Epoch: [2][5800/5972] Elapsed 54m 54s (remain 1m 37s) Loss: 0.3915 Grad: 11.7113 LR: 0.000001 \n",
            "Epoch: [2][5900/5972] Elapsed 55m 51s (remain 0m 40s) Loss: 0.3904 Grad: 0.8743 LR: 0.000000 \n",
            "Epoch: [2][5971/5972] Elapsed 56m 32s (remain 0m 0s) Loss: 0.3901 Grad: 23.8877 LR: 0.000000 \n",
            "EVAL: [0/748] Elapsed 0m 0s (remain 9m 41s) Loss: 0.4381 \n",
            "EVAL: [100/748] Elapsed 0m 17s (remain 1m 55s) Loss: 0.4249 \n",
            "EVAL: [200/748] Elapsed 0m 35s (remain 1m 35s) Loss: 0.3939 \n",
            "EVAL: [300/748] Elapsed 0m 52s (remain 1m 17s) Loss: 0.3911 \n",
            "EVAL: [400/748] Elapsed 1m 9s (remain 1m 0s) Loss: 0.3451 \n",
            "EVAL: [500/748] Elapsed 1m 26s (remain 0m 42s) Loss: 0.3284 \n",
            "EVAL: [600/748] Elapsed 1m 43s (remain 0m 25s) Loss: 0.3365 \n",
            "EVAL: [700/748] Elapsed 2m 1s (remain 0m 8s) Loss: 0.3271 \n",
            "EVAL: [747/748] Elapsed 2m 9s (remain 0m 0s) Loss: 0.3130 \n",
            "Post-processing 223 example predictions split into 2990 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.6292697959738319, Train Loss: 0.3901, Val Loss: 0.3130, Time: 3523s\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.61701\n",
            "========== fold: 3 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/5946] Elapsed 0m 1s (remain 114m 8s) Loss: 5.8738 Grad: 5.4338 LR: 0.000000 \n",
            "Epoch: [1][100/5946] Elapsed 0m 57s (remain 55m 53s) Loss: 4.8771 Grad: 12.4135 LR: 0.000084 \n",
            "Epoch: [1][200/5946] Elapsed 1m 54s (remain 54m 38s) Loss: 3.2591 Grad: 56.7605 LR: 0.000168 \n",
            "Epoch: [1][300/5946] Elapsed 2m 51s (remain 53m 35s) Loss: 2.5323 Grad: 23.3776 LR: 0.000253 \n",
            "Epoch: [1][400/5946] Elapsed 3m 48s (remain 52m 34s) Loss: 2.2037 Grad: 21.3488 LR: 0.000337 \n",
            "Epoch: [1][500/5946] Elapsed 4m 44s (remain 51m 36s) Loss: 1.9158 Grad: 13.2074 LR: 0.000421 \n",
            "Epoch: [1][600/5946] Elapsed 5m 41s (remain 50m 38s) Loss: 1.7673 Grad: 10.5749 LR: 0.000505 \n",
            "Epoch: [1][700/5946] Elapsed 6m 38s (remain 49m 40s) Loss: 1.6463 Grad: 7.8696 LR: 0.000589 \n",
            "Epoch: [1][800/5946] Elapsed 7m 35s (remain 48m 43s) Loss: 1.5712 Grad: 25.9002 LR: 0.000673 \n",
            "Epoch: [1][900/5946] Elapsed 8m 31s (remain 47m 46s) Loss: 1.4982 Grad: 22.7828 LR: 0.000758 \n",
            "Epoch: [1][1000/5946] Elapsed 9m 28s (remain 46m 49s) Loss: 1.4346 Grad: 1.9943 LR: 0.000842 \n",
            "Epoch: [1][1100/5946] Elapsed 10m 25s (remain 45m 52s) Loss: 1.3972 Grad: 13.8547 LR: 0.000926 \n",
            "Epoch: [1][1200/5946] Elapsed 11m 22s (remain 44m 55s) Loss: 1.3566 Grad: 8.2710 LR: 0.001000 \n",
            "Epoch: [1][1300/5946] Elapsed 12m 18s (remain 43m 58s) Loss: 1.3314 Grad: 13.3592 LR: 0.001000 \n",
            "Epoch: [1][1400/5946] Elapsed 13m 15s (remain 43m 1s) Loss: 1.3123 Grad: 11.9891 LR: 0.000999 \n",
            "Epoch: [1][1500/5946] Elapsed 14m 12s (remain 42m 4s) Loss: 1.2834 Grad: 23.6811 LR: 0.000998 \n",
            "Epoch: [1][1600/5946] Elapsed 15m 9s (remain 41m 7s) Loss: 1.2589 Grad: 13.6066 LR: 0.000996 \n",
            "Epoch: [1][1700/5946] Elapsed 16m 5s (remain 40m 10s) Loss: 1.2423 Grad: 2.4800 LR: 0.000994 \n",
            "Epoch: [1][1800/5946] Elapsed 17m 2s (remain 39m 13s) Loss: 1.2274 Grad: 36.8243 LR: 0.000992 \n",
            "Epoch: [1][1900/5946] Elapsed 17m 59s (remain 38m 17s) Loss: 1.2216 Grad: 15.8915 LR: 0.000989 \n",
            "Epoch: [1][2000/5946] Elapsed 18m 56s (remain 37m 20s) Loss: 1.2088 Grad: 14.5645 LR: 0.000986 \n",
            "Epoch: [1][2100/5946] Elapsed 19m 53s (remain 36m 23s) Loss: 1.1927 Grad: 10.5541 LR: 0.000982 \n",
            "Epoch: [1][2200/5946] Elapsed 20m 49s (remain 35m 26s) Loss: 1.1856 Grad: 14.5729 LR: 0.000978 \n",
            "Epoch: [1][2300/5946] Elapsed 21m 46s (remain 34m 29s) Loss: 1.1805 Grad: 0.1246 LR: 0.000974 \n",
            "Epoch: [1][2400/5946] Elapsed 22m 43s (remain 33m 33s) Loss: 1.1700 Grad: 11.6123 LR: 0.000969 \n",
            "Epoch: [1][2500/5946] Elapsed 23m 40s (remain 32m 36s) Loss: 1.1567 Grad: 11.1788 LR: 0.000963 \n",
            "Epoch: [1][2600/5946] Elapsed 24m 37s (remain 31m 39s) Loss: 1.1490 Grad: 10.7148 LR: 0.000958 \n",
            "Epoch: [1][2700/5946] Elapsed 25m 33s (remain 30m 42s) Loss: 1.1402 Grad: 5.5029 LR: 0.000952 \n",
            "Epoch: [1][2800/5946] Elapsed 26m 30s (remain 29m 45s) Loss: 1.1376 Grad: 2.5121 LR: 0.000945 \n",
            "Epoch: [1][2900/5946] Elapsed 27m 27s (remain 28m 49s) Loss: 1.1329 Grad: 6.3180 LR: 0.000938 \n",
            "Epoch: [1][3000/5946] Elapsed 28m 24s (remain 27m 52s) Loss: 1.1279 Grad: 16.1401 LR: 0.000931 \n",
            "Epoch: [1][3100/5946] Elapsed 29m 21s (remain 26m 55s) Loss: 1.1180 Grad: 14.2580 LR: 0.000923 \n",
            "Epoch: [1][3200/5946] Elapsed 30m 17s (remain 25m 58s) Loss: 1.1121 Grad: 6.5347 LR: 0.000915 \n",
            "Epoch: [1][3300/5946] Elapsed 31m 14s (remain 25m 2s) Loss: 1.1078 Grad: 4.3035 LR: 0.000907 \n",
            "Epoch: [1][3400/5946] Elapsed 32m 11s (remain 24m 5s) Loss: 1.1002 Grad: 12.5501 LR: 0.000898 \n",
            "Epoch: [1][3500/5946] Elapsed 33m 8s (remain 23m 8s) Loss: 1.0972 Grad: 1.1158 LR: 0.000889 \n",
            "Epoch: [1][3600/5946] Elapsed 34m 4s (remain 22m 11s) Loss: 1.0946 Grad: 7.0642 LR: 0.000880 \n",
            "Epoch: [1][3700/5946] Elapsed 35m 1s (remain 21m 14s) Loss: 1.0879 Grad: 27.0577 LR: 0.000870 \n",
            "Epoch: [1][3800/5946] Elapsed 35m 58s (remain 20m 18s) Loss: 1.0813 Grad: 14.5831 LR: 0.000860 \n",
            "Epoch: [1][3900/5946] Elapsed 36m 55s (remain 19m 21s) Loss: 1.0730 Grad: 21.7465 LR: 0.000850 \n",
            "Epoch: [1][4000/5946] Elapsed 37m 52s (remain 18m 24s) Loss: 1.0670 Grad: 4.4330 LR: 0.000839 \n",
            "Epoch: [1][4100/5946] Elapsed 38m 48s (remain 17m 27s) Loss: 1.0629 Grad: 14.5452 LR: 0.000828 \n",
            "Epoch: [1][4200/5946] Elapsed 39m 45s (remain 16m 30s) Loss: 1.0580 Grad: 5.1876 LR: 0.000817 \n",
            "Epoch: [1][4300/5946] Elapsed 40m 42s (remain 15m 34s) Loss: 1.0502 Grad: 13.3879 LR: 0.000805 \n",
            "Epoch: [1][4400/5946] Elapsed 41m 39s (remain 14m 37s) Loss: 1.0421 Grad: 13.5813 LR: 0.000794 \n",
            "Epoch: [1][4500/5946] Elapsed 42m 36s (remain 13m 40s) Loss: 1.0367 Grad: 11.3699 LR: 0.000782 \n",
            "Epoch: [1][4600/5946] Elapsed 43m 32s (remain 12m 43s) Loss: 1.0332 Grad: 3.8235 LR: 0.000769 \n",
            "Epoch: [1][4700/5946] Elapsed 44m 29s (remain 11m 47s) Loss: 1.0309 Grad: 2.7836 LR: 0.000757 \n",
            "Epoch: [1][4800/5946] Elapsed 45m 26s (remain 10m 50s) Loss: 1.0255 Grad: 7.0590 LR: 0.000744 \n",
            "Epoch: [1][4900/5946] Elapsed 46m 23s (remain 9m 53s) Loss: 1.0245 Grad: 1.3748 LR: 0.000731 \n",
            "Epoch: [1][5000/5946] Elapsed 47m 20s (remain 8m 56s) Loss: 1.0215 Grad: 0.2392 LR: 0.000718 \n",
            "Epoch: [1][5100/5946] Elapsed 48m 16s (remain 7m 59s) Loss: 1.0156 Grad: 26.5018 LR: 0.000705 \n",
            "Epoch: [1][5200/5946] Elapsed 49m 13s (remain 7m 3s) Loss: 1.0146 Grad: 15.1992 LR: 0.000691 \n",
            "Epoch: [1][5300/5946] Elapsed 50m 10s (remain 6m 6s) Loss: 1.0099 Grad: 10.9635 LR: 0.000678 \n",
            "Epoch: [1][5400/5946] Elapsed 51m 7s (remain 5m 9s) Loss: 1.0079 Grad: 14.7915 LR: 0.000664 \n",
            "Epoch: [1][5500/5946] Elapsed 52m 3s (remain 4m 12s) Loss: 1.0047 Grad: 13.6587 LR: 0.000650 \n",
            "Epoch: [1][5600/5946] Elapsed 53m 0s (remain 3m 15s) Loss: 1.0028 Grad: 4.1961 LR: 0.000636 \n",
            "Epoch: [1][5700/5946] Elapsed 53m 57s (remain 2m 19s) Loss: 0.9974 Grad: 13.9550 LR: 0.000622 \n",
            "Epoch: [1][5800/5946] Elapsed 54m 54s (remain 1m 22s) Loss: 0.9944 Grad: 7.1924 LR: 0.000607 \n",
            "Epoch: [1][5900/5946] Elapsed 55m 51s (remain 0m 25s) Loss: 0.9909 Grad: 16.1754 LR: 0.000593 \n",
            "Epoch: [1][5945/5946] Elapsed 56m 15s (remain 0m 0s) Loss: 0.9878 Grad: 9.0372 LR: 0.000587 \n",
            "EVAL: [0/774] Elapsed 0m 0s (remain 10m 4s) Loss: 0.0075 \n",
            "EVAL: [100/774] Elapsed 0m 17s (remain 1m 59s) Loss: 0.2876 \n",
            "EVAL: [200/774] Elapsed 0m 35s (remain 1m 40s) Loss: 0.3028 \n",
            "EVAL: [300/774] Elapsed 0m 52s (remain 1m 22s) Loss: 0.2867 \n",
            "EVAL: [400/774] Elapsed 1m 9s (remain 1m 4s) Loss: 0.2477 \n",
            "EVAL: [500/774] Elapsed 1m 26s (remain 0m 47s) Loss: 0.2249 \n",
            "EVAL: [600/774] Elapsed 1m 43s (remain 0m 29s) Loss: 0.2377 \n",
            "EVAL: [700/774] Elapsed 2m 0s (remain 0m 12s) Loss: 0.2447 \n",
            "EVAL: [773/774] Elapsed 2m 13s (remain 0m 0s) Loss: 0.2449 \n",
            "Post-processing 223 example predictions split into 3096 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6765606092960353, Train Loss: 0.9878, Val Loss: 0.2449, Time: 3511s\n",
            "Epoch 1 - Save Best Model. score: 0.6766, loss: 0.2449\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [2][0/5946] Elapsed 0m 1s (remain 113m 13s) Loss: 0.8298 Grad: 9.8366 LR: 0.000587 \n",
            "Epoch: [2][100/5946] Elapsed 0m 57s (remain 55m 51s) Loss: 0.5468 Grad: 3.3152 LR: 0.000572 \n",
            "Epoch: [2][200/5946] Elapsed 1m 54s (remain 54m 38s) Loss: 0.4925 Grad: 5.7035 LR: 0.000558 \n",
            "Epoch: [2][300/5946] Elapsed 2m 51s (remain 53m 36s) Loss: 0.4801 Grad: 6.6466 LR: 0.000543 \n",
            "Epoch: [2][400/5946] Elapsed 3m 48s (remain 52m 36s) Loss: 0.4770 Grad: 16.1599 LR: 0.000528 \n",
            "Epoch: [2][500/5946] Elapsed 4m 45s (remain 51m 37s) Loss: 0.4685 Grad: 4.7532 LR: 0.000514 \n",
            "Epoch: [2][600/5946] Elapsed 5m 41s (remain 50m 39s) Loss: 0.4552 Grad: 8.8939 LR: 0.000499 \n",
            "Epoch: [2][700/5946] Elapsed 6m 38s (remain 49m 41s) Loss: 0.4582 Grad: 6.0077 LR: 0.000484 \n",
            "Epoch: [2][800/5946] Elapsed 7m 35s (remain 48m 44s) Loss: 0.4562 Grad: 31.2376 LR: 0.000470 \n",
            "Epoch: [2][900/5946] Elapsed 8m 32s (remain 47m 47s) Loss: 0.4434 Grad: 12.5483 LR: 0.000455 \n",
            "Epoch: [2][1000/5946] Elapsed 9m 28s (remain 46m 50s) Loss: 0.4365 Grad: 0.6296 LR: 0.000441 \n",
            "Epoch: [2][1100/5946] Elapsed 10m 25s (remain 45m 53s) Loss: 0.4308 Grad: 0.1173 LR: 0.000426 \n",
            "Epoch: [2][1200/5946] Elapsed 11m 22s (remain 44m 56s) Loss: 0.4276 Grad: 0.6798 LR: 0.000412 \n",
            "Epoch: [2][1300/5946] Elapsed 12m 19s (remain 43m 59s) Loss: 0.4261 Grad: 7.0569 LR: 0.000397 \n",
            "Epoch: [2][1400/5946] Elapsed 13m 16s (remain 43m 2s) Loss: 0.4245 Grad: 16.1761 LR: 0.000383 \n",
            "Epoch: [2][1500/5946] Elapsed 14m 12s (remain 42m 5s) Loss: 0.4255 Grad: 0.4269 LR: 0.000369 \n",
            "Epoch: [2][1600/5946] Elapsed 15m 9s (remain 41m 8s) Loss: 0.4309 Grad: 5.5006 LR: 0.000354 \n",
            "Epoch: [2][1700/5946] Elapsed 16m 6s (remain 40m 12s) Loss: 0.4327 Grad: 9.1372 LR: 0.000340 \n",
            "Epoch: [2][1800/5946] Elapsed 17m 3s (remain 39m 15s) Loss: 0.4363 Grad: 9.4161 LR: 0.000327 \n",
            "Epoch: [2][1900/5946] Elapsed 18m 0s (remain 38m 18s) Loss: 0.4359 Grad: 3.9370 LR: 0.000313 \n",
            "Epoch: [2][2000/5946] Elapsed 18m 56s (remain 37m 21s) Loss: 0.4307 Grad: 3.4540 LR: 0.000299 \n",
            "Epoch: [2][2100/5946] Elapsed 19m 53s (remain 36m 24s) Loss: 0.4303 Grad: 12.4711 LR: 0.000286 \n",
            "Epoch: [2][2200/5946] Elapsed 20m 52s (remain 35m 31s) Loss: 0.4360 Grad: 0.0470 LR: 0.000273 \n",
            "Epoch: [2][2300/5946] Elapsed 21m 49s (remain 34m 34s) Loss: 0.4317 Grad: 30.0471 LR: 0.000260 \n",
            "Epoch: [2][2400/5946] Elapsed 22m 46s (remain 33m 36s) Loss: 0.4315 Grad: 14.4934 LR: 0.000247 \n",
            "Epoch: [2][2500/5946] Elapsed 23m 42s (remain 32m 39s) Loss: 0.4330 Grad: 4.7533 LR: 0.000235 \n",
            "Epoch: [2][2600/5946] Elapsed 24m 39s (remain 31m 42s) Loss: 0.4283 Grad: 23.7402 LR: 0.000222 \n",
            "Epoch: [2][2700/5946] Elapsed 25m 36s (remain 30m 45s) Loss: 0.4267 Grad: 3.4495 LR: 0.000210 \n",
            "Epoch: [2][2800/5946] Elapsed 26m 33s (remain 29m 48s) Loss: 0.4237 Grad: 14.7103 LR: 0.000198 \n",
            "Epoch: [2][2900/5946] Elapsed 27m 30s (remain 28m 51s) Loss: 0.4249 Grad: 8.2938 LR: 0.000187 \n",
            "Epoch: [2][3000/5946] Elapsed 28m 26s (remain 27m 54s) Loss: 0.4261 Grad: 7.2612 LR: 0.000175 \n",
            "Epoch: [2][3100/5946] Elapsed 29m 23s (remain 26m 58s) Loss: 0.4280 Grad: 0.0081 LR: 0.000164 \n",
            "Epoch: [2][3200/5946] Elapsed 30m 20s (remain 26m 1s) Loss: 0.4265 Grad: 5.7421 LR: 0.000154 \n",
            "Epoch: [2][3300/5946] Elapsed 31m 17s (remain 25m 4s) Loss: 0.4293 Grad: 17.3973 LR: 0.000143 \n",
            "Epoch: [2][3400/5946] Elapsed 32m 14s (remain 24m 7s) Loss: 0.4282 Grad: 0.0262 LR: 0.000133 \n",
            "Epoch: [2][3500/5946] Elapsed 33m 10s (remain 23m 10s) Loss: 0.4268 Grad: 16.4582 LR: 0.000123 \n",
            "Epoch: [2][3600/5946] Elapsed 34m 7s (remain 22m 13s) Loss: 0.4258 Grad: 1.1849 LR: 0.000114 \n",
            "Epoch: [2][3700/5946] Elapsed 35m 4s (remain 21m 16s) Loss: 0.4259 Grad: 21.0963 LR: 0.000105 \n",
            "Epoch: [2][3800/5946] Elapsed 36m 1s (remain 20m 19s) Loss: 0.4243 Grad: 19.1529 LR: 0.000096 \n",
            "Epoch: [2][3900/5946] Elapsed 36m 57s (remain 19m 22s) Loss: 0.4226 Grad: 0.4734 LR: 0.000087 \n",
            "Epoch: [2][4000/5946] Elapsed 37m 54s (remain 18m 25s) Loss: 0.4194 Grad: 0.1613 LR: 0.000079 \n",
            "Epoch: [2][4100/5946] Elapsed 38m 51s (remain 17m 28s) Loss: 0.4194 Grad: 18.0844 LR: 0.000072 \n",
            "Epoch: [2][4200/5946] Elapsed 39m 48s (remain 16m 32s) Loss: 0.4187 Grad: 33.6303 LR: 0.000064 \n",
            "Epoch: [2][4300/5946] Elapsed 40m 45s (remain 15m 35s) Loss: 0.4162 Grad: 24.2531 LR: 0.000057 \n",
            "Epoch: [2][4400/5946] Elapsed 41m 41s (remain 14m 38s) Loss: 0.4152 Grad: 9.1982 LR: 0.000051 \n",
            "Epoch: [2][4500/5946] Elapsed 42m 38s (remain 13m 41s) Loss: 0.4149 Grad: 10.0983 LR: 0.000044 \n",
            "Epoch: [2][4600/5946] Elapsed 43m 35s (remain 12m 44s) Loss: 0.4131 Grad: 19.7933 LR: 0.000038 \n",
            "Epoch: [2][4700/5946] Elapsed 44m 32s (remain 11m 47s) Loss: 0.4126 Grad: 3.9653 LR: 0.000033 \n",
            "Epoch: [2][4800/5946] Elapsed 45m 29s (remain 10m 50s) Loss: 0.4119 Grad: 7.7709 LR: 0.000028 \n",
            "Epoch: [2][4900/5946] Elapsed 46m 25s (remain 9m 54s) Loss: 0.4102 Grad: 0.7918 LR: 0.000023 \n",
            "Epoch: [2][5000/5946] Elapsed 47m 22s (remain 8m 57s) Loss: 0.4100 Grad: 0.2300 LR: 0.000019 \n",
            "Epoch: [2][5100/5946] Elapsed 48m 19s (remain 8m 0s) Loss: 0.4088 Grad: 8.3547 LR: 0.000015 \n",
            "Epoch: [2][5200/5946] Elapsed 49m 16s (remain 7m 3s) Loss: 0.4081 Grad: 11.6800 LR: 0.000012 \n",
            "Epoch: [2][5300/5946] Elapsed 50m 12s (remain 6m 6s) Loss: 0.4064 Grad: 3.8459 LR: 0.000009 \n",
            "Epoch: [2][5400/5946] Elapsed 51m 9s (remain 5m 9s) Loss: 0.4054 Grad: 12.4178 LR: 0.000006 \n",
            "Epoch: [2][5500/5946] Elapsed 52m 6s (remain 4m 12s) Loss: 0.4047 Grad: 0.1109 LR: 0.000004 \n",
            "Epoch: [2][5600/5946] Elapsed 53m 3s (remain 3m 16s) Loss: 0.4044 Grad: 0.8355 LR: 0.000003 \n",
            "Epoch: [2][5700/5946] Elapsed 54m 0s (remain 2m 19s) Loss: 0.4048 Grad: 2.7889 LR: 0.000001 \n",
            "Epoch: [2][5800/5946] Elapsed 54m 56s (remain 1m 22s) Loss: 0.4039 Grad: 0.7747 LR: 0.000000 \n",
            "Epoch: [2][5900/5946] Elapsed 55m 53s (remain 0m 25s) Loss: 0.4029 Grad: 6.1700 LR: 0.000000 \n",
            "Epoch: [2][5945/5946] Elapsed 56m 18s (remain 0m 0s) Loss: 0.4026 Grad: 7.1184 LR: 0.000000 \n",
            "EVAL: [0/774] Elapsed 0m 0s (remain 10m 7s) Loss: 0.0118 \n",
            "EVAL: [100/774] Elapsed 0m 17s (remain 1m 59s) Loss: 0.2638 \n",
            "EVAL: [200/774] Elapsed 0m 35s (remain 1m 40s) Loss: 0.2717 \n",
            "EVAL: [300/774] Elapsed 0m 52s (remain 1m 22s) Loss: 0.2487 \n",
            "EVAL: [400/774] Elapsed 1m 9s (remain 1m 4s) Loss: 0.2115 \n",
            "EVAL: [500/774] Elapsed 1m 26s (remain 0m 47s) Loss: 0.1986 \n",
            "EVAL: [600/774] Elapsed 1m 43s (remain 0m 29s) Loss: 0.2069 \n",
            "EVAL: [700/774] Elapsed 2m 1s (remain 0m 12s) Loss: 0.2126 \n",
            "EVAL: [773/774] Elapsed 2m 13s (remain 0m 0s) Loss: 0.2092 \n",
            "Post-processing 223 example predictions split into 3096 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Score: 0.7082621538899566, Train Loss: 0.4026, Val Loss: 0.2092, Time: 3513s\n",
            "Epoch 2 - Save Best Model. score: 0.7083, loss: 0.2092\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.70826\n",
            "========== fold: 4 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [1][0/6020] Elapsed 0m 1s (remain 115m 3s) Loss: 5.9964 Grad: 6.2226 LR: 0.000000 \n",
            "Epoch: [1][100/6020] Elapsed 0m 57s (remain 56m 33s) Loss: 5.0989 Grad: 18.4213 LR: 0.000083 \n",
            "Epoch: [1][200/6020] Elapsed 1m 54s (remain 55m 18s) Loss: 3.4402 Grad: 67.2813 LR: 0.000166 \n",
            "Epoch: [1][300/6020] Elapsed 2m 51s (remain 54m 15s) Loss: 2.6594 Grad: 22.1092 LR: 0.000249 \n",
            "Epoch: [1][400/6020] Elapsed 3m 48s (remain 53m 16s) Loss: 2.2566 Grad: 22.0113 LR: 0.000332 \n",
            "Epoch: [1][500/6020] Elapsed 4m 44s (remain 52m 17s) Loss: 1.9712 Grad: 8.5952 LR: 0.000415 \n",
            "Epoch: [1][600/6020] Elapsed 5m 41s (remain 51m 19s) Loss: 1.8354 Grad: 7.8990 LR: 0.000498 \n",
            "Epoch: [1][700/6020] Elapsed 6m 38s (remain 50m 21s) Loss: 1.7187 Grad: 8.5338 LR: 0.000581 \n",
            "Epoch: [1][800/6020] Elapsed 7m 35s (remain 49m 24s) Loss: 1.6236 Grad: 8.2404 LR: 0.000664 \n",
            "Epoch: [1][900/6020] Elapsed 8m 31s (remain 48m 27s) Loss: 1.5585 Grad: 18.0406 LR: 0.000748 \n",
            "Epoch: [1][1000/6020] Elapsed 9m 28s (remain 47m 30s) Loss: 1.5011 Grad: 1.3508 LR: 0.000831 \n",
            "Epoch: [1][1100/6020] Elapsed 10m 25s (remain 46m 33s) Loss: 1.4497 Grad: 13.7799 LR: 0.000914 \n",
            "Epoch: [1][1200/6020] Elapsed 11m 22s (remain 45m 36s) Loss: 1.4112 Grad: 12.3032 LR: 0.000997 \n",
            "Epoch: [1][1300/6020] Elapsed 12m 18s (remain 44m 39s) Loss: 1.3736 Grad: 35.8154 LR: 0.001000 \n",
            "Epoch: [1][1400/6020] Elapsed 13m 15s (remain 43m 43s) Loss: 1.3519 Grad: 11.5256 LR: 0.000999 \n",
            "Epoch: [1][1500/6020] Elapsed 14m 12s (remain 42m 46s) Loss: 1.3289 Grad: 19.0438 LR: 0.000998 \n",
            "Epoch: [1][1600/6020] Elapsed 15m 9s (remain 41m 49s) Loss: 1.3094 Grad: 4.6385 LR: 0.000997 \n",
            "Epoch: [1][1700/6020] Elapsed 16m 6s (remain 40m 52s) Loss: 1.2964 Grad: 8.4810 LR: 0.000995 \n",
            "Epoch: [1][1800/6020] Elapsed 17m 2s (remain 39m 55s) Loss: 1.2810 Grad: 6.4934 LR: 0.000993 \n",
            "Epoch: [1][1900/6020] Elapsed 17m 59s (remain 38m 59s) Loss: 1.2666 Grad: 12.6137 LR: 0.000990 \n",
            "Epoch: [1][2000/6020] Elapsed 18m 56s (remain 38m 2s) Loss: 1.2476 Grad: 18.2870 LR: 0.000987 \n",
            "Epoch: [1][2100/6020] Elapsed 19m 53s (remain 37m 5s) Loss: 1.2326 Grad: 13.4937 LR: 0.000983 \n",
            "Epoch: [1][2200/6020] Elapsed 20m 49s (remain 36m 8s) Loss: 1.2249 Grad: 10.9392 LR: 0.000979 \n",
            "Epoch: [1][2300/6020] Elapsed 21m 46s (remain 35m 11s) Loss: 1.2088 Grad: 12.8820 LR: 0.000975 \n",
            "Epoch: [1][2400/6020] Elapsed 22m 43s (remain 34m 15s) Loss: 1.1986 Grad: 0.0685 LR: 0.000970 \n",
            "Epoch: [1][2500/6020] Elapsed 23m 40s (remain 33m 18s) Loss: 1.1888 Grad: 2.6586 LR: 0.000965 \n",
            "Epoch: [1][2600/6020] Elapsed 24m 36s (remain 32m 21s) Loss: 1.1776 Grad: 12.2363 LR: 0.000960 \n",
            "Epoch: [1][2700/6020] Elapsed 25m 33s (remain 31m 24s) Loss: 1.1640 Grad: 0.7418 LR: 0.000954 \n",
            "Epoch: [1][2800/6020] Elapsed 26m 30s (remain 30m 27s) Loss: 1.1505 Grad: 11.9648 LR: 0.000947 \n",
            "Epoch: [1][2900/6020] Elapsed 27m 27s (remain 29m 31s) Loss: 1.1424 Grad: 10.6933 LR: 0.000941 \n",
            "Epoch: [1][3000/6020] Elapsed 28m 24s (remain 28m 34s) Loss: 1.1390 Grad: 3.6214 LR: 0.000934 \n",
            "Epoch: [1][3100/6020] Elapsed 29m 20s (remain 27m 37s) Loss: 1.1301 Grad: 11.0744 LR: 0.000926 \n",
            "Epoch: [1][3200/6020] Elapsed 30m 17s (remain 26m 40s) Loss: 1.1187 Grad: 9.3551 LR: 0.000919 \n",
            "Epoch: [1][3300/6020] Elapsed 31m 14s (remain 25m 43s) Loss: 1.1144 Grad: 0.9078 LR: 0.000910 \n",
            "Epoch: [1][3400/6020] Elapsed 32m 11s (remain 24m 47s) Loss: 1.1141 Grad: 8.1637 LR: 0.000902 \n",
            "Epoch: [1][3500/6020] Elapsed 33m 8s (remain 23m 50s) Loss: 1.1061 Grad: 12.7293 LR: 0.000893 \n",
            "Epoch: [1][3600/6020] Elapsed 34m 4s (remain 22m 53s) Loss: 1.0974 Grad: 11.2188 LR: 0.000884 \n",
            "Epoch: [1][3700/6020] Elapsed 35m 1s (remain 21m 56s) Loss: 1.0892 Grad: 10.0663 LR: 0.000875 \n",
            "Epoch: [1][3800/6020] Elapsed 35m 58s (remain 21m 0s) Loss: 1.0804 Grad: 3.2213 LR: 0.000865 \n",
            "Epoch: [1][3900/6020] Elapsed 36m 55s (remain 20m 3s) Loss: 1.0756 Grad: 7.9749 LR: 0.000855 \n",
            "Epoch: [1][4000/6020] Elapsed 37m 51s (remain 19m 6s) Loss: 1.0697 Grad: 19.0801 LR: 0.000845 \n",
            "Epoch: [1][4100/6020] Elapsed 38m 48s (remain 18m 9s) Loss: 1.0688 Grad: 19.4604 LR: 0.000834 \n",
            "Epoch: [1][4200/6020] Elapsed 39m 45s (remain 17m 12s) Loss: 1.0611 Grad: 6.3657 LR: 0.000823 \n",
            "Epoch: [1][4300/6020] Elapsed 40m 42s (remain 16m 16s) Loss: 1.0552 Grad: 6.7804 LR: 0.000812 \n",
            "Epoch: [1][4400/6020] Elapsed 41m 39s (remain 15m 19s) Loss: 1.0502 Grad: 7.5121 LR: 0.000800 \n",
            "Epoch: [1][4500/6020] Elapsed 42m 35s (remain 14m 22s) Loss: 1.0415 Grad: 24.2456 LR: 0.000789 \n",
            "Epoch: [1][4600/6020] Elapsed 43m 32s (remain 13m 25s) Loss: 1.0381 Grad: 6.8126 LR: 0.000777 \n",
            "Epoch: [1][4700/6020] Elapsed 44m 29s (remain 12m 28s) Loss: 1.0343 Grad: 11.2001 LR: 0.000764 \n",
            "Epoch: [1][4800/6020] Elapsed 45m 26s (remain 11m 32s) Loss: 1.0296 Grad: 1.2737 LR: 0.000752 \n",
            "Epoch: [1][4900/6020] Elapsed 46m 22s (remain 10m 35s) Loss: 1.0255 Grad: 13.0472 LR: 0.000739 \n",
            "Epoch: [1][5000/6020] Elapsed 47m 19s (remain 9m 38s) Loss: 1.0221 Grad: 23.4842 LR: 0.000727 \n",
            "Epoch: [1][5100/6020] Elapsed 48m 16s (remain 8m 41s) Loss: 1.0171 Grad: 3.6221 LR: 0.000714 \n",
            "Epoch: [1][5200/6020] Elapsed 49m 13s (remain 7m 45s) Loss: 1.0130 Grad: 1.1252 LR: 0.000700 \n",
            "Epoch: [1][5300/6020] Elapsed 50m 10s (remain 6m 48s) Loss: 1.0077 Grad: 4.8591 LR: 0.000687 \n",
            "Epoch: [1][5400/6020] Elapsed 51m 6s (remain 5m 51s) Loss: 1.0042 Grad: 9.2367 LR: 0.000673 \n",
            "Epoch: [1][5500/6020] Elapsed 52m 3s (remain 4m 54s) Loss: 1.0006 Grad: 8.0483 LR: 0.000660 \n",
            "Epoch: [1][5600/6020] Elapsed 53m 0s (remain 3m 57s) Loss: 0.9953 Grad: 0.0728 LR: 0.000646 \n",
            "Epoch: [1][5700/6020] Elapsed 53m 57s (remain 3m 1s) Loss: 0.9929 Grad: 11.6730 LR: 0.000632 \n",
            "Epoch: [1][5800/6020] Elapsed 54m 54s (remain 2m 4s) Loss: 0.9901 Grad: 12.1694 LR: 0.000618 \n",
            "Epoch: [1][5900/6020] Elapsed 55m 50s (remain 1m 7s) Loss: 0.9870 Grad: 10.6531 LR: 0.000604 \n",
            "Epoch: [1][6000/6020] Elapsed 56m 47s (remain 0m 10s) Loss: 0.9825 Grad: 43.5148 LR: 0.000590 \n",
            "Epoch: [1][6019/6020] Elapsed 56m 58s (remain 0m 0s) Loss: 0.9812 Grad: 22.1867 LR: 0.000587 \n",
            "EVAL: [0/701] Elapsed 0m 0s (remain 9m 0s) Loss: 0.0429 \n",
            "EVAL: [100/701] Elapsed 0m 17s (remain 1m 46s) Loss: 0.2919 \n",
            "EVAL: [200/701] Elapsed 0m 35s (remain 1m 27s) Loss: 0.2783 \n",
            "EVAL: [300/701] Elapsed 0m 52s (remain 1m 9s) Loss: 0.2925 \n",
            "EVAL: [400/701] Elapsed 1m 9s (remain 0m 51s) Loss: 0.2769 \n",
            "EVAL: [500/701] Elapsed 1m 26s (remain 0m 34s) Loss: 0.2461 \n",
            "EVAL: [600/701] Elapsed 1m 43s (remain 0m 17s) Loss: 0.2419 \n",
            "EVAL: [700/701] Elapsed 2m 0s (remain 0m 0s) Loss: 0.2653 \n",
            "Post-processing 222 example predictions split into 2801 features.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Score: 0.6846758159258161, Train Loss: 0.9812, Val Loss: 0.2653, Time: 3540s\n",
            "Epoch 1 - Save Best Model. score: 0.6847, loss: 0.2653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2][0/6020] Elapsed 0m 1s (remain 119m 17s) Loss: 0.1092 Grad: 2.7537 LR: 0.000587 \n",
            "Epoch: [2][100/6020] Elapsed 0m 58s (remain 56m 39s) Loss: 0.4740 Grad: 9.6065 LR: 0.000573 \n",
            "Epoch: [2][200/6020] Elapsed 1m 54s (remain 55m 23s) Loss: 0.5346 Grad: 17.6754 LR: 0.000558 \n",
            "Epoch: [2][300/6020] Elapsed 2m 51s (remain 54m 19s) Loss: 0.5276 Grad: 8.2166 LR: 0.000544 \n",
            "Epoch: [2][400/6020] Elapsed 3m 48s (remain 53m 19s) Loss: 0.5056 Grad: 2.8062 LR: 0.000529 \n",
            "Epoch: [2][500/6020] Elapsed 4m 45s (remain 52m 20s) Loss: 0.5009 Grad: 20.7418 LR: 0.000515 \n",
            "Epoch: [2][600/6020] Elapsed 5m 41s (remain 51m 22s) Loss: 0.4812 Grad: 23.6862 LR: 0.000500 \n",
            "Epoch: [2][700/6020] Elapsed 6m 38s (remain 50m 24s) Loss: 0.4737 Grad: 9.3035 LR: 0.000486 \n",
            "Epoch: [2][800/6020] Elapsed 7m 35s (remain 49m 27s) Loss: 0.4763 Grad: 27.4298 LR: 0.000471 \n",
            "Epoch: [2][900/6020] Elapsed 8m 32s (remain 48m 30s) Loss: 0.4734 Grad: 5.7332 LR: 0.000457 \n",
            "Epoch: [2][1000/6020] Elapsed 9m 28s (remain 47m 32s) Loss: 0.4669 Grad: 2.1513 LR: 0.000442 \n",
            "Epoch: [2][1100/6020] Elapsed 10m 25s (remain 46m 35s) Loss: 0.4696 Grad: 22.4149 LR: 0.000428 \n",
            "Epoch: [2][1200/6020] Elapsed 11m 22s (remain 45m 38s) Loss: 0.4641 Grad: 0.0552 LR: 0.000414 \n",
            "Epoch: [2][1300/6020] Elapsed 12m 19s (remain 44m 41s) Loss: 0.4603 Grad: 0.3741 LR: 0.000400 \n",
            "Epoch: [2][1400/6020] Elapsed 13m 16s (remain 43m 44s) Loss: 0.4548 Grad: 0.6557 LR: 0.000385 \n",
            "Epoch: [2][1500/6020] Elapsed 14m 12s (remain 42m 47s) Loss: 0.4513 Grad: 18.0729 LR: 0.000371 \n",
            "Epoch: [2][1600/6020] Elapsed 15m 9s (remain 41m 50s) Loss: 0.4519 Grad: 11.1635 LR: 0.000357 \n",
            "Epoch: [2][1700/6020] Elapsed 16m 6s (remain 40m 53s) Loss: 0.4523 Grad: 28.4844 LR: 0.000344 \n",
            "Epoch: [2][1800/6020] Elapsed 17m 3s (remain 39m 57s) Loss: 0.4498 Grad: 17.7103 LR: 0.000330 \n",
            "Epoch: [2][1900/6020] Elapsed 18m 0s (remain 39m 0s) Loss: 0.4429 Grad: 0.7005 LR: 0.000316 \n",
            "Epoch: [2][2000/6020] Elapsed 18m 56s (remain 38m 3s) Loss: 0.4417 Grad: 0.4763 LR: 0.000303 \n",
            "Epoch: [2][2100/6020] Elapsed 19m 53s (remain 37m 6s) Loss: 0.4418 Grad: 11.2234 LR: 0.000290 \n",
            "Epoch: [2][2200/6020] Elapsed 20m 50s (remain 36m 9s) Loss: 0.4396 Grad: 1.7000 LR: 0.000277 \n",
            "Epoch: [2][2300/6020] Elapsed 21m 47s (remain 35m 12s) Loss: 0.4403 Grad: 4.5209 LR: 0.000264 \n",
            "Epoch: [2][2400/6020] Elapsed 22m 43s (remain 34m 15s) Loss: 0.4426 Grad: 12.1165 LR: 0.000251 \n",
            "Epoch: [2][2500/6020] Elapsed 23m 40s (remain 33m 19s) Loss: 0.4410 Grad: 8.9022 LR: 0.000239 \n",
            "Epoch: [2][2600/6020] Elapsed 24m 37s (remain 32m 22s) Loss: 0.4397 Grad: 32.5762 LR: 0.000226 \n",
            "Epoch: [2][2700/6020] Elapsed 25m 34s (remain 31m 25s) Loss: 0.4371 Grad: 9.9595 LR: 0.000214 \n",
            "Epoch: [2][2800/6020] Elapsed 26m 31s (remain 30m 28s) Loss: 0.4339 Grad: 31.4998 LR: 0.000203 \n",
            "Epoch: [2][2900/6020] Elapsed 27m 27s (remain 29m 31s) Loss: 0.4331 Grad: 9.0957 LR: 0.000191 \n",
            "Epoch: [2][3000/6020] Elapsed 28m 24s (remain 28m 34s) Loss: 0.4316 Grad: 24.1502 LR: 0.000180 \n",
            "Epoch: [2][3100/6020] Elapsed 29m 21s (remain 27m 38s) Loss: 0.4310 Grad: 5.3162 LR: 0.000169 \n",
            "Epoch: [2][3200/6020] Elapsed 30m 18s (remain 26m 41s) Loss: 0.4295 Grad: 17.6143 LR: 0.000158 \n",
            "Epoch: [2][3300/6020] Elapsed 31m 15s (remain 25m 44s) Loss: 0.4273 Grad: 14.9254 LR: 0.000148 \n",
            "Epoch: [2][3400/6020] Elapsed 32m 11s (remain 24m 47s) Loss: 0.4260 Grad: 25.2759 LR: 0.000137 \n",
            "Epoch: [2][3500/6020] Elapsed 33m 8s (remain 23m 50s) Loss: 0.4265 Grad: 1.0142 LR: 0.000128 \n",
            "Epoch: [2][3600/6020] Elapsed 34m 5s (remain 22m 54s) Loss: 0.4275 Grad: 1.3185 LR: 0.000118 \n",
            "Epoch: [2][3700/6020] Elapsed 35m 2s (remain 21m 57s) Loss: 0.4259 Grad: 12.8528 LR: 0.000109 \n",
            "Epoch: [2][3800/6020] Elapsed 35m 58s (remain 21m 0s) Loss: 0.4259 Grad: 1.0626 LR: 0.000100 \n",
            "Epoch: [2][3900/6020] Elapsed 36m 55s (remain 20m 3s) Loss: 0.4247 Grad: 10.6176 LR: 0.000092 \n",
            "Epoch: [2][4000/6020] Elapsed 37m 52s (remain 19m 6s) Loss: 0.4255 Grad: 19.5561 LR: 0.000083 \n",
            "Epoch: [2][4100/6020] Elapsed 38m 49s (remain 18m 9s) Loss: 0.4229 Grad: 20.2569 LR: 0.000075 \n",
            "Epoch: [2][4200/6020] Elapsed 39m 46s (remain 17m 13s) Loss: 0.4219 Grad: 11.7576 LR: 0.000068 \n",
            "Epoch: [2][4300/6020] Elapsed 40m 42s (remain 16m 16s) Loss: 0.4201 Grad: 4.8885 LR: 0.000061 \n",
            "Epoch: [2][4400/6020] Elapsed 41m 39s (remain 15m 19s) Loss: 0.4191 Grad: 0.5919 LR: 0.000054 \n",
            "Epoch: [2][4500/6020] Elapsed 42m 36s (remain 14m 22s) Loss: 0.4178 Grad: 23.2632 LR: 0.000048 \n",
            "Epoch: [2][4600/6020] Elapsed 43m 33s (remain 13m 25s) Loss: 0.4154 Grad: 1.1219 LR: 0.000042 \n",
            "Epoch: [2][4700/6020] Elapsed 44m 30s (remain 12m 29s) Loss: 0.4130 Grad: 0.0110 LR: 0.000036 \n",
            "Epoch: [2][4800/6020] Elapsed 45m 26s (remain 11m 32s) Loss: 0.4134 Grad: 6.3199 LR: 0.000031 \n",
            "Epoch: [2][4900/6020] Elapsed 46m 23s (remain 10m 35s) Loss: 0.4131 Grad: 28.6774 LR: 0.000026 \n",
            "Epoch: [2][5000/6020] Elapsed 47m 20s (remain 9m 38s) Loss: 0.4131 Grad: 1.7657 LR: 0.000022 \n",
            "Epoch: [2][5100/6020] Elapsed 48m 17s (remain 8m 41s) Loss: 0.4122 Grad: 4.1824 LR: 0.000018 \n",
            "Epoch: [2][5200/6020] Elapsed 49m 13s (remain 7m 45s) Loss: 0.4111 Grad: 2.8446 LR: 0.000014 \n",
            "Epoch: [2][5300/6020] Elapsed 50m 10s (remain 6m 48s) Loss: 0.4097 Grad: 21.0463 LR: 0.000011 \n",
            "Epoch: [2][5400/6020] Elapsed 51m 7s (remain 5m 51s) Loss: 0.4088 Grad: 15.2352 LR: 0.000008 \n",
            "Epoch: [2][5500/6020] Elapsed 52m 4s (remain 4m 54s) Loss: 0.4096 Grad: 1.1631 LR: 0.000006 \n",
            "Epoch: [2][5600/6020] Elapsed 53m 1s (remain 3m 57s) Loss: 0.4090 Grad: 15.0714 LR: 0.000004 \n",
            "Epoch: [2][5700/6020] Elapsed 53m 57s (remain 3m 1s) Loss: 0.4102 Grad: 37.8453 LR: 0.000002 \n",
            "Epoch: [2][5800/6020] Elapsed 54m 54s (remain 2m 4s) Loss: 0.4095 Grad: 3.4113 LR: 0.000001 \n",
            "Epoch: [2][5900/6020] Elapsed 55m 51s (remain 1m 7s) Loss: 0.4112 Grad: 0.1139 LR: 0.000000 \n",
            "Epoch: [2][6000/6020] Elapsed 56m 48s (remain 0m 10s) Loss: 0.4095 Grad: 1.0205 LR: 0.000000 \n",
            "Epoch: [2][6019/6020] Elapsed 56m 58s (remain 0m 0s) Loss: 0.4093 Grad: 16.6907 LR: 0.000000 \n",
            "EVAL: [0/701] Elapsed 0m 0s (remain 9m 12s) Loss: 0.0914 \n",
            "EVAL: [100/701] Elapsed 0m 17s (remain 1m 46s) Loss: 0.2917 \n",
            "EVAL: [200/701] Elapsed 0m 35s (remain 1m 27s) Loss: 0.2629 \n",
            "EVAL: [300/701] Elapsed 0m 52s (remain 1m 9s) Loss: 0.2840 \n",
            "EVAL: [400/701] Elapsed 1m 9s (remain 0m 51s) Loss: 0.2712 \n",
            "EVAL: [500/701] Elapsed 1m 26s (remain 0m 34s) Loss: 0.2436 \n",
            "EVAL: [600/701] Elapsed 1m 43s (remain 0m 17s) Loss: 0.2361 \n",
            "EVAL: [700/701] Elapsed 2m 0s (remain 0m 0s) Loss: 0.2566 \n",
            "Post-processing 222 example predictions split into 2801 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - Score: 0.6981196168696169, Train Loss: 0.4093, Val Loss: 0.2566, Time: 3541s\n",
            "Epoch 2 - Save Best Model. score: 0.6981, loss: 0.2566\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.69812\n",
            "========== CV ==========\n",
            "Score: 0.66476\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./models)... Done. 140.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1pdUFYKYmOM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6a3ade1ccab743598b1606e9afa575b3"
          ]
        },
        "outputId": "d2f10fd8-9f74-412c-8e9c-0ac6452e8f4f"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1278<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a3ade1ccab743598b1606e9afa575b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 10727.45MB of 10727.45MB uploaded (13.51MB deduped)\\r'), FloatProgress(value=1.0,…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210914_024111-2cj0dbgj/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210914_024111-2cj0dbgj/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>2</td></tr><tr><td>val_loss/fold0</td><td>0.2812</td></tr><tr><td>score/fold0</td><td>0.65584</td></tr><tr><td>_runtime</td><td>35950</td></tr><tr><td>_timestamp</td><td>1631623221</td></tr><tr><td>_step</td><td>17</td></tr><tr><td>CV_fold0</td><td>0.64735</td></tr><tr><td>val_loss/fold1</td><td>0.25161</td></tr><tr><td>score/fold1</td><td>0.69538</td></tr><tr><td>CV_fold1</td><td>0.65321</td></tr><tr><td>val_loss/fold2</td><td>0.31296</td></tr><tr><td>score/fold2</td><td>0.62927</td></tr><tr><td>CV_fold2</td><td>0.61701</td></tr><tr><td>val_loss/fold3</td><td>0.20916</td></tr><tr><td>score/fold3</td><td>0.70826</td></tr><tr><td>CV_fold3</td><td>0.70826</td></tr><tr><td>val_loss/fold4</td><td>0.25657</td></tr><tr><td>score/fold4</td><td>0.69812</td></tr><tr><td>CV_fold4</td><td>0.69812</td></tr><tr><td>CV</td><td>0.66476</td></tr><tr><td>loss</td><td>0.24994</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁█▁█▁█▁█▁█</td></tr><tr><td>val_loss/fold0</td><td>▁█</td></tr><tr><td>score/fold0</td><td>▁█</td></tr><tr><td>_runtime</td><td>▁▂▂▃▃▃▄▅▅▆▆▆▇█████</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▃▃▄▅▅▆▆▆▇█████</td></tr><tr><td>_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>CV_fold0</td><td>▁</td></tr><tr><td>val_loss/fold1</td><td>▁█</td></tr><tr><td>score/fold1</td><td>▁█</td></tr><tr><td>CV_fold1</td><td>▁</td></tr><tr><td>val_loss/fold2</td><td>▁█</td></tr><tr><td>score/fold2</td><td>▁█</td></tr><tr><td>CV_fold2</td><td>▁</td></tr><tr><td>val_loss/fold3</td><td>█▁</td></tr><tr><td>score/fold3</td><td>▁█</td></tr><tr><td>CV_fold3</td><td>▁</td></tr><tr><td>val_loss/fold4</td><td>█▁</td></tr><tr><td>score/fold4</td><td>▁█</td></tr><tr><td>CV_fold4</td><td>▁</td></tr><tr><td>CV</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 52 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">bumbling-bird-8</strong>: <a href=\"https://wandb.ai/imokuri/chaii/runs/2cj0dbgj\" target=\"_blank\">https://wandb.ai/imokuri/chaii/runs/2cj0dbgj</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}