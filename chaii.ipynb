{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "chaii.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4ZKL964vSc/iUkQAuq+ac",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IMOKURI/chaii-Hindi-and-Tamil-QA/blob/main/chaii.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8HABKC9jNl"
      },
      "source": [
        "# About this notebook ...\n",
        "\n",
        "[chaii - Hindi and Tamil Question Answering](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U4_rc_e9rOY"
      },
      "source": [
        "# Memo\n",
        "\n",
        "\n",
        "\n",
        "## ToDo\n",
        "\n",
        "- [ ] [ラベルノイズ補正](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264395)\n",
        "    - [ ] [これもかな](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266109)\n",
        "- [ ] [Post process でスコアアップ](https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765)\n",
        "- [ ] [位置によるペナルティを課す Loss](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/266832)\n",
        "\n",
        "## Done\n",
        "\n",
        "\n",
        "## Works Well\n",
        "\n",
        "\n",
        "## Doesn't Work\n",
        "\n",
        "\n",
        "## Not To Do\n",
        "\n",
        "- [2つのモデルを作る](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/267604) - [経緯](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264749)\n",
        "\n",
        "\n",
        "## Additional Datasets\n",
        "\n",
        "Search from [here](https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/264581).\n",
        "\n",
        "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) hindi のみ\n",
        "- [Squad_Translated_to_Tamil for Chaii](https://www.kaggle.com/msafi04/squad-translated-to-tamil-for-chaii) tamil のみ\n",
        "\n",
        "\n",
        "## Reference Notebooks\n",
        "\n",
        "- [ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/)\n",
        "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VBhwVS89vKZ"
      },
      "source": [
        "# Prepare for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEN6L1ol9d6d",
        "outputId": "9a96e8a1-3823-4503-b689-6a8d794f3a8e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 12 12:21:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl14Cnli91Ol",
        "outputId": "3506b06b-9fad-49fa-df6a-7f5de7b46ad6"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('init.txt'):\n",
        "    print(\"Already initialized.\")\n",
        "\n",
        "else:\n",
        "    if 'google.colab' in sys.modules:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        dataset_dir = \"/content/drive/MyDrive/Datasets\"\n",
        "\n",
        "        # ====================================================\n",
        "        # Competition datasets\n",
        "        # ====================================================\n",
        "        with zipfile.ZipFile(f\"{dataset_dir}/chaii-hindi-and-tamil-question-answering.zip\", \"r\") as zp:\n",
        "            zp.extractall(path=\"./\")\n",
        "        # with zipfile.ZipFile(f\"{dataset_dir}/chaii-external-data-mlqa-xquad-preprocessing.zip\", \"r\") as zp:\n",
        "        #     zp.extractall(path=\"./\")\n",
        "        # with zipfile.ZipFile(f\"{dataset_dir}/chaii-Squad_Translated_to_Tamil.zip\", \"r\") as zp:\n",
        "        #     zp.extractall(path=\"./\")\n",
        "\n",
        "    # for StratifiedGroupKFold\n",
        "    # !pip uninstall -y scikit-learn\n",
        "    # !pip install --pre --extra-index https://pypi.anaconda.org/scipy-wheels-nightly/simple scikit-learn\n",
        "\n",
        "    # for MultilabelStratifiedKFold\n",
        "    # !pip install -q iterative-stratification\n",
        "\n",
        "    # for CosineAnnealingWarmupRestarts\n",
        "    # !pip install -qU 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup'\n",
        "\n",
        "    !pip install -q wandb\n",
        "    # !pip install -q optuna\n",
        "\n",
        "    # ====================================================\n",
        "    # Competition specific libraries\n",
        "    # ====================================================\n",
        "    !pip install -q transformers\n",
        "    !pip install -q sentencepiece\n",
        "    # !pip install -q textstat\n",
        "    # !pip install -q nlpaug\n",
        "\n",
        "    !touch init.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXvMPm5_4h0"
      },
      "source": [
        "# Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB47hsio_5y6"
      },
      "source": [
        "# General libraries\n",
        "import collections\n",
        "import glob\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import statistics\n",
        "import time\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.cuda.amp as amp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "# from cosine_annealing_warmup import CosineAnnealingWarmupRestarts\n",
        "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error, jaccard_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold  # , StratifiedGroupKFold\n",
        "from torch.optim import SGD, Adam\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyiVJefp4oOi"
      },
      "source": [
        "# Competition specific libraries\n",
        "# import nlpaug.augmenter.word as naw\n",
        "# import nlpaug.augmenter.sentence as nas\n",
        "# import nltk\n",
        "# import textstat\n",
        "import transformers as T"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-snxxwfCAO92"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxr17nzJAS2c"
      },
      "source": [
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxTu8mx-AXcw",
        "outputId": "2fa7fa69-4ea1-4072-e021-eee6a06339bb"
      },
      "source": [
        "netrc = \"/content/drive/MyDrive/.netrc\" if 'google.colab' in sys.modules else \"../input/wandbtoken/.netrc\"\n",
        "!cp -f {netrc} ~/\n",
        "!wandb login\n",
        "\n",
        "wandb_tags = []"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WavcpUepAQOs"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    wandb_tags.append(torch.cuda.get_device_name(0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_GYEnAnAqmJ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbzq6jTIAszZ"
      },
      "source": [
        "DATA_DIR = \"./\" if 'google.colab' in sys.modules else \"../input/chaii-hindi-and-tamil-question-answering/\"\n",
        "OUTPUT_DIR = \"./\"\n",
        "MODEL_DIR = \"./models/\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ey9Vh4CBMgo"
      },
      "source": [
        "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
        "sub = pd.read_csv(DATA_DIR + \"sample_submission.csv\")\n",
        "\n",
        "# external_squad_translated_tamil = pd.read_csv(DATA_DIR + \"squad_translated_tamil.csv\")\n",
        "# external_mlqa = pd.read_csv(DATA_DIR + \"mlqa_hindi.csv\")\n",
        "# external_xquad = pd.read_csv(DATA_DIR + \"xquad.csv\")\n",
        "# external_train = pd.concat([external_mlqa, external_xquad])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm2Lqm0KBeb_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsNi7VmB9_M",
        "outputId": "e81c542f-de9b-45b3-bfb9-8cb0ecd38769"
      },
      "source": [
        "# seed = random.randrange(10000)\n",
        "seed = 440\n",
        "print(seed)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDIEcAvEBdqj"
      },
      "source": [
        "class Config:\n",
        "    wandb_entity = \"imokuri\"\n",
        "    wandb_project = \"chaii\"\n",
        "    print_freq = 100\n",
        "\n",
        "    preprocess = False\n",
        "    train = True\n",
        "    validate = False\n",
        "    inference = False\n",
        "\n",
        "    debug = False\n",
        "    num_debug_data = 50\n",
        "\n",
        "    amp = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI5SEjO0CS8k"
      },
      "source": [
        "config_defaults = {\n",
        "    \"seed\": seed,\n",
        "    # \"n_class\": 1,\n",
        "    \"n_fold\": 5,\n",
        "    \"epochs\": 3,\n",
        "    \"batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"criterion\": \"ChaiiCrossEntropyLoss\",\n",
        "    \"optimizer\": \"BertAdamW\",\n",
        "    \"scheduler\": \"get_cosine_schedule_with_warmup\",\n",
        "    \"lr\": 1e-5,\n",
        "    \"min_lr\": 1e-5,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"model_name\": \"deepset/xlm-roberta-large-squad2\",\n",
        "    \"max_len\": 384,\n",
        "    \"doc_stride\": 128,\n",
        "    \"dropout\": 0.0,\n",
        "    \"layer_norm_eps\": 1e-7,\n",
        "    \"init_weights\": True,\n",
        "    # \"reinit_layers\": 0,\n",
        "    # \"freeze_layers\": 0,\n",
        "    \"datasets\": [\n",
        "        \"rhtsingh-mlqa_hindi:v0\",\n",
        "        \"rhtsingh-xquad:v0\",\n",
        "        \"squad_translated_tamil:v0\",\n",
        "    ],\n",
        "    \"runs\": [\n",
        "    ],\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7p8-C9cDVuD"
      },
      "source": [
        "if config_defaults[\"optimizer\"] == \"BertAdamW\":\n",
        "    config_defaults[\"lr_second\"] = 2e-5\n",
        "    config_defaults[\"lr_third\"] = 5e-5"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Pk66rRDTGC"
      },
      "source": [
        "if Config.debug:\n",
        "    config_defaults[\"epochs\"] = 1\n",
        "    Config.print_freq = 10"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9WtvbYBtfN"
      },
      "source": [
        "if Config.train:\n",
        "    wandb_job_type = \"training\"\n",
        "\n",
        "elif Config.inference:\n",
        "    wandb_job_type = \"inference\"\n",
        "\n",
        "elif Config.validate:\n",
        "    wandb_job_type = \"validation\"\n",
        "\n",
        "elif Config.preprocess:\n",
        "    wandb_job_type = \"preprocess\"\n",
        "\n",
        "else:\n",
        "    wandb_job_type = \"\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2rVE3iK7Xaf"
      },
      "source": [
        "if Config.debug:\n",
        "    wandb_tags.append(\"debug\")\n",
        "    \n",
        "if Config.amp:\n",
        "    wandb_tags.append(\"amp\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "9kskKEy9Dyqk",
        "outputId": "0d4fe56c-43d0-4e3e-b948-bcfff8d0b4cd"
      },
      "source": [
        "if Config.debug:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        tags=wandb_tags,\n",
        "        mode=\"disabled\",\n",
        "    )\n",
        "else:\n",
        "    run = wandb.init(\n",
        "        entity=Config.wandb_entity,\n",
        "        project=Config.wandb_project,\n",
        "        config=config_defaults,\n",
        "        job_type=wandb_job_type,\n",
        "        tags=wandb_tags,\n",
        "        save_code=True,\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimokuri\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">deep-meadow-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/imokuri/chaii\" target=\"_blank\">https://wandb.ai/imokuri/chaii</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/imokuri/chaii/runs/ncxn0f77\" target=\"_blank\">https://wandb.ai/imokuri/chaii/runs/ncxn0f77</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210912_122143-ncxn0f77</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev3bvwDvEMuS"
      },
      "source": [
        "config = wandb.config"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1pXRWoHy1_"
      },
      "source": [
        "# Load Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSYiJXBD4oy"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_data = []\n",
        "    for name_version in config.datasets:\n",
        "        name, version = name_version.split(\":\")\n",
        "        os.makedirs(name, exist_ok=True)\n",
        "\n",
        "        if Config.debug:\n",
        "            artifact_path = f\"{Config.wandb_entity}/{Config.wandb_project}/{name_version}\"\n",
        "            api = wandb.Api()\n",
        "            artifact = api.artifact(artifact_path)\n",
        "\n",
        "        else:\n",
        "            artifact_path = f\"{name_version}\"\n",
        "            artifact = run.use_artifact(artifact_path)\n",
        "\n",
        "        artifact.download(name)\n",
        "\n",
        "        df = pd.read_csv(f\"{name}/{name}.csv\")\n",
        "        external_data.append(df)\n",
        "\n",
        "    external_train = pd.concat(external_data)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JepYxTDVFarm"
      },
      "source": [
        "# EDA-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGORKcNtFcAk",
        "outputId": "7409add3-8a70-41d7-cb7f-ade8ab305eb1"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df.isnull().sum())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "id              0\n",
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "id          0\n",
            "context     0\n",
            "question    0\n",
            "language    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvCjtVwvGacB",
        "outputId": "f99960d3-0aee-4f26-afbc-1eb495c79f27"
      },
      "source": [
        "for df in [train, test]:\n",
        "    print(f\"=\" * 120)\n",
        "    print(df[\"language\"].value_counts())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "hindi    746\n",
            "tamil    368\n",
            "Name: language, dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    3\n",
            "tamil    2\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUXUouHDHa-i",
        "outputId": "58d9ffea-f9b2-49a3-85ee-460003b6130b"
      },
      "source": [
        "if config.datasets != []:\n",
        "    print(external_train.isnull().sum())\n",
        "    print(f\"=\" * 120)\n",
        "    print(external_train[\"language\"].value_counts())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context         0\n",
            "question        0\n",
            "answer_text     0\n",
            "answer_start    0\n",
            "language        0\n",
            "dtype: int64\n",
            "========================================================================================================================\n",
            "hindi    6615\n",
            "tamil    3567\n",
            "Name: language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW1WEUKWEPrV"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0_3tcXS_AEO"
      },
      "source": [
        "def convert_answers(row):\n",
        "    return {'answer_start': [row[0]], 'text': [row[1]]}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mGJYUQ0liem"
      },
      "source": [
        "def correct_labels(df):\n",
        "    df.loc[df['id'] == '', 'answer_text'] = ''\n",
        "    df.loc[df['id'] == '', 'answer_start'] = 0\n",
        "    pass"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdjatcP_FJ9r"
      },
      "source": [
        "def get_train_data(train):\n",
        "    train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)\n",
        "\n",
        "    return train"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-A8bRGjFVMw"
      },
      "source": [
        "def get_test_data(test):\n",
        "\n",
        "    return test"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yae6ysRGvMH"
      },
      "source": [
        "train = get_train_data(train)\n",
        "\n",
        "if config.datasets != []:\n",
        "    external_train = get_train_data(external_train)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-okotQ7GwJT"
      },
      "source": [
        "test = get_test_data(test)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54joajJkBRbm"
      },
      "source": [
        "### External Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8RSTX1SBZef"
      },
      "source": [
        "# 前処理\n",
        "if Config.preprocess:\n",
        "    external_squad_translated_tamil[\"language\"] = \"tamil\"\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0bvUgZyzasg"
      },
      "source": [
        "# dataset 保存\n",
        "if Config.preprocess:\n",
        "    !mkdir -p squad_translated_tamil\n",
        "    external_squad_translated_tamil.to_csv(\"squad_translated_tamil/squad_translated_tamil.csv\", index=False)\n",
        "    artifact = wandb.Artifact('squad_translated_tamil', type='dataset')\n",
        "    artifact.add_dir(\"squad_translated_tamil/\")\n",
        "    run.log_artifact(artifact)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEkueCnpHpGW"
      },
      "source": [
        "# EDA-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CEM8_FTTHqrM",
        "outputId": "99dbc384-7c0e-49e4-9545-6fe73435a474"
      },
      "source": [
        "for df in [train, test, sub]:\n",
        "    print(f\"=\" * 120)\n",
        "    df.info()\n",
        "    display(df.head())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1114 entries, 0 to 1113\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   id            1114 non-null   object\n",
            " 1   context       1114 non-null   object\n",
            " 2   question      1114 non-null   object\n",
            " 3   answer_text   1114 non-null   object\n",
            " 4   answer_start  1114 non-null   int64 \n",
            " 5   language      1114 non-null   object\n",
            " 6   answers       1114 non-null   object\n",
            "dtypes: int64(1), object(6)\n",
            "memory usage: 61.0+ KB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>language</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>903deec17</td>\n",
              "      <td>ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...</td>\n",
              "      <td>மனித உடலில் எத்தனை எலும்புகள் உள்ளன?</td>\n",
              "      <td>206</td>\n",
              "      <td>53</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [53], 'text': ['206']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d9841668c</td>\n",
              "      <td>காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...</td>\n",
              "      <td>காளிதாசன் எங்கு பிறந்தார்?</td>\n",
              "      <td>காசுமீரில்</td>\n",
              "      <td>2358</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [2358], 'text': ['காசுமீரில்']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29d154b56</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...</td>\n",
              "      <td>பென்சிலின் கண்டுபிடித்தவர் யார்?</td>\n",
              "      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங்</td>\n",
              "      <td>0</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41660850a</td>\n",
              "      <td>குழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...</td>\n",
              "      <td>தமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...</td>\n",
              "      <td>தாலாட்டு</td>\n",
              "      <td>68</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [68], 'text': ['தாலாட்டு']}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b29c82c22</td>\n",
              "      <td>சூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...</td>\n",
              "      <td>பூமியின் அருகில் உள்ள விண்மீன் எது?</td>\n",
              "      <td>சூரியனும்</td>\n",
              "      <td>585</td>\n",
              "      <td>tamil</td>\n",
              "      <td>{'answer_start': [585], 'text': ['சூரியனும்']}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ...                                            answers\n",
              "0  903deec17  ...            {'answer_start': [53], 'text': ['206']}\n",
              "1  d9841668c  ...   {'answer_start': [2358], 'text': ['காசுமீரில்']}\n",
              "2  29d154b56  ...  {'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...\n",
              "3  41660850a  ...       {'answer_start': [68], 'text': ['தாலாட்டு']}\n",
              "4  b29c82c22  ...     {'answer_start': [585], 'text': ['சூரியனும்']}\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        5 non-null      object\n",
            " 1   context   5 non-null      object\n",
            " 2   question  5 non-null      object\n",
            " 3   language  5 non-null      object\n",
            "dtypes: object(4)\n",
            "memory usage: 288.0+ bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>ज्वाला गुट्टा (जन्म: 7 सितंबर 1983; वर्धा, महा...</td>\n",
              "      <td>ज्वाला गुट्टा की माँ का नाम क्या है</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>गूगल मानचित्र (Google Maps) (पूर्व में गूगल लो...</td>\n",
              "      <td>गूगल मैप्स कब लॉन्च किया गया था?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>गुस्ताव रॉबर्ट किरचॉफ़ (१२ मार्च १८२४ - १७ अक्...</td>\n",
              "      <td>गुस्ताव किरचॉफ का जन्म कब हुआ था?</td>\n",
              "      <td>hindi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>அலுமினியம் (ஆங்கிலம்: அலுமினியம்; வட அமெரிக்க ...</td>\n",
              "      <td>அலுமினியத்தின் அணு எண் என்ன?</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>கூட்டுறவு இயக்க வரலாறு, இங்கிலாந்து  நாட்டில் ...</td>\n",
              "      <td>இந்தியாவில் பசுமை புரட்சியின் தந்தை என்று கருத...</td>\n",
              "      <td>tamil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... language\n",
              "0  22bff3dec  ...    hindi\n",
              "1  282758170  ...    hindi\n",
              "2  d60987e0e  ...    hindi\n",
              "3  f99c770dc  ...    tamil\n",
              "4  40dec1964  ...    tamil\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   id                5 non-null      object \n",
            " 1   PredictionString  0 non-null      float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 208.0+ bytes\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>PredictionString</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22bff3dec</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>282758170</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d60987e0e</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f99c770dc</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40dec1964</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  PredictionString\n",
              "0  22bff3dec               NaN\n",
              "1  282758170               NaN\n",
              "2  d60987e0e               NaN\n",
              "3  f99c770dc               NaN\n",
              "4  40dec1964               NaN"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODUeiq5P_O8s"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyZ9-uMH2gK"
      },
      "source": [
        "# CV Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM6xvuGssH-4"
      },
      "source": [
        "if Config.debug:\n",
        "    train = train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if config.datasets != []:\n",
        "        external_train = external_train.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "    if len(sub) > Config.num_debug_data:\n",
        "        test = test.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)\n",
        "        sub = sub.sample(n=Config.num_debug_data, random_state=config.seed).reset_index(drop=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XnW0e1AH4Cn",
        "outputId": "1ea6fde5-a97a-427c-d8c5-8077918f7a98"
      },
      "source": [
        "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"language\"])):\n",
        "    train.loc[val_index, \"fold\"] = int(n)\n",
        "train[\"fold\"] = train[\"fold\"].astype(np.int8)\n",
        "print(train.groupby([\"fold\", \"language\"]).size())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold  language\n",
            "0     hindi       149\n",
            "      tamil        74\n",
            "1     hindi       149\n",
            "      tamil        74\n",
            "2     hindi       149\n",
            "      tamil        74\n",
            "3     hindi       150\n",
            "      tamil        73\n",
            "4     hindi       149\n",
            "      tamil        73\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W902aP490hEk"
      },
      "source": [
        "if config.datasets != []:\n",
        "    external_train[\"fold\"] = -1\n",
        "    external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
        "    train = pd.concat([train, external_train]).reset_index(drop=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD0991CRIMH-"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veHPCKXQIJRv"
      },
      "source": [
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    LOGGER.info(f\"[{name}] start\")\n",
        "    yield\n",
        "    LOGGER.info(f\"[{name}] done in {time.time() - t0:.0f} s.\")\n",
        "\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
        "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
        "\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "seed_torch(seed=config.seed)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTjVqALnIXKQ"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqzv3qSpITRU"
      },
      "source": [
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, df, model_name, include_labels=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = T.AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.features = []\n",
        "        if include_labels:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_train_features(row)\n",
        "        else:\n",
        "            for i, row in df.iterrows():\n",
        "                self.features += self.prepare_test_features(row)\n",
        "\n",
        "        self.include_labels = include_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        feature = self.features[item]\n",
        "\n",
        "        if self.include_labels:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                # 'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
        "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
        "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
        "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
        "                'offset_mapping':feature['offset_mapping'],\n",
        "                'sequence_ids':feature['sequence_ids'],\n",
        "                'id':feature['example_id'],\n",
        "                'context': feature['context'],\n",
        "                'question': feature['question']\n",
        "            }\n",
        "\n",
        "    def prepare_train_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
        "        offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
        "\n",
        "        features = []\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "\n",
        "            input_ids = tokenized_example[\"input_ids\"][i]\n",
        "            attention_mask = tokenized_example[\"attention_mask\"][i]\n",
        "\n",
        "            feature['input_ids'] = input_ids\n",
        "            feature['attention_mask'] = attention_mask\n",
        "            feature['offset_mapping'] = offsets\n",
        "\n",
        "            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n",
        "            sequence_ids = tokenized_example.sequence_ids(i)\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in sequence_ids]\n",
        "\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = example[\"answers\"]\n",
        "\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                feature[\"start_position\"] = cls_index\n",
        "                feature[\"end_position\"] = cls_index\n",
        "            else:\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != 1:\n",
        "                    token_start_index += 1\n",
        "\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != 1:\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    feature[\"start_position\"] = cls_index\n",
        "                    feature[\"end_position\"] = cls_index\n",
        "                else:\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    feature[\"start_position\"] = token_start_index - 1\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    feature[\"end_position\"] = token_end_index + 1\n",
        "\n",
        "            features.append(feature)\n",
        "        return features\n",
        "\n",
        "    def prepare_test_features(self, example):\n",
        "        example[\"question\"] = example[\"question\"].lstrip()\n",
        "        tokenized_example = self.tokenizer(\n",
        "            example[\"question\"],\n",
        "            example[\"context\"],\n",
        "            truncation=\"only_second\",\n",
        "            max_length=config.max_len,\n",
        "            stride=config.doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        features = []\n",
        "        for i in range(len(tokenized_example[\"input_ids\"])):\n",
        "            feature = {}\n",
        "            feature[\"example_id\"] = example['id']\n",
        "            feature['context'] = example['context']\n",
        "            feature['question'] = example['question']\n",
        "            feature['input_ids'] = tokenized_example['input_ids'][i]\n",
        "            feature['attention_mask'] = tokenized_example['attention_mask'][i]\n",
        "            feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n",
        "            feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n",
        "            features.append(feature)\n",
        "        return features"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77rjufTiUtJb",
        "outputId": "901ce803-79fc-415c-c3a1-06dfabc50b10"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    train_ds = BaseDataset(train, config.model_name)\n",
        "    print(train_ds[0])\n",
        "    print(len(train_ds))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([     0,  69535,  81049,  37368, 153264,  12095,  52989,  21883,   1629,\n",
            "        145615,     32,      2,      2,   3219, 224013, 124335,   5966,  69535,\n",
            "          4930,  74149,  12095,  52989,  21883, 182394,   3686,  51833,  57210,\n",
            "        101912,     15,   6161,   2912,  70597,  52989,  21883, 102080,  54512,\n",
            "         91585,   1962, 212933,  18599,  16242,  94236,     16, 198236,  29160,\n",
            "         12095,  52989,  21883, 173139,  23618,  72817,      5,   5894, 198236,\n",
            "         81049,  37334, 144257,   7827,  82890,  84853,  80517, 114452, 232094,\n",
            "          3686,  17984,  11830,  62001, 182394,   4167,      5, 203312,  10753,\n",
            "         50667,   2650,      4,  45303,   1962, 163062, 198236,  29160, 176030,\n",
            "         15453,      4,   3219, 171093,   5944,   2650,   8120,  10175,  12095,\n",
            "         52989,  21883,     15,   2650,  24183,   5638,  14861,     16,  56735,\n",
            "          3219, 171093,   5944,   2650,  12009, 145578,  10832,   2802,   2650,\n",
            "         26873,  52989,  21883,  53336,  26415,  38640,  31067,     74, 105457,\n",
            "          5966,  22050,  12095,  52989,  21883, 202342,  59386,  12095,  52989,\n",
            "         13070,   2650,   1962,  39311,   9654,  37964,  18806,      4, 196586,\n",
            "        105457,   5966,  22467,  78876,  52989,  21883,     74, 102080,   6896,\n",
            "            20,  21162,  14233,  94097,   4864,  12152,  12095,  52989,  21883,\n",
            "          1629, 210828,   1381, 198236,   2782, 191297,  10832,   2802,   2650,\n",
            "         26873,  52989,  21883,   1629,   3912,  59987,   1962, 212933,  26415,\n",
            "         20567,      5,  69535,   9696, 184936,  28258,  89933,   1039,  12095,\n",
            "         52989,  21883,   1629,     15,  10753,   2802,   6149,  11674,  16043,\n",
            "         26873,   2913,  21883, 202342, 137010,     16, 145615,     74, 164359,\n",
            "         12095,  11993, 181576,  87783,  35186,     15,  15182,  53208,     16,\n",
            "         12095,  52989,  21883,  91585,  11772,    616,  71987,  12095,  52989,\n",
            "         21883,  91585,  11772,     15,   1021,  26136,  32881,      7,     16,\n",
            "         73417,  73949, 163493,      5,     15,   4875,   5427,   3770, 136259,\n",
            "          1629,  88115,   2650, 230988, 112578,  53336,   4167, 136259, 173139,\n",
            "             6,  79464,   2798, 143825,      5,     16, 181576,  87783,  35186,\n",
            "         12095,  52989,  21883,   1629,  10021,    106, 190707,   4875, 128236,\n",
            "         52989,  21883,     15,  20549,    289,  32881,     16,    116,  14184,\n",
            "          3937, 145181,  52989,  21883,     15,  24980,     13,   1803,  32881,\n",
            "            16,   1737,    138, 116180,  17056,   3686,   4875, 128236,  52989,\n",
            "         21883,     15,  99736,    289,  32881,     16,   1737,    201,  22262,\n",
            "         95344,  12095,  52989,  21883,     15,   6652,  88354,    289,  32881,\n",
            "            16,   6001,   6343,   8850,  12095,  52989,  21883,     15,      7,\n",
            "         88322,  48899,  32881,     16,  60070,  12784,   2782,  35424,  26873,\n",
            "         52989,  21883,     15,  12421,    432,    532,  32881,     16,  71987,\n",
            "         12095,  52989,  21883,   1629,  31203,    361, 145578,  51153,  18805,\n",
            "         12095,  52989,  21883,     15,  12018,  28236,     16,    305, 177292,\n",
            "         95424,  18805,  12095,  52989,  21883,     15,  24084,   2298,     16,\n",
            "          1737,   2690,  42353,  78876,  52989,  21883,     15,  16917,  10325,\n",
            "         32881,     16,   1737,    190,   6390,  62481,  12095,  52989,  21883,\n",
            "            15,   3285,    519,  47148,  32881,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start_position': tensor(27), 'end_position': tensor(27)}\n",
            "26881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBEpGYP-kjH",
        "outputId": "9cc48e92-85ce-4521-f762-f82ddcf1461f"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    test_ds = BaseDataset(test, config.model_name, include_labels=False)\n",
        "    print(test_ds[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([     0,      6,  38033,  91262,  20546,  85149,    471,  58380,    641,\n",
            "          8062,   6004,    460,      2,      2,      6,  38033,  91262,  20546,\n",
            "         85149,     15, 206327,     12,    361, 182198,  26819,     74,      6,\n",
            "        196859,   1026,      4,  15297,     16,    967,   9261,  41162, 156793,\n",
            "         64382,  46297, 103766,   1404,   7294,   1293,    125, 222600,   5725,\n",
            "         11515,      6,  38033,  91262,  20546,  85149,    641,  22274,    361,\n",
            "        182198,  26819,    629,      6, 196859,   1026,      4,  15297,    421,\n",
            "         11645,   3813,    125,  24939,  62573,  39477,      5, 233097,   4429,\n",
            "         45951,  25594,    871,  56980,   4149,   1471,    998,  23263,    646,\n",
            "          1293,    125,  35056,  56980,   4149,   1471,    998,  20546,  85149,\n",
            "         47211,  12797,  27193,    421,   5564, 220307,   9331,    287,   3765,\n",
            "          3946,  13430, 120116,    125,      6,  38033,  91262,  20546,  85149,\n",
            "           471, 222600,   5725,  53812,   9729, 204778,    646,  17035,    871,\n",
            "         33171,    838,    646,  21191,  41162, 156793,  64382,  46297,  12189,\n",
            "          1748,   1780,  26252,   4029,    125,  82645, 209393,    209,   9512,\n",
            "           471,  78087,    646,   2093,      6,  38033,  91262,  20546,  85149,\n",
            "          1142,  31160,      5,  31598,      5,  33142, 160674,    646,  77022,\n",
            "         10067, 166922,  26252,   1896,   8785,   3813,    125,  31160,      5,\n",
            "         31598,      5,  33142, 160674,   3946,    287,  13325,   4592,   1187,\n",
            "         12189, 201742,   1293, 207411,   8771,  74163,   4846, 157426, 138314,\n",
            "           646, 191604,   4029,   5349,    460,    125,  47211,  12797,    702,\n",
            "          9512,    471,  78087,    421,  21191,  22009,   4415, 188408,  41162,\n",
            "        156793,  64382,  46297, 115739,  77666,  51476, 151576,  41657,    659,\n",
            "          9917,    125,   9512,   3576,    421,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,    729,   9512,    471,  78087,    421,  70159,  85134,\n",
            "        188408,  41162, 156793,  64382,  46297, 115739,  77666,  51476, 151576,\n",
            "         41657,    659,    125,  64021,   9512,  21191,      6, 170555,   3558,\n",
            "        159967,  51476,    287,   3765, 227649,   6473,    421,  91298,    659,\n",
            "         17837,   2617,   9179,  73457,    287, 227649,   6473,  70159,  85134,\n",
            "        188408,  41162, 156793,  64382,  46297, 115739,  77666,  51476, 151576,\n",
            "           871,  13371,    998,  85134, 188408,  41162, 156793,  64382,  46297,\n",
            "        115739,  77666,  51476, 151576,    421,  41657,  76613,    471,    125,\n",
            "             6, 170555,   3558, 159967,  51476,    287,   3765,  35056,  91298,\n",
            "           659,  52170, 195730,   6927,   7231, 192872,    125,   5726,    646,\n",
            "          2021,   7231,  73451,  32534,  12797,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,  73457,    287, 188408, 115474,   1471,  73254,    421,\n",
            "         41657,  76613,    471,    125,  39556,   9236, 227649,   6473,    287,\n",
            "          3765,      9, 105456,      6,  38033,  91262,  20546,  85149,   1142,\n",
            "        119253,   3282, 227649,   6473,    421,   1780,  67963,  76613,    471,\n",
            "           871,   3946,    471, 227649,   6473,    421,  13353, 203104, 160161,\n",
            "        118689,    838,    125,  54968,   1532,  59308,  26609,   8683,  13056,\n",
            "          8531, 156180,   6473,    421,   1780,      6,  38033,  91262,  20546,\n",
            "         85149,   1142,   5564, 221876,   2139,      2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'offset_mapping': [(0, 0), (0, 1), (0, 2), (2, 6), (6, 9), (9, 13), (13, 16), (16, 20), (20, 23), (23, 27), (27, 32), (32, 35), (0, 0), (0, 0), (0, 1), (0, 2), (2, 6), (6, 9), (9, 13), (13, 15), (15, 19), (19, 20), (20, 22), (22, 29), (29, 34), (34, 35), (35, 36), (36, 40), (40, 41), (41, 42), (42, 53), (53, 54), (54, 57), (57, 64), (64, 67), (67, 69), (69, 71), (71, 73), (73, 76), (76, 78), (78, 80), (80, 84), (84, 85), (87, 95), (95, 97), (97, 102), (103, 104), (104, 106), (106, 110), (110, 113), (113, 117), (117, 120), (120, 125), (125, 127), (127, 134), (134, 139), (139, 142), (142, 143), (143, 147), (147, 148), (148, 149), (149, 160), (160, 164), (164, 168), (168, 171), (171, 172), (172, 177), (177, 182), (182, 185), (185, 186), (186, 194), (194, 197), (197, 199), (199, 201), (201, 204), (204, 208), (208, 211), (211, 212), (212, 213), (213, 217), (217, 220), (220, 224), (224, 225), (225, 230), (230, 234), (234, 237), (237, 238), (238, 239), (239, 242), (242, 246), (246, 251), (251, 255), (255, 260), (260, 264), (264, 269), (269, 274), (274, 277), (277, 280), (280, 284), (284, 289), (289, 292), (292, 296), (296, 297), (297, 298), (298, 300), (300, 304), (304, 307), (307, 311), (311, 314), (314, 322), (322, 324), (324, 328), (328, 330), (330, 339), (339, 342), (342, 346), (346, 349), (349, 353), (353, 354), (354, 357), (357, 366), (366, 369), (369, 371), (371, 373), (373, 375), (375, 379), (379, 381), (381, 384), (384, 389), (389, 394), (394, 395), (397, 400), (400, 404), (405, 408), (408, 412), (412, 415), (415, 420), (420, 423), (423, 426), (426, 427), (427, 429), (429, 433), (433, 436), (436, 440), (440, 443), (443, 446), (446, 447), (447, 449), (449, 450), (450, 453), (453, 455), (455, 458), (458, 464), (464, 467), (467, 472), (472, 477), (477, 480), (480, 485), (485, 488), (488, 489), (489, 492), (492, 493), (493, 495), (495, 496), (496, 499), (499, 501), (501, 506), (506, 509), (509, 514), (514, 517), (517, 519), (519, 523), (523, 533), (533, 537), (537, 545), (545, 547), (547, 550), (550, 551), (551, 557), (557, 564), (564, 567), (567, 576), (576, 581), (581, 585), (585, 588), (588, 589), (589, 594), (594, 598), (598, 601), (601, 605), (605, 608), (608, 613), (613, 617), (617, 626), (626, 629), (629, 631), (631, 637), (637, 640), (640, 642), (642, 644), (644, 646), (646, 649), (649, 651), (651, 654), (654, 657), (657, 661), (661, 662), (662, 665), (665, 666), (666, 670), (670, 675), (675, 679), (679, 680), (680, 682), (682, 686), (686, 689), (689, 693), (693, 696), (696, 699), (699, 703), (703, 706), (706, 711), (711, 715), (715, 719), (719, 722), (722, 728), (728, 731), (731, 733), (733, 735), (735, 737), (737, 740), (740, 742), (742, 745), (745, 748), (748, 752), (752, 753), (753, 754), (754, 758), (758, 762), (762, 771), (771, 772), (772, 777), (777, 778), (778, 782), (782, 785), (785, 788), (788, 792), (792, 796), (796, 798), (798, 802), (802, 807), (807, 808), (808, 812), (812, 814), (814, 818), (818, 826), (826, 829), (829, 833), (833, 835), (835, 839), (839, 842), (842, 848), (848, 851), (851, 853), (853, 855), (855, 857), (857, 860), (860, 862), (862, 865), (865, 868), (868, 871), (871, 874), (874, 875), (875, 878), (878, 884), (884, 887), (887, 889), (889, 891), (891, 893), (893, 896), (896, 898), (898, 901), (901, 904), (904, 908), (908, 912), (912, 918), (918, 921), (921, 922), (922, 923), (923, 928), (928, 929), (929, 933), (933, 936), (936, 939), (939, 943), (943, 948), (948, 953), (953, 954), (954, 959), (959, 964), (964, 968), (968, 971), (971, 975), (975, 976), (976, 981), (981, 984), (984, 989), (989, 992), (992, 999), (999, 1003), (1003, 1007), (1007, 1008), (1008, 1010), (1010, 1014), (1014, 1017), (1017, 1021), (1021, 1024), (1024, 1032), (1032, 1035), (1035, 1041), (1041, 1045), (1045, 1046), (1046, 1058), (1058, 1062), (1062, 1066), (1066, 1072), (1072, 1075), (1075, 1076), (1076, 1079), (1079, 1085), (1085, 1089), (1089, 1091), (1091, 1094), (1094, 1098), (1098, 1099), (1099, 1102), (1102, 1103), (1103, 1105), (1105, 1109), (1109, 1112), (1112, 1116), (1116, 1119), (1119, 1125), (1125, 1127), (1127, 1131), (1131, 1133), (1133, 1137), (1137, 1140), (1140, 1146), (1146, 1152), (1152, 1155), (1155, 1158), (1158, 1163), (1163, 1166), (1166, 1170), (1170, 1172), (1172, 1176), (1176, 1181), (1181, 1189), (1189, 1197), (1197, 1201), (1201, 1202), (1202, 1203), (1203, 1206), (1206, 1211), (1211, 1214), (1214, 1216), (1216, 1218), (1218, 1220), (1220, 1221), (1221, 1225), (1225, 1227), (1227, 1231), (1231, 1234), (1234, 1235), (1235, 1237), (1237, 1241), (1241, 1244), (1244, 1248), (1248, 1251), (1251, 1256), (1256, 1264), (1264, 1266), (0, 0)], 'sequence_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'id': '22bff3dec', 'context': 'ज्वाला गुट्टा (जन्म: 7 सितंबर 1983; वर्धा, महाराष्ट्र) एक भारतीय बैडमिंटन खिलाडी हैं। \\n प्रारंभिक जीवन \\nज्वाला गुट्टा का जन्म 7 सितंबर 1983 को वर्धा, महाराष्ट्र में हुआ था। उनके पिता एम. क्रांति तेलुगु और मां येलन चीन से हैं। उनकी मां येलन गुट्टा पहली बार 1977 में अपने दादा जी के साथ भारत आई थीं। ज्वाला गुट्टा की प्रारंभिक पढ़ाई हैदराबाद से हुई और यहीं से उन्होंने बैडमिंटन खेलना भी शुरू किया। \\n कॅरियर \\n10 साल की उम्र से ही ज्वाला गुट्टा ने एस.एम. आरिफ से ट्रेनिंग लेना शुरू कर दिया था। एस.एम. आरिफ भारत के जाने माने खेल प्रशिक्षक हैं जिन्हें द्रोणाचार्य अवार्ड से सम्मानित किया गया है। पहली बार 13 साल की उम्र में उन्होंने मिनी नेशनल बैडमिंटन चैंपियनशिप जीती थी। साल 2000 में ज्वाला गुट्टा ने 17 साल की उम्र में जूनियर नेशनल बैडमिंटन चैंपियनशिप जीती। इसी साल उन्होंने श्रुति कुरियन के साथ डबल्स में जोड़ी बनाते हुए महिलाओं के डबल्स जूनियर नेशनल बैडमिंटन चैंपियनशिप और सीनियर नेशनल बैडमिंटन चैंपियनशिप में जीत हासिल की। श्रुति कुरियन के साथ उनकी जोड़ी काफी लंबे समय तक चली। 2002 से 2008 तक लगातार सात बार ज्वाला गुट्टा ने महिलाओं के नेशनल युगल प्रतियोगिता में जीत हासिल की।[2]\\nमहिला डबल्स के साथ-साथ ज्वाला गुट्टा ने मिश्रित डबल्स में भी सफलता हासिल की और भारत की डबल्स में सबसे बेहतरीन खिलाड़ी बनीं।[3] 2010 कॉमनवेल्थ गेम्स में भी ज्वाला गुट्टा ने अपने पार्टनर अश्विनी पोनप्पा के साथ भारत के लिए स्वर्ण पदक जीता। कॉमनवेल्थ गेम्स के बाद से एक बार फिर ज्वाला गुट्टा भारतीय बैडमिंटन में चर्चा का विषय बन गई हैं।[4][5]\\nग्लासगो में आयोजित कॉमनवेल्थ गेम्स, 2014 में ज्वाला गुट्टा ने स्वर्ण पदक हासिल किया।\\n व्यक्तिगत जीवन \\nमैदान पर बाएं हाथ से तेज-तर्रार शॉट लगाने वाली ज्वाला निजी जिंदगी में भी काफी तेज और चर्चाओं में छाई रहती हैं। ज्वाला ने 2005 में बैडमिंटन खिलाड़ी चेतन आनंद से शादी की थी, 29 जून 2011 को उन्होंने अपने पति पूर्व बैडमिंटन खिलाड़ी चेतन आनंद से तलाक लिया है। चेतन आनंद भी एक बेहतरीन भारतीय बैडमिंटन खिलाड़ी हैं।\\n फिल्मोग्राफी \\nGunde Jaari Gallanthayyinde[6] \\nफुगली (2014)\\n उपलब्धियां \\nरिकॉर्ड 13 बार नेशनल बैडमिंटन चैंपियनशिप की विजेता। \\nभारत की सबसे बेहतरीन डबल्स प्लेयर। \\nसाल 2011 में उन्हें “अर्जुन पुरस्कार” से सम्मानित किया गया। \\nराष्ट्रमंडल खेल, 2014 (ग्लासगो) में स्वर्ण पदक जीता। \\n चित्र दीर्घा \\n\\n\\nवी दीजू और ज्वाला गुट्टा\\nकेबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\\nकेबीसी के सेट पर सुशील कुमार, ज्वाला गुट्टा, लिएंडर पेस, श्रीसंत\\n\\n सन्दर्भ \\n\\n बाहरी कड़ियाँ \\n\\n\\n\\n\\n\\nश्रेणी:हिन्द की बेटियाँ\\nश्रेणी:विकिपरियोजना हिन्द की बेटियाँ\\nश्रेणी:भारत के खिलाड़ी\\nश्रेणी:1983 में जन्मे लोग\\nश्रेणी:जीवित लोग\\nश्रेणी:भारतीय महिला बैडमिंटन खिलाड़ी\\nश्रेणी:राष्ट्रमंडल खेलों के पदक प्राप्तकर्ता\\nश्रेणी:महाराष्ट्र के लोग\\nश्रेणी:बैडमिंटन खिलाड़ी', 'question': 'ज्वाला गुट्टा की माँ का नाम क्या है'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4zuBuCCAv-8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tJieX2AAueP",
        "outputId": "4cb9ed1e-45db-4132-d746-e340d7144c19"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    print(T.AutoConfig.from_pretrained(config.model_name))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLMRobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"language\": \"english\",\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"name\": \"XLMRoberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZtghUknA1b8"
      },
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "\n",
        "        self.auto_config = T.AutoConfig.from_pretrained(model_name)\n",
        "        self.auto_config.update({\n",
        "            \"hidden_dropout_prob\": config.dropout,\n",
        "            \"layer_norm_eps\": config.layer_norm_eps,\n",
        "        })\n",
        "\n",
        "        self.auto_model = T.AutoModel.from_pretrained(model_name, config=self.auto_config)\n",
        "\n",
        "        self.qa_outputs = nn.Linear(self.auto_config.hidden_size, 2)\n",
        "\n",
        "        if config.init_weights:\n",
        "            self._init_weights(self.qa_outputs)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.auto_config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(\n",
        "        self, \n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        # token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.auto_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        \n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        \n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M92xPyOyCSQZ",
        "outputId": "5a2564d4-c23e-4375-9fad-4e38eabce3fe"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    model = BaseModel(config.model_name)\n",
        "    print(model)\n",
        "\n",
        "    train_dataset = BaseDataset(train, config.model_name)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "    for features in train_loader:\n",
        "        output = model(features[\"input_ids\"], features[\"attention_mask\"])\n",
        "        print(output)\n",
        "        break"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModel(\n",
            "  (auto_model): XLMRobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 1024)\n",
            "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (12): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (13): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (14): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (15): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (16): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (17): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (18): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (19): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (20): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (21): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (22): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (23): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): RobertaPooler(\n",
            "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
            ")\n",
            "(tensor([[ 0.4347, -0.5973, -0.3525,  ...,  0.3601,  0.0522, -0.1170],\n",
            "        [ 0.5268, -0.5712,  0.0193,  ..., -0.1326,  0.1418, -0.0029],\n",
            "        [ 0.5423, -0.5091, -0.3287,  ...,  0.2318, -0.0871,  0.0593],\n",
            "        [ 0.6198, -0.0092, -0.0458,  ...,  0.2521,  0.1111,  0.0767]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 0.8080,  0.5062,  0.1613,  ...,  0.6447, -0.3005,  0.4472],\n",
            "        [ 0.7734,  0.2550,  0.1986,  ...,  0.0394,  0.2994,  0.4758],\n",
            "        [ 0.6555, -0.0380,  0.0295,  ...,  0.2900,  0.6517,  0.3815],\n",
            "        [ 0.4939,  0.2416,  0.0213,  ...,  0.2831,  0.3079,  0.4419]],\n",
            "       grad_fn=<SqueezeBackward1>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wEQxmYCwUr",
        "outputId": "9b0d0169-073d-4cd6-9379-d9bb8135350b"
      },
      "source": [
        "if config.model_name != \"\":\n",
        "    for n, (name, tensor) in enumerate(list(model.named_parameters())):\n",
        "        print(f\"{n:>4}: {tensor.requires_grad}, {name}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0: True, auto_model.embeddings.word_embeddings.weight\n",
            "   1: True, auto_model.embeddings.position_embeddings.weight\n",
            "   2: True, auto_model.embeddings.token_type_embeddings.weight\n",
            "   3: True, auto_model.embeddings.LayerNorm.weight\n",
            "   4: True, auto_model.embeddings.LayerNorm.bias\n",
            "   5: True, auto_model.encoder.layer.0.attention.self.query.weight\n",
            "   6: True, auto_model.encoder.layer.0.attention.self.query.bias\n",
            "   7: True, auto_model.encoder.layer.0.attention.self.key.weight\n",
            "   8: True, auto_model.encoder.layer.0.attention.self.key.bias\n",
            "   9: True, auto_model.encoder.layer.0.attention.self.value.weight\n",
            "  10: True, auto_model.encoder.layer.0.attention.self.value.bias\n",
            "  11: True, auto_model.encoder.layer.0.attention.output.dense.weight\n",
            "  12: True, auto_model.encoder.layer.0.attention.output.dense.bias\n",
            "  13: True, auto_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "  14: True, auto_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "  15: True, auto_model.encoder.layer.0.intermediate.dense.weight\n",
            "  16: True, auto_model.encoder.layer.0.intermediate.dense.bias\n",
            "  17: True, auto_model.encoder.layer.0.output.dense.weight\n",
            "  18: True, auto_model.encoder.layer.0.output.dense.bias\n",
            "  19: True, auto_model.encoder.layer.0.output.LayerNorm.weight\n",
            "  20: True, auto_model.encoder.layer.0.output.LayerNorm.bias\n",
            "  21: True, auto_model.encoder.layer.1.attention.self.query.weight\n",
            "  22: True, auto_model.encoder.layer.1.attention.self.query.bias\n",
            "  23: True, auto_model.encoder.layer.1.attention.self.key.weight\n",
            "  24: True, auto_model.encoder.layer.1.attention.self.key.bias\n",
            "  25: True, auto_model.encoder.layer.1.attention.self.value.weight\n",
            "  26: True, auto_model.encoder.layer.1.attention.self.value.bias\n",
            "  27: True, auto_model.encoder.layer.1.attention.output.dense.weight\n",
            "  28: True, auto_model.encoder.layer.1.attention.output.dense.bias\n",
            "  29: True, auto_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "  30: True, auto_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "  31: True, auto_model.encoder.layer.1.intermediate.dense.weight\n",
            "  32: True, auto_model.encoder.layer.1.intermediate.dense.bias\n",
            "  33: True, auto_model.encoder.layer.1.output.dense.weight\n",
            "  34: True, auto_model.encoder.layer.1.output.dense.bias\n",
            "  35: True, auto_model.encoder.layer.1.output.LayerNorm.weight\n",
            "  36: True, auto_model.encoder.layer.1.output.LayerNorm.bias\n",
            "  37: True, auto_model.encoder.layer.2.attention.self.query.weight\n",
            "  38: True, auto_model.encoder.layer.2.attention.self.query.bias\n",
            "  39: True, auto_model.encoder.layer.2.attention.self.key.weight\n",
            "  40: True, auto_model.encoder.layer.2.attention.self.key.bias\n",
            "  41: True, auto_model.encoder.layer.2.attention.self.value.weight\n",
            "  42: True, auto_model.encoder.layer.2.attention.self.value.bias\n",
            "  43: True, auto_model.encoder.layer.2.attention.output.dense.weight\n",
            "  44: True, auto_model.encoder.layer.2.attention.output.dense.bias\n",
            "  45: True, auto_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "  46: True, auto_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "  47: True, auto_model.encoder.layer.2.intermediate.dense.weight\n",
            "  48: True, auto_model.encoder.layer.2.intermediate.dense.bias\n",
            "  49: True, auto_model.encoder.layer.2.output.dense.weight\n",
            "  50: True, auto_model.encoder.layer.2.output.dense.bias\n",
            "  51: True, auto_model.encoder.layer.2.output.LayerNorm.weight\n",
            "  52: True, auto_model.encoder.layer.2.output.LayerNorm.bias\n",
            "  53: True, auto_model.encoder.layer.3.attention.self.query.weight\n",
            "  54: True, auto_model.encoder.layer.3.attention.self.query.bias\n",
            "  55: True, auto_model.encoder.layer.3.attention.self.key.weight\n",
            "  56: True, auto_model.encoder.layer.3.attention.self.key.bias\n",
            "  57: True, auto_model.encoder.layer.3.attention.self.value.weight\n",
            "  58: True, auto_model.encoder.layer.3.attention.self.value.bias\n",
            "  59: True, auto_model.encoder.layer.3.attention.output.dense.weight\n",
            "  60: True, auto_model.encoder.layer.3.attention.output.dense.bias\n",
            "  61: True, auto_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "  62: True, auto_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "  63: True, auto_model.encoder.layer.3.intermediate.dense.weight\n",
            "  64: True, auto_model.encoder.layer.3.intermediate.dense.bias\n",
            "  65: True, auto_model.encoder.layer.3.output.dense.weight\n",
            "  66: True, auto_model.encoder.layer.3.output.dense.bias\n",
            "  67: True, auto_model.encoder.layer.3.output.LayerNorm.weight\n",
            "  68: True, auto_model.encoder.layer.3.output.LayerNorm.bias\n",
            "  69: True, auto_model.encoder.layer.4.attention.self.query.weight\n",
            "  70: True, auto_model.encoder.layer.4.attention.self.query.bias\n",
            "  71: True, auto_model.encoder.layer.4.attention.self.key.weight\n",
            "  72: True, auto_model.encoder.layer.4.attention.self.key.bias\n",
            "  73: True, auto_model.encoder.layer.4.attention.self.value.weight\n",
            "  74: True, auto_model.encoder.layer.4.attention.self.value.bias\n",
            "  75: True, auto_model.encoder.layer.4.attention.output.dense.weight\n",
            "  76: True, auto_model.encoder.layer.4.attention.output.dense.bias\n",
            "  77: True, auto_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "  78: True, auto_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "  79: True, auto_model.encoder.layer.4.intermediate.dense.weight\n",
            "  80: True, auto_model.encoder.layer.4.intermediate.dense.bias\n",
            "  81: True, auto_model.encoder.layer.4.output.dense.weight\n",
            "  82: True, auto_model.encoder.layer.4.output.dense.bias\n",
            "  83: True, auto_model.encoder.layer.4.output.LayerNorm.weight\n",
            "  84: True, auto_model.encoder.layer.4.output.LayerNorm.bias\n",
            "  85: True, auto_model.encoder.layer.5.attention.self.query.weight\n",
            "  86: True, auto_model.encoder.layer.5.attention.self.query.bias\n",
            "  87: True, auto_model.encoder.layer.5.attention.self.key.weight\n",
            "  88: True, auto_model.encoder.layer.5.attention.self.key.bias\n",
            "  89: True, auto_model.encoder.layer.5.attention.self.value.weight\n",
            "  90: True, auto_model.encoder.layer.5.attention.self.value.bias\n",
            "  91: True, auto_model.encoder.layer.5.attention.output.dense.weight\n",
            "  92: True, auto_model.encoder.layer.5.attention.output.dense.bias\n",
            "  93: True, auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "  94: True, auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "  95: True, auto_model.encoder.layer.5.intermediate.dense.weight\n",
            "  96: True, auto_model.encoder.layer.5.intermediate.dense.bias\n",
            "  97: True, auto_model.encoder.layer.5.output.dense.weight\n",
            "  98: True, auto_model.encoder.layer.5.output.dense.bias\n",
            "  99: True, auto_model.encoder.layer.5.output.LayerNorm.weight\n",
            " 100: True, auto_model.encoder.layer.5.output.LayerNorm.bias\n",
            " 101: True, auto_model.encoder.layer.6.attention.self.query.weight\n",
            " 102: True, auto_model.encoder.layer.6.attention.self.query.bias\n",
            " 103: True, auto_model.encoder.layer.6.attention.self.key.weight\n",
            " 104: True, auto_model.encoder.layer.6.attention.self.key.bias\n",
            " 105: True, auto_model.encoder.layer.6.attention.self.value.weight\n",
            " 106: True, auto_model.encoder.layer.6.attention.self.value.bias\n",
            " 107: True, auto_model.encoder.layer.6.attention.output.dense.weight\n",
            " 108: True, auto_model.encoder.layer.6.attention.output.dense.bias\n",
            " 109: True, auto_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
            " 110: True, auto_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
            " 111: True, auto_model.encoder.layer.6.intermediate.dense.weight\n",
            " 112: True, auto_model.encoder.layer.6.intermediate.dense.bias\n",
            " 113: True, auto_model.encoder.layer.6.output.dense.weight\n",
            " 114: True, auto_model.encoder.layer.6.output.dense.bias\n",
            " 115: True, auto_model.encoder.layer.6.output.LayerNorm.weight\n",
            " 116: True, auto_model.encoder.layer.6.output.LayerNorm.bias\n",
            " 117: True, auto_model.encoder.layer.7.attention.self.query.weight\n",
            " 118: True, auto_model.encoder.layer.7.attention.self.query.bias\n",
            " 119: True, auto_model.encoder.layer.7.attention.self.key.weight\n",
            " 120: True, auto_model.encoder.layer.7.attention.self.key.bias\n",
            " 121: True, auto_model.encoder.layer.7.attention.self.value.weight\n",
            " 122: True, auto_model.encoder.layer.7.attention.self.value.bias\n",
            " 123: True, auto_model.encoder.layer.7.attention.output.dense.weight\n",
            " 124: True, auto_model.encoder.layer.7.attention.output.dense.bias\n",
            " 125: True, auto_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
            " 126: True, auto_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
            " 127: True, auto_model.encoder.layer.7.intermediate.dense.weight\n",
            " 128: True, auto_model.encoder.layer.7.intermediate.dense.bias\n",
            " 129: True, auto_model.encoder.layer.7.output.dense.weight\n",
            " 130: True, auto_model.encoder.layer.7.output.dense.bias\n",
            " 131: True, auto_model.encoder.layer.7.output.LayerNorm.weight\n",
            " 132: True, auto_model.encoder.layer.7.output.LayerNorm.bias\n",
            " 133: True, auto_model.encoder.layer.8.attention.self.query.weight\n",
            " 134: True, auto_model.encoder.layer.8.attention.self.query.bias\n",
            " 135: True, auto_model.encoder.layer.8.attention.self.key.weight\n",
            " 136: True, auto_model.encoder.layer.8.attention.self.key.bias\n",
            " 137: True, auto_model.encoder.layer.8.attention.self.value.weight\n",
            " 138: True, auto_model.encoder.layer.8.attention.self.value.bias\n",
            " 139: True, auto_model.encoder.layer.8.attention.output.dense.weight\n",
            " 140: True, auto_model.encoder.layer.8.attention.output.dense.bias\n",
            " 141: True, auto_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
            " 142: True, auto_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
            " 143: True, auto_model.encoder.layer.8.intermediate.dense.weight\n",
            " 144: True, auto_model.encoder.layer.8.intermediate.dense.bias\n",
            " 145: True, auto_model.encoder.layer.8.output.dense.weight\n",
            " 146: True, auto_model.encoder.layer.8.output.dense.bias\n",
            " 147: True, auto_model.encoder.layer.8.output.LayerNorm.weight\n",
            " 148: True, auto_model.encoder.layer.8.output.LayerNorm.bias\n",
            " 149: True, auto_model.encoder.layer.9.attention.self.query.weight\n",
            " 150: True, auto_model.encoder.layer.9.attention.self.query.bias\n",
            " 151: True, auto_model.encoder.layer.9.attention.self.key.weight\n",
            " 152: True, auto_model.encoder.layer.9.attention.self.key.bias\n",
            " 153: True, auto_model.encoder.layer.9.attention.self.value.weight\n",
            " 154: True, auto_model.encoder.layer.9.attention.self.value.bias\n",
            " 155: True, auto_model.encoder.layer.9.attention.output.dense.weight\n",
            " 156: True, auto_model.encoder.layer.9.attention.output.dense.bias\n",
            " 157: True, auto_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
            " 158: True, auto_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
            " 159: True, auto_model.encoder.layer.9.intermediate.dense.weight\n",
            " 160: True, auto_model.encoder.layer.9.intermediate.dense.bias\n",
            " 161: True, auto_model.encoder.layer.9.output.dense.weight\n",
            " 162: True, auto_model.encoder.layer.9.output.dense.bias\n",
            " 163: True, auto_model.encoder.layer.9.output.LayerNorm.weight\n",
            " 164: True, auto_model.encoder.layer.9.output.LayerNorm.bias\n",
            " 165: True, auto_model.encoder.layer.10.attention.self.query.weight\n",
            " 166: True, auto_model.encoder.layer.10.attention.self.query.bias\n",
            " 167: True, auto_model.encoder.layer.10.attention.self.key.weight\n",
            " 168: True, auto_model.encoder.layer.10.attention.self.key.bias\n",
            " 169: True, auto_model.encoder.layer.10.attention.self.value.weight\n",
            " 170: True, auto_model.encoder.layer.10.attention.self.value.bias\n",
            " 171: True, auto_model.encoder.layer.10.attention.output.dense.weight\n",
            " 172: True, auto_model.encoder.layer.10.attention.output.dense.bias\n",
            " 173: True, auto_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
            " 174: True, auto_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
            " 175: True, auto_model.encoder.layer.10.intermediate.dense.weight\n",
            " 176: True, auto_model.encoder.layer.10.intermediate.dense.bias\n",
            " 177: True, auto_model.encoder.layer.10.output.dense.weight\n",
            " 178: True, auto_model.encoder.layer.10.output.dense.bias\n",
            " 179: True, auto_model.encoder.layer.10.output.LayerNorm.weight\n",
            " 180: True, auto_model.encoder.layer.10.output.LayerNorm.bias\n",
            " 181: True, auto_model.encoder.layer.11.attention.self.query.weight\n",
            " 182: True, auto_model.encoder.layer.11.attention.self.query.bias\n",
            " 183: True, auto_model.encoder.layer.11.attention.self.key.weight\n",
            " 184: True, auto_model.encoder.layer.11.attention.self.key.bias\n",
            " 185: True, auto_model.encoder.layer.11.attention.self.value.weight\n",
            " 186: True, auto_model.encoder.layer.11.attention.self.value.bias\n",
            " 187: True, auto_model.encoder.layer.11.attention.output.dense.weight\n",
            " 188: True, auto_model.encoder.layer.11.attention.output.dense.bias\n",
            " 189: True, auto_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
            " 190: True, auto_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
            " 191: True, auto_model.encoder.layer.11.intermediate.dense.weight\n",
            " 192: True, auto_model.encoder.layer.11.intermediate.dense.bias\n",
            " 193: True, auto_model.encoder.layer.11.output.dense.weight\n",
            " 194: True, auto_model.encoder.layer.11.output.dense.bias\n",
            " 195: True, auto_model.encoder.layer.11.output.LayerNorm.weight\n",
            " 196: True, auto_model.encoder.layer.11.output.LayerNorm.bias\n",
            " 197: True, auto_model.encoder.layer.12.attention.self.query.weight\n",
            " 198: True, auto_model.encoder.layer.12.attention.self.query.bias\n",
            " 199: True, auto_model.encoder.layer.12.attention.self.key.weight\n",
            " 200: True, auto_model.encoder.layer.12.attention.self.key.bias\n",
            " 201: True, auto_model.encoder.layer.12.attention.self.value.weight\n",
            " 202: True, auto_model.encoder.layer.12.attention.self.value.bias\n",
            " 203: True, auto_model.encoder.layer.12.attention.output.dense.weight\n",
            " 204: True, auto_model.encoder.layer.12.attention.output.dense.bias\n",
            " 205: True, auto_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
            " 206: True, auto_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
            " 207: True, auto_model.encoder.layer.12.intermediate.dense.weight\n",
            " 208: True, auto_model.encoder.layer.12.intermediate.dense.bias\n",
            " 209: True, auto_model.encoder.layer.12.output.dense.weight\n",
            " 210: True, auto_model.encoder.layer.12.output.dense.bias\n",
            " 211: True, auto_model.encoder.layer.12.output.LayerNorm.weight\n",
            " 212: True, auto_model.encoder.layer.12.output.LayerNorm.bias\n",
            " 213: True, auto_model.encoder.layer.13.attention.self.query.weight\n",
            " 214: True, auto_model.encoder.layer.13.attention.self.query.bias\n",
            " 215: True, auto_model.encoder.layer.13.attention.self.key.weight\n",
            " 216: True, auto_model.encoder.layer.13.attention.self.key.bias\n",
            " 217: True, auto_model.encoder.layer.13.attention.self.value.weight\n",
            " 218: True, auto_model.encoder.layer.13.attention.self.value.bias\n",
            " 219: True, auto_model.encoder.layer.13.attention.output.dense.weight\n",
            " 220: True, auto_model.encoder.layer.13.attention.output.dense.bias\n",
            " 221: True, auto_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
            " 222: True, auto_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
            " 223: True, auto_model.encoder.layer.13.intermediate.dense.weight\n",
            " 224: True, auto_model.encoder.layer.13.intermediate.dense.bias\n",
            " 225: True, auto_model.encoder.layer.13.output.dense.weight\n",
            " 226: True, auto_model.encoder.layer.13.output.dense.bias\n",
            " 227: True, auto_model.encoder.layer.13.output.LayerNorm.weight\n",
            " 228: True, auto_model.encoder.layer.13.output.LayerNorm.bias\n",
            " 229: True, auto_model.encoder.layer.14.attention.self.query.weight\n",
            " 230: True, auto_model.encoder.layer.14.attention.self.query.bias\n",
            " 231: True, auto_model.encoder.layer.14.attention.self.key.weight\n",
            " 232: True, auto_model.encoder.layer.14.attention.self.key.bias\n",
            " 233: True, auto_model.encoder.layer.14.attention.self.value.weight\n",
            " 234: True, auto_model.encoder.layer.14.attention.self.value.bias\n",
            " 235: True, auto_model.encoder.layer.14.attention.output.dense.weight\n",
            " 236: True, auto_model.encoder.layer.14.attention.output.dense.bias\n",
            " 237: True, auto_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
            " 238: True, auto_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
            " 239: True, auto_model.encoder.layer.14.intermediate.dense.weight\n",
            " 240: True, auto_model.encoder.layer.14.intermediate.dense.bias\n",
            " 241: True, auto_model.encoder.layer.14.output.dense.weight\n",
            " 242: True, auto_model.encoder.layer.14.output.dense.bias\n",
            " 243: True, auto_model.encoder.layer.14.output.LayerNorm.weight\n",
            " 244: True, auto_model.encoder.layer.14.output.LayerNorm.bias\n",
            " 245: True, auto_model.encoder.layer.15.attention.self.query.weight\n",
            " 246: True, auto_model.encoder.layer.15.attention.self.query.bias\n",
            " 247: True, auto_model.encoder.layer.15.attention.self.key.weight\n",
            " 248: True, auto_model.encoder.layer.15.attention.self.key.bias\n",
            " 249: True, auto_model.encoder.layer.15.attention.self.value.weight\n",
            " 250: True, auto_model.encoder.layer.15.attention.self.value.bias\n",
            " 251: True, auto_model.encoder.layer.15.attention.output.dense.weight\n",
            " 252: True, auto_model.encoder.layer.15.attention.output.dense.bias\n",
            " 253: True, auto_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
            " 254: True, auto_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
            " 255: True, auto_model.encoder.layer.15.intermediate.dense.weight\n",
            " 256: True, auto_model.encoder.layer.15.intermediate.dense.bias\n",
            " 257: True, auto_model.encoder.layer.15.output.dense.weight\n",
            " 258: True, auto_model.encoder.layer.15.output.dense.bias\n",
            " 259: True, auto_model.encoder.layer.15.output.LayerNorm.weight\n",
            " 260: True, auto_model.encoder.layer.15.output.LayerNorm.bias\n",
            " 261: True, auto_model.encoder.layer.16.attention.self.query.weight\n",
            " 262: True, auto_model.encoder.layer.16.attention.self.query.bias\n",
            " 263: True, auto_model.encoder.layer.16.attention.self.key.weight\n",
            " 264: True, auto_model.encoder.layer.16.attention.self.key.bias\n",
            " 265: True, auto_model.encoder.layer.16.attention.self.value.weight\n",
            " 266: True, auto_model.encoder.layer.16.attention.self.value.bias\n",
            " 267: True, auto_model.encoder.layer.16.attention.output.dense.weight\n",
            " 268: True, auto_model.encoder.layer.16.attention.output.dense.bias\n",
            " 269: True, auto_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
            " 270: True, auto_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
            " 271: True, auto_model.encoder.layer.16.intermediate.dense.weight\n",
            " 272: True, auto_model.encoder.layer.16.intermediate.dense.bias\n",
            " 273: True, auto_model.encoder.layer.16.output.dense.weight\n",
            " 274: True, auto_model.encoder.layer.16.output.dense.bias\n",
            " 275: True, auto_model.encoder.layer.16.output.LayerNorm.weight\n",
            " 276: True, auto_model.encoder.layer.16.output.LayerNorm.bias\n",
            " 277: True, auto_model.encoder.layer.17.attention.self.query.weight\n",
            " 278: True, auto_model.encoder.layer.17.attention.self.query.bias\n",
            " 279: True, auto_model.encoder.layer.17.attention.self.key.weight\n",
            " 280: True, auto_model.encoder.layer.17.attention.self.key.bias\n",
            " 281: True, auto_model.encoder.layer.17.attention.self.value.weight\n",
            " 282: True, auto_model.encoder.layer.17.attention.self.value.bias\n",
            " 283: True, auto_model.encoder.layer.17.attention.output.dense.weight\n",
            " 284: True, auto_model.encoder.layer.17.attention.output.dense.bias\n",
            " 285: True, auto_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
            " 286: True, auto_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
            " 287: True, auto_model.encoder.layer.17.intermediate.dense.weight\n",
            " 288: True, auto_model.encoder.layer.17.intermediate.dense.bias\n",
            " 289: True, auto_model.encoder.layer.17.output.dense.weight\n",
            " 290: True, auto_model.encoder.layer.17.output.dense.bias\n",
            " 291: True, auto_model.encoder.layer.17.output.LayerNorm.weight\n",
            " 292: True, auto_model.encoder.layer.17.output.LayerNorm.bias\n",
            " 293: True, auto_model.encoder.layer.18.attention.self.query.weight\n",
            " 294: True, auto_model.encoder.layer.18.attention.self.query.bias\n",
            " 295: True, auto_model.encoder.layer.18.attention.self.key.weight\n",
            " 296: True, auto_model.encoder.layer.18.attention.self.key.bias\n",
            " 297: True, auto_model.encoder.layer.18.attention.self.value.weight\n",
            " 298: True, auto_model.encoder.layer.18.attention.self.value.bias\n",
            " 299: True, auto_model.encoder.layer.18.attention.output.dense.weight\n",
            " 300: True, auto_model.encoder.layer.18.attention.output.dense.bias\n",
            " 301: True, auto_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
            " 302: True, auto_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
            " 303: True, auto_model.encoder.layer.18.intermediate.dense.weight\n",
            " 304: True, auto_model.encoder.layer.18.intermediate.dense.bias\n",
            " 305: True, auto_model.encoder.layer.18.output.dense.weight\n",
            " 306: True, auto_model.encoder.layer.18.output.dense.bias\n",
            " 307: True, auto_model.encoder.layer.18.output.LayerNorm.weight\n",
            " 308: True, auto_model.encoder.layer.18.output.LayerNorm.bias\n",
            " 309: True, auto_model.encoder.layer.19.attention.self.query.weight\n",
            " 310: True, auto_model.encoder.layer.19.attention.self.query.bias\n",
            " 311: True, auto_model.encoder.layer.19.attention.self.key.weight\n",
            " 312: True, auto_model.encoder.layer.19.attention.self.key.bias\n",
            " 313: True, auto_model.encoder.layer.19.attention.self.value.weight\n",
            " 314: True, auto_model.encoder.layer.19.attention.self.value.bias\n",
            " 315: True, auto_model.encoder.layer.19.attention.output.dense.weight\n",
            " 316: True, auto_model.encoder.layer.19.attention.output.dense.bias\n",
            " 317: True, auto_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
            " 318: True, auto_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
            " 319: True, auto_model.encoder.layer.19.intermediate.dense.weight\n",
            " 320: True, auto_model.encoder.layer.19.intermediate.dense.bias\n",
            " 321: True, auto_model.encoder.layer.19.output.dense.weight\n",
            " 322: True, auto_model.encoder.layer.19.output.dense.bias\n",
            " 323: True, auto_model.encoder.layer.19.output.LayerNorm.weight\n",
            " 324: True, auto_model.encoder.layer.19.output.LayerNorm.bias\n",
            " 325: True, auto_model.encoder.layer.20.attention.self.query.weight\n",
            " 326: True, auto_model.encoder.layer.20.attention.self.query.bias\n",
            " 327: True, auto_model.encoder.layer.20.attention.self.key.weight\n",
            " 328: True, auto_model.encoder.layer.20.attention.self.key.bias\n",
            " 329: True, auto_model.encoder.layer.20.attention.self.value.weight\n",
            " 330: True, auto_model.encoder.layer.20.attention.self.value.bias\n",
            " 331: True, auto_model.encoder.layer.20.attention.output.dense.weight\n",
            " 332: True, auto_model.encoder.layer.20.attention.output.dense.bias\n",
            " 333: True, auto_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
            " 334: True, auto_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
            " 335: True, auto_model.encoder.layer.20.intermediate.dense.weight\n",
            " 336: True, auto_model.encoder.layer.20.intermediate.dense.bias\n",
            " 337: True, auto_model.encoder.layer.20.output.dense.weight\n",
            " 338: True, auto_model.encoder.layer.20.output.dense.bias\n",
            " 339: True, auto_model.encoder.layer.20.output.LayerNorm.weight\n",
            " 340: True, auto_model.encoder.layer.20.output.LayerNorm.bias\n",
            " 341: True, auto_model.encoder.layer.21.attention.self.query.weight\n",
            " 342: True, auto_model.encoder.layer.21.attention.self.query.bias\n",
            " 343: True, auto_model.encoder.layer.21.attention.self.key.weight\n",
            " 344: True, auto_model.encoder.layer.21.attention.self.key.bias\n",
            " 345: True, auto_model.encoder.layer.21.attention.self.value.weight\n",
            " 346: True, auto_model.encoder.layer.21.attention.self.value.bias\n",
            " 347: True, auto_model.encoder.layer.21.attention.output.dense.weight\n",
            " 348: True, auto_model.encoder.layer.21.attention.output.dense.bias\n",
            " 349: True, auto_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
            " 350: True, auto_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
            " 351: True, auto_model.encoder.layer.21.intermediate.dense.weight\n",
            " 352: True, auto_model.encoder.layer.21.intermediate.dense.bias\n",
            " 353: True, auto_model.encoder.layer.21.output.dense.weight\n",
            " 354: True, auto_model.encoder.layer.21.output.dense.bias\n",
            " 355: True, auto_model.encoder.layer.21.output.LayerNorm.weight\n",
            " 356: True, auto_model.encoder.layer.21.output.LayerNorm.bias\n",
            " 357: True, auto_model.encoder.layer.22.attention.self.query.weight\n",
            " 358: True, auto_model.encoder.layer.22.attention.self.query.bias\n",
            " 359: True, auto_model.encoder.layer.22.attention.self.key.weight\n",
            " 360: True, auto_model.encoder.layer.22.attention.self.key.bias\n",
            " 361: True, auto_model.encoder.layer.22.attention.self.value.weight\n",
            " 362: True, auto_model.encoder.layer.22.attention.self.value.bias\n",
            " 363: True, auto_model.encoder.layer.22.attention.output.dense.weight\n",
            " 364: True, auto_model.encoder.layer.22.attention.output.dense.bias\n",
            " 365: True, auto_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
            " 366: True, auto_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
            " 367: True, auto_model.encoder.layer.22.intermediate.dense.weight\n",
            " 368: True, auto_model.encoder.layer.22.intermediate.dense.bias\n",
            " 369: True, auto_model.encoder.layer.22.output.dense.weight\n",
            " 370: True, auto_model.encoder.layer.22.output.dense.bias\n",
            " 371: True, auto_model.encoder.layer.22.output.LayerNorm.weight\n",
            " 372: True, auto_model.encoder.layer.22.output.LayerNorm.bias\n",
            " 373: True, auto_model.encoder.layer.23.attention.self.query.weight\n",
            " 374: True, auto_model.encoder.layer.23.attention.self.query.bias\n",
            " 375: True, auto_model.encoder.layer.23.attention.self.key.weight\n",
            " 376: True, auto_model.encoder.layer.23.attention.self.key.bias\n",
            " 377: True, auto_model.encoder.layer.23.attention.self.value.weight\n",
            " 378: True, auto_model.encoder.layer.23.attention.self.value.bias\n",
            " 379: True, auto_model.encoder.layer.23.attention.output.dense.weight\n",
            " 380: True, auto_model.encoder.layer.23.attention.output.dense.bias\n",
            " 381: True, auto_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
            " 382: True, auto_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
            " 383: True, auto_model.encoder.layer.23.intermediate.dense.weight\n",
            " 384: True, auto_model.encoder.layer.23.intermediate.dense.bias\n",
            " 385: True, auto_model.encoder.layer.23.output.dense.weight\n",
            " 386: True, auto_model.encoder.layer.23.output.dense.bias\n",
            " 387: True, auto_model.encoder.layer.23.output.LayerNorm.weight\n",
            " 388: True, auto_model.encoder.layer.23.output.LayerNorm.bias\n",
            " 389: True, auto_model.pooler.dense.weight\n",
            " 390: True, auto_model.pooler.dense.bias\n",
            " 391: True, qa_outputs.weight\n",
            " 392: True, qa_outputs.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6NSxPjSDAp0"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_stCiHpfE4Cj"
      },
      "source": [
        "def bert_optimizer(model):\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    if \"base\" in config.model_name or \"L-12\" in config.model_name:\n",
        "        bert_parameters = named_parameters[:197]    \n",
        "        regressor_parameters = named_parameters[199:]\n",
        "        second_block = 69\n",
        "        third_block = 133\n",
        "\n",
        "    elif \"large\" in config.model_name or \"L-24\" in config.model_name:\n",
        "        bert_parameters = named_parameters[:388]    \n",
        "        regressor_parameters = named_parameters[391:]\n",
        "        second_block = 133\n",
        "        third_block = 261\n",
        "        \n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(bert_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else config.weight_decay\n",
        "\n",
        "        lr = config.lr\n",
        "\n",
        "        if layer_num >= second_block:\n",
        "            lr = config.lr_second\n",
        "\n",
        "        if layer_num >= third_block:\n",
        "            lr = config.lr_third\n",
        "\n",
        "        parameters.append({\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr})\n",
        "\n",
        "    return T.AdamW(parameters)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENcidivxDVIj"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkAGe7WFDWG4"
      },
      "source": [
        "def chaii_cross_entropy(preds, labels):\n",
        "    start_preds, end_preds = preds\n",
        "    start_labels, end_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    total_loss = (start_loss + end_loss) / 2\n",
        "    return total_loss"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJr7FSz5KvLt"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXFVz_OVKu2O"
      },
      "source": [
        "def jaccard(row): \n",
        "    str1 = row[0]\n",
        "    str2 = row[1]\n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfqKjGpLaMMX"
      },
      "source": [
        "def get_result(result_df, fold=config.n_fold):\n",
        "    score = result_df[\"jaccard\"].mean()\n",
        "    LOGGER.info(f\"Score: {score:<.5f}\")\n",
        "    if fold == config.n_fold:\n",
        "        wandb.log({\"CV\": score})\n",
        "    else:\n",
        "        wandb.log({f\"CV_fold{fold}\": score})"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7aZ38xCMG__"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD2wDdHMMMSc"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return \"%dm %ds\" % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrVjLs3H8DXV"
      },
      "source": [
        "def compute_grad_norm(parameters, norm_type=2.0):\n",
        "    \"\"\"Refer to torch.nn.utils.clip_grad_norm_\"\"\"\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    norm_type = float(norm_type)\n",
        "    total_norm = 0\n",
        "    for p in parameters:\n",
        "        param_norm = p.grad.data.norm(norm_type)\n",
        "        total_norm += param_norm.item() ** norm_type\n",
        "    total_norm = total_norm ** (1. / norm_type)\n",
        "    return total_norm"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laoX2YvHMW40"
      },
      "source": [
        "def train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, features in enumerate(train_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        with amp.autocast(enabled=Config.amp):\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "            loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "            losses.update(loss.item(), batch_size)\n",
        "            loss = loss / config.gradient_accumulation_steps\n",
        "            \n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        else:\n",
        "            grad_norm = compute_grad_norm(model.parameters())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(train_loader) - 1):\n",
        "            print(\n",
        "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "                f\"Grad: {grad_norm:.4f} \"\n",
        "                f\"LR: {scheduler.get_lr()[0]:.6f} \"\n",
        "            )\n",
        "            wandb.log({\n",
        "                \"step\": (epoch) * len(train_loader) + step,\n",
        "                f\"loss/fold{fold}\": losses.avg,\n",
        "                f\"grad/fold{fold}\": grad_norm,\n",
        "                f\"lr/fold{fold}\": scheduler.get_lr()[0],\n",
        "            })\n",
        "\n",
        "    return losses.avg"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-4GZ8PcPpLt"
      },
      "source": [
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds_start = []\n",
        "    preds_end = []\n",
        "    start = time.time()\n",
        "\n",
        "    for step, features in enumerate(valid_loader):\n",
        "        input_ids = features[\"input_ids\"].to(device)\n",
        "        attention_mask = features[\"attention_mask\"].to(device)\n",
        "        labels_start = features[\"start_position\"].to(device)\n",
        "        labels_end = features[\"end_position\"].to(device)\n",
        "        batch_size = labels_start.size(0)\n",
        "\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            out_start, out_end = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion((out_start, out_end), (labels_start, labels_end))\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        preds_start.append(out_start.to(\"cpu\").numpy())\n",
        "        preds_end.append(out_end.to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.softmax(1).to(\"cpu\").numpy())\n",
        "        # preds.append(y_preds.to(\"cpu\").numpy())\n",
        "\n",
        "        end = time.time()\n",
        "        if step % Config.print_freq == 0 or step == (len(valid_loader) - 1):\n",
        "            print(\n",
        "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
        "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
        "                f\"Loss: {losses.avg:.4f} \"\n",
        "            )\n",
        "\n",
        "    predictions_start = np.concatenate(preds_start)\n",
        "    predictions_end = np.concatenate(preds_end)\n",
        "    return losses.avg, predictions_start, predictions_end"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aS_0cqjWy5P"
      },
      "source": [
        "# Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5pkRxmW0z_"
      },
      "source": [
        "def postprocess_qa_predictions(examples, features, tokenizer, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    \n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    for example_index, example in examples.iterrows():\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        for feature_index in feature_indices:\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "\n",
        "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
        "            context_index = 1\n",
        "\n",
        "            features[feature_index][\"offset_mapping\"] = [\n",
        "                (o if sequence_ids[k] == context_index else None)\n",
        "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
        "            ]\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vwcRHThRbcm"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKmu1ZdXRdA7"
      },
      "source": [
        "def train_loop(df, fold):\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # Data Loader\n",
        "    # ====================================================\n",
        "    trn_idx = df[df[\"fold\"] != fold].index\n",
        "    val_idx = df[df[\"fold\"] == fold].index\n",
        "\n",
        "    train_folds = df.loc[trn_idx].reset_index(drop=True)\n",
        "    valid_folds = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = BaseDataset(train_folds, config.model_name)\n",
        "    valid_dataset = BaseDataset(valid_folds, config.model_name)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # ====================================================\n",
        "    # Optimizer\n",
        "    # ====================================================\n",
        "    def get_optimizer(model):\n",
        "        if config.optimizer == \"Adam\":\n",
        "            optimizer = Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"AdamW\":\n",
        "            optimizer = T.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "        elif config.optimizer == \"BertAdamW\":\n",
        "            optimizer = bert_optimizer(model)\n",
        "        return optimizer\n",
        "\n",
        "    # ====================================================\n",
        "    # Scheduler\n",
        "    # ====================================================\n",
        "    def get_scheduler(optimizer):\n",
        "        # num_data = len(train_folds)\n",
        "        num_data = len(train_dataset)\n",
        "        num_steps = num_data // (config.batch_size * config.gradient_accumulation_steps) * config.epochs\n",
        "\n",
        "        if config.scheduler == \"CosineAnnealingWarmRestarts\":\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=num_steps, T_mult=1, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingLR\":\n",
        "            scheduler = CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=config.min_lr, last_epoch=-1)\n",
        "        elif config.scheduler == \"CosineAnnealingWarmupRestarts\":\n",
        "            scheduler = CosineAnnealingWarmupRestarts(\n",
        "                optimizer, first_cycle_steps=num_steps, max_lr=config.lr, min_lr=config.min_lr, warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        elif config.scheduler == \"get_cosine_schedule_with_warmup\":\n",
        "            scheduler = T.get_cosine_schedule_with_warmup(\n",
        "                optimizer, num_training_steps=num_steps, num_warmup_steps=(num_steps // 10)\n",
        "            )\n",
        "        return scheduler\n",
        "\n",
        "    # ====================================================\n",
        "    # Model\n",
        "    # ====================================================\n",
        "    model = BaseModel(config.model_name)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = get_optimizer(model)\n",
        "    scaler = amp.GradScaler(enabled=Config.amp)\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "\n",
        "    # ====================================================\n",
        "    # Criterion\n",
        "    # ====================================================\n",
        "    def get_criterion():\n",
        "        if config.criterion == \"CrossEntropyLoss\":\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "        elif config.criterion == \"BCEWithLogitsLoss\":\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "        elif config.criterion == \"MSELoss\":\n",
        "            criterion = nn.MSELoss()\n",
        "        elif config.criterion == \"ChaiiCrossEntropyLoss\":\n",
        "            criterion = chaii_cross_entropy\n",
        "        return criterion\n",
        "\n",
        "    criterion = get_criterion()\n",
        "\n",
        "    # ====================================================\n",
        "    # Loop\n",
        "    # ====================================================\n",
        "    best_score = -1\n",
        "    best_loss = np.inf\n",
        "    best_preds = None\n",
        "\n",
        "    wandb.watch(model, log_freq=Config.print_freq)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(train_loader, model, criterion, optimizer, scheduler, scaler, fold, epoch, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, preds_start, preds_end = valid_fn(valid_loader, model, criterion, device)\n",
        "\n",
        "        predictions = postprocess_qa_predictions(\n",
        "            valid_folds, valid_dataset.features, valid_dataset.tokenizer, (preds_start, preds_end)\n",
        "        )\n",
        "\n",
        "        # if config.criterion == \"BCEWithLogitsLoss\":\n",
        "        #     preds = 1 / (1 + np.exp(-preds))\n",
        "\n",
        "        # scoring\n",
        "        # score = get_score(valid_labels, preds.argmax(1))\n",
        "        # score = get_score(valid_labels, preds)\n",
        "        oof_df = valid_folds[[\"id\", \"answer_text\"]]\n",
        "        oof_df[\"prediction\"] = oof_df['id'].apply(lambda r: predictions[r])\n",
        "        oof_df['jaccard'] = oof_df[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
        "        score = oof_df[\"jaccard\"].mean()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {elapsed:.0f}s\")\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            f\"val_loss/fold{fold}\": avg_val_loss,\n",
        "            f\"score/fold{fold}\": score,\n",
        "        })\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_score = score\n",
        "            best_loss = avg_val_loss\n",
        "            best_preds = predictions\n",
        "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Model. score: {best_score:.4f}, loss: {best_loss:.4f}\")\n",
        "\n",
        "            torch.save(\n",
        "                {\"model\": model.state_dict(), \"preds\": predictions}, MODEL_DIR + f\"{config.model_name.replace('/', '-')}_fold{fold}_best.pth\"\n",
        "            )\n",
        "            # wandb.save(MODEL_DIR + f\"{config.model_name.replace('/', '-')}_fold{fold}_best.pth\")\n",
        "\n",
        "    valid_folds[\"prediction\"] = valid_folds['id'].apply(lambda r: best_preds[r])\n",
        "    valid_folds['jaccard'] = valid_folds[['answer_text', 'prediction']].apply(jaccard, axis=1)\n",
        "\n",
        "    return valid_folds, best_score, best_loss"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znc9U4s9YPqs"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPgK02eYRCX"
      },
      "source": [
        "def main():\n",
        "    # ====================================================\n",
        "    # Training\n",
        "    # ====================================================\n",
        "    if Config.train:\n",
        "        oof_df = pd.DataFrame()\n",
        "        oof_result = []\n",
        "        for fold in range(config.n_fold):\n",
        "            seed_torch(seed + fold)\n",
        "\n",
        "            _oof_df, score, loss = train_loop(train, fold)\n",
        "            oof_df = pd.concat([oof_df, _oof_df])\n",
        "            oof_result.append([fold, score, loss])\n",
        "\n",
        "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "            get_result(_oof_df, fold)\n",
        "            \n",
        "        # CV result\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        get_result(oof_df)\n",
        "        \n",
        "        loss = statistics.mean([d[2] for d in oof_result])\n",
        "        wandb.log({\"loss\": loss})\n",
        "\n",
        "        table = wandb.Table(data=oof_result, columns = [\"fold\", \"score\", \"loss\"])\n",
        "        run.log({\"Fold Result\": table})\n",
        "        \n",
        "        # save result\n",
        "        oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
        "        wandb.save(OUTPUT_DIR + \"oof_df.csv\")\n",
        "\n",
        "        artifact = wandb.Artifact('base-models', type='model')\n",
        "        artifact.add_dir(MODEL_DIR)\n",
        "        run.log_artifact(artifact)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Q3YuoeYiLS",
        "outputId": "8d094c09-62d2-47e4-901f-c14aa0c8f0c0"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 training ==========\n",
            "Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1][0/5982] Elapsed 0m 2s (remain 277m 41s) Loss: 5.2789 Grad: 10.7220 LR: 0.000000 \n",
            "Epoch: [1][100/5982] Elapsed 0m 59s (remain 58m 1s) Loss: 4.9598 Grad: 9.1316 LR: 0.000056 \n",
            "Epoch: [1][200/5982] Elapsed 1m 56s (remain 55m 52s) Loss: 3.5570 Grad: 3.6718 LR: 0.000112 \n",
            "Epoch: [1][300/5982] Elapsed 2m 53s (remain 54m 32s) Loss: 2.7357 Grad: 19.3096 LR: 0.000167 \n",
            "Epoch: [1][400/5982] Elapsed 3m 50s (remain 53m 23s) Loss: 2.3132 Grad: 9.7860 LR: 0.000223 \n",
            "Epoch: [1][500/5982] Elapsed 4m 46s (remain 52m 19s) Loss: 2.0515 Grad: 19.5590 LR: 0.000279 \n",
            "Epoch: [1][600/5982] Elapsed 5m 43s (remain 51m 17s) Loss: 1.8494 Grad: 23.6757 LR: 0.000335 \n",
            "Epoch: [1][700/5982] Elapsed 6m 40s (remain 50m 17s) Loss: 1.7120 Grad: 8.8506 LR: 0.000391 \n",
            "Epoch: [1][800/5982] Elapsed 7m 37s (remain 49m 18s) Loss: 1.6093 Grad: 14.9337 LR: 0.000446 \n",
            "Epoch: [1][900/5982] Elapsed 8m 34s (remain 48m 19s) Loss: 1.5317 Grad: 23.5414 LR: 0.000502 \n",
            "Epoch: [1][1000/5982] Elapsed 9m 31s (remain 47m 21s) Loss: 1.4685 Grad: 31.7141 LR: 0.000558 \n",
            "Epoch: [1][1100/5982] Elapsed 10m 27s (remain 46m 23s) Loss: 1.4088 Grad: 1.1713 LR: 0.000614 \n",
            "Epoch: [1][1200/5982] Elapsed 11m 24s (remain 45m 25s) Loss: 1.3673 Grad: 23.5482 LR: 0.000670 \n",
            "Epoch: [1][1300/5982] Elapsed 12m 21s (remain 44m 27s) Loss: 1.3305 Grad: 3.1566 LR: 0.000725 \n",
            "Epoch: [1][1400/5982] Elapsed 13m 18s (remain 43m 30s) Loss: 1.2931 Grad: 2.2921 LR: 0.000781 \n",
            "Epoch: [1][1500/5982] Elapsed 14m 15s (remain 42m 32s) Loss: 1.2672 Grad: 7.1072 LR: 0.000837 \n",
            "Epoch: [1][1600/5982] Elapsed 15m 11s (remain 41m 35s) Loss: 1.2416 Grad: 22.7192 LR: 0.000893 \n",
            "Epoch: [1][1700/5982] Elapsed 16m 8s (remain 40m 37s) Loss: 1.2240 Grad: 6.6009 LR: 0.000949 \n",
            "Epoch: [1][1800/5982] Elapsed 17m 5s (remain 39m 40s) Loss: 1.2056 Grad: 5.3178 LR: 0.001000 \n",
            "Epoch: [1][1900/5982] Elapsed 18m 2s (remain 38m 43s) Loss: 1.1901 Grad: 7.1000 LR: 0.001000 \n",
            "Epoch: [1][2000/5982] Elapsed 18m 59s (remain 37m 46s) Loss: 1.1827 Grad: 6.4969 LR: 0.001000 \n",
            "Epoch: [1][2100/5982] Elapsed 19m 55s (remain 36m 48s) Loss: 1.1675 Grad: 23.0018 LR: 0.000999 \n",
            "Epoch: [1][2200/5982] Elapsed 20m 52s (remain 35m 51s) Loss: 1.1557 Grad: 10.1847 LR: 0.000998 \n",
            "Epoch: [1][2300/5982] Elapsed 21m 49s (remain 34m 54s) Loss: 1.1459 Grad: 1.1511 LR: 0.000998 \n",
            "Epoch: [1][2400/5982] Elapsed 22m 46s (remain 33m 57s) Loss: 1.1349 Grad: 17.4442 LR: 0.000997 \n",
            "Epoch: [1][2500/5982] Elapsed 23m 43s (remain 33m 0s) Loss: 1.1286 Grad: 24.6891 LR: 0.000995 \n",
            "Epoch: [1][2600/5982] Elapsed 24m 39s (remain 32m 3s) Loss: 1.1217 Grad: 2.3300 LR: 0.000994 \n",
            "Epoch: [1][2700/5982] Elapsed 25m 36s (remain 31m 6s) Loss: 1.1166 Grad: 13.3222 LR: 0.000992 \n",
            "Epoch: [1][2800/5982] Elapsed 26m 33s (remain 30m 9s) Loss: 1.1071 Grad: 14.1114 LR: 0.000990 \n",
            "Epoch: [1][2900/5982] Elapsed 27m 30s (remain 29m 12s) Loss: 1.1044 Grad: 6.0981 LR: 0.000988 \n",
            "Epoch: [1][3000/5982] Elapsed 28m 27s (remain 28m 15s) Loss: 1.1012 Grad: 0.0848 LR: 0.000986 \n",
            "Epoch: [1][3100/5982] Elapsed 29m 23s (remain 27m 18s) Loss: 1.0936 Grad: 17.9005 LR: 0.000984 \n",
            "Epoch: [1][3200/5982] Elapsed 30m 20s (remain 26m 21s) Loss: 1.0848 Grad: 17.6060 LR: 0.000981 \n",
            "Epoch: [1][3300/5982] Elapsed 31m 17s (remain 25m 24s) Loss: 1.0808 Grad: 4.6981 LR: 0.000979 \n",
            "Epoch: [1][3400/5982] Elapsed 32m 14s (remain 24m 28s) Loss: 1.0742 Grad: 7.1129 LR: 0.000976 \n",
            "Epoch: [1][3500/5982] Elapsed 33m 11s (remain 23m 31s) Loss: 1.0677 Grad: 17.2044 LR: 0.000973 \n",
            "Epoch: [1][3600/5982] Elapsed 34m 8s (remain 22m 34s) Loss: 1.0649 Grad: 2.0771 LR: 0.000969 \n",
            "Epoch: [1][3700/5982] Elapsed 35m 4s (remain 21m 37s) Loss: 1.0608 Grad: 0.5249 LR: 0.000966 \n",
            "Epoch: [1][3800/5982] Elapsed 36m 1s (remain 20m 40s) Loss: 1.0556 Grad: 11.7892 LR: 0.000962 \n",
            "Epoch: [1][3900/5982] Elapsed 36m 58s (remain 19m 43s) Loss: 1.0488 Grad: 2.3892 LR: 0.000959 \n",
            "Epoch: [1][4000/5982] Elapsed 37m 55s (remain 18m 46s) Loss: 1.0432 Grad: 2.7984 LR: 0.000955 \n",
            "Epoch: [1][4100/5982] Elapsed 38m 52s (remain 17m 49s) Loss: 1.0375 Grad: 55.2900 LR: 0.000950 \n",
            "Epoch: [1][4200/5982] Elapsed 39m 48s (remain 16m 52s) Loss: 1.0370 Grad: 0.2817 LR: 0.000946 \n",
            "Epoch: [1][4300/5982] Elapsed 40m 45s (remain 15m 55s) Loss: 1.0329 Grad: 11.9199 LR: 0.000942 \n",
            "Epoch: [1][4400/5982] Elapsed 41m 42s (remain 14m 59s) Loss: 1.0301 Grad: 35.7893 LR: 0.000937 \n",
            "Epoch: [1][4500/5982] Elapsed 42m 39s (remain 14m 2s) Loss: 1.0270 Grad: 1.8229 LR: 0.000932 \n",
            "Epoch: [1][4600/5982] Elapsed 43m 36s (remain 13m 5s) Loss: 1.0238 Grad: 8.5885 LR: 0.000927 \n",
            "Epoch: [1][4700/5982] Elapsed 44m 32s (remain 12m 8s) Loss: 1.0184 Grad: 7.7835 LR: 0.000922 \n",
            "Epoch: [1][4800/5982] Elapsed 45m 29s (remain 11m 11s) Loss: 1.0135 Grad: 21.6774 LR: 0.000917 \n",
            "Epoch: [1][4900/5982] Elapsed 46m 26s (remain 10m 14s) Loss: 1.0111 Grad: 4.3444 LR: 0.000911 \n",
            "Epoch: [1][5000/5982] Elapsed 47m 25s (remain 9m 18s) Loss: 1.0084 Grad: 10.8355 LR: 0.000906 \n",
            "Epoch: [1][5100/5982] Elapsed 48m 22s (remain 8m 21s) Loss: 1.0085 Grad: 12.1587 LR: 0.000900 \n",
            "Epoch: [1][5200/5982] Elapsed 49m 19s (remain 7m 24s) Loss: 1.0068 Grad: 15.8587 LR: 0.000894 \n",
            "Epoch: [1][5300/5982] Elapsed 50m 15s (remain 6m 27s) Loss: 1.0057 Grad: 18.3565 LR: 0.000888 \n",
            "Epoch: [1][5400/5982] Elapsed 51m 12s (remain 5m 30s) Loss: 1.0046 Grad: 14.6029 LR: 0.000882 \n",
            "Epoch: [1][5500/5982] Elapsed 52m 9s (remain 4m 33s) Loss: 0.9992 Grad: 10.4025 LR: 0.000875 \n",
            "Epoch: [1][5600/5982] Elapsed 53m 6s (remain 3m 36s) Loss: 0.9979 Grad: 3.1401 LR: 0.000869 \n",
            "Epoch: [1][5700/5982] Elapsed 54m 3s (remain 2m 39s) Loss: 0.9961 Grad: 0.1878 LR: 0.000862 \n",
            "Epoch: [1][5800/5982] Elapsed 54m 59s (remain 1m 42s) Loss: 0.9929 Grad: 5.6816 LR: 0.000856 \n",
            "Epoch: [1][5900/5982] Elapsed 55m 56s (remain 0m 46s) Loss: 0.9893 Grad: 4.7720 LR: 0.000849 \n",
            "Epoch: [1][5981/5982] Elapsed 56m 41s (remain 0m 0s) Loss: 0.9854 Grad: 6.8090 LR: 0.000843 \n",
            "EVAL: [0/738] Elapsed 0m 0s (remain 9m 41s) Loss: 0.2096 \n",
            "EVAL: [100/738] Elapsed 0m 17s (remain 1m 53s) Loss: 0.3331 \n",
            "EVAL: [200/738] Elapsed 0m 35s (remain 1m 33s) Loss: 0.2864 \n",
            "EVAL: [300/738] Elapsed 0m 52s (remain 1m 15s) Loss: 0.2583 \n",
            "EVAL: [400/738] Elapsed 1m 9s (remain 0m 58s) Loss: 0.2568 \n",
            "EVAL: [500/738] Elapsed 1m 26s (remain 0m 40s) Loss: 0.2475 \n",
            "EVAL: [600/738] Elapsed 1m 43s (remain 0m 23s) Loss: 0.2491 \n",
            "EVAL: [700/738] Elapsed 2m 0s (remain 0m 6s) Loss: 0.2501 \n",
            "EVAL: [737/738] Elapsed 2m 7s (remain 0m 0s) Loss: 0.2522 \n",
            "Post-processing 223 example predictions split into 2952 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Score: 0.6151576624670794, Train Loss: 0.9854, Val Loss: 0.2522, Time: 3530s\n",
            "Epoch 1 - Save Best Model. score: 0.6152, loss: 0.2522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [2][0/5982] Elapsed 0m 1s (remain 116m 24s) Loss: 0.0208 Grad: 0.9577 LR: 0.000843 \n",
            "Epoch: [2][100/5982] Elapsed 0m 57s (remain 56m 16s) Loss: 0.5109 Grad: 3.4629 LR: 0.000836 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1pdUFYKYmOM"
      },
      "source": [
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}